{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Import libraries:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import cross_validation, metrics   #Additional     scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV   #Perforing grid search\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('LargeTrain.csv')\n",
    "target = 'Class'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def modelfit(alg, dtrain, predictors,useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,stratified=True,\n",
    "            metrics={'mlogloss'}, early_stopping_rounds=early_stopping_rounds, callbacks=[xgb.callback.print_evaluation(show_stdv=False),                                                               xgb.callback.early_stop(3)])\n",
    "\n",
    "        print (cvresult)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain['Class'],eval_metric='mlogloss')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])\n",
    "\n",
    "    #Print model report:\n",
    "    print (\"\\nModel Report\")\n",
    "    print (\"Accuracy : %.4g\" % metrics.accuracy_score(dtrain['Class'].values, dtrain_predictions))\n",
    "    print (\"Log Loss Score (Train): %f\" % metrics.log_loss(dtrain['Class'], dtrain_predprob))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/xgboost/python-package/xgboost/sklearn.py:161: DeprecationWarning: The seed parameter is deprecated as of version .6.Please use random_state instead.seed is deprecated.\n",
      "  'seed is deprecated.', DeprecationWarning)\n",
      "/home/peter/xgboost/python-package/xgboost/sklearn.py:171: DeprecationWarning: The nthread parameter is deprecated as of version .6.Please use n_jobs instead.nthread is deprecated.\n",
      "  'nthread is deprecated.', DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.99623, std: 0.00068, params: {'min_child_weight': 1},\n",
       "  mean: 0.99614, std: 0.00111, params: {'min_child_weight': 3}],\n",
       " {'min_child_weight': 1},\n",
       " 0.9962272969064084)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first try with all random para\n",
    "\n",
    "#Choose all predictors except target \n",
    "predictors = [x for x in train.columns if x not in target]\n",
    "\n",
    "param_test1 = {\n",
    " 'min_child_weight':[1, 3]\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=50, max_depth=5,\n",
    " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'multi:softprob', num_class = 10, nthread=4, scale_pos_weight=1, seed=27), \n",
    " param_grid = param_test1, n_jobs=1,iid=False, cv=5)\n",
    "gsearch1.fit(train[predictors],train[target])\n",
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.83874\ttest-mlogloss:1.84137\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 3 rounds.\n",
      "[1]\ttrain-mlogloss:1.54737\ttest-mlogloss:1.55199\n",
      "[2]\ttrain-mlogloss:1.3338\ttest-mlogloss:1.34004\n",
      "[3]\ttrain-mlogloss:1.16554\ttest-mlogloss:1.17334\n",
      "[4]\ttrain-mlogloss:1.02753\ttest-mlogloss:1.03639\n",
      "[5]\ttrain-mlogloss:0.911387\ttest-mlogloss:0.921146\n",
      "[6]\ttrain-mlogloss:0.812091\ttest-mlogloss:0.822702\n",
      "[7]\ttrain-mlogloss:0.726076\ttest-mlogloss:0.73731\n",
      "[8]\ttrain-mlogloss:0.651017\ttest-mlogloss:0.663016\n",
      "[9]\ttrain-mlogloss:0.584863\ttest-mlogloss:0.597296\n",
      "[10]\ttrain-mlogloss:0.526404\ttest-mlogloss:0.539375\n",
      "[11]\ttrain-mlogloss:0.47434\ttest-mlogloss:0.48783\n",
      "[12]\ttrain-mlogloss:0.427926\ttest-mlogloss:0.44187\n",
      "[13]\ttrain-mlogloss:0.38654\ttest-mlogloss:0.40084\n",
      "[14]\ttrain-mlogloss:0.349427\ttest-mlogloss:0.363964\n",
      "[15]\ttrain-mlogloss:0.31608\ttest-mlogloss:0.330723\n",
      "[16]\ttrain-mlogloss:0.286152\ttest-mlogloss:0.30093\n",
      "[17]\ttrain-mlogloss:0.259172\ttest-mlogloss:0.274146\n",
      "[18]\ttrain-mlogloss:0.234889\ttest-mlogloss:0.250031\n",
      "[19]\ttrain-mlogloss:0.212927\ttest-mlogloss:0.228064\n",
      "[20]\ttrain-mlogloss:0.193152\ttest-mlogloss:0.208383\n",
      "[21]\ttrain-mlogloss:0.175278\ttest-mlogloss:0.190525\n",
      "[22]\ttrain-mlogloss:0.159047\ttest-mlogloss:0.174305\n",
      "[23]\ttrain-mlogloss:0.144401\ttest-mlogloss:0.159617\n",
      "[24]\ttrain-mlogloss:0.131144\ttest-mlogloss:0.146359\n",
      "[25]\ttrain-mlogloss:0.119132\ttest-mlogloss:0.134185\n",
      "[26]\ttrain-mlogloss:0.108293\ttest-mlogloss:0.123379\n",
      "[27]\ttrain-mlogloss:0.0984564\ttest-mlogloss:0.113506\n",
      "[28]\ttrain-mlogloss:0.0895458\ttest-mlogloss:0.104556\n",
      "[29]\ttrain-mlogloss:0.0814936\ttest-mlogloss:0.0964566\n",
      "[30]\ttrain-mlogloss:0.0741758\ttest-mlogloss:0.089047\n",
      "[31]\ttrain-mlogloss:0.0675464\ttest-mlogloss:0.0823176\n",
      "[32]\ttrain-mlogloss:0.0615386\ttest-mlogloss:0.0761546\n",
      "[33]\ttrain-mlogloss:0.0560864\ttest-mlogloss:0.0705618\n",
      "[34]\ttrain-mlogloss:0.0511286\ttest-mlogloss:0.0654766\n",
      "[35]\ttrain-mlogloss:0.046658\ttest-mlogloss:0.0609106\n",
      "[36]\ttrain-mlogloss:0.0425992\ttest-mlogloss:0.056734\n",
      "[37]\ttrain-mlogloss:0.038917\ttest-mlogloss:0.0529918\n",
      "[38]\ttrain-mlogloss:0.0355796\ttest-mlogloss:0.049495\n",
      "[39]\ttrain-mlogloss:0.032551\ttest-mlogloss:0.0464088\n",
      "[40]\ttrain-mlogloss:0.0298012\ttest-mlogloss:0.0436074\n",
      "[41]\ttrain-mlogloss:0.0272888\ttest-mlogloss:0.0409516\n",
      "[42]\ttrain-mlogloss:0.0250102\ttest-mlogloss:0.038606\n",
      "[43]\ttrain-mlogloss:0.0229354\ttest-mlogloss:0.0364476\n",
      "[44]\ttrain-mlogloss:0.0210492\ttest-mlogloss:0.0344858\n",
      "[45]\ttrain-mlogloss:0.0193274\ttest-mlogloss:0.0326448\n",
      "[46]\ttrain-mlogloss:0.017766\ttest-mlogloss:0.0310228\n",
      "[47]\ttrain-mlogloss:0.0163516\ttest-mlogloss:0.0295502\n",
      "[48]\ttrain-mlogloss:0.0150626\ttest-mlogloss:0.0281362\n",
      "[49]\ttrain-mlogloss:0.013894\ttest-mlogloss:0.0268488\n",
      "[50]\ttrain-mlogloss:0.0128358\ttest-mlogloss:0.0257342\n",
      "[51]\ttrain-mlogloss:0.0118676\ttest-mlogloss:0.0246868\n",
      "[52]\ttrain-mlogloss:0.0109816\ttest-mlogloss:0.0237634\n",
      "[53]\ttrain-mlogloss:0.0101752\ttest-mlogloss:0.022905\n",
      "[54]\ttrain-mlogloss:0.0094376\ttest-mlogloss:0.0220936\n",
      "[55]\ttrain-mlogloss:0.0087694\ttest-mlogloss:0.0213444\n",
      "[56]\ttrain-mlogloss:0.0081542\ttest-mlogloss:0.0206856\n",
      "[57]\ttrain-mlogloss:0.0075968\ttest-mlogloss:0.0200714\n",
      "[58]\ttrain-mlogloss:0.0070878\ttest-mlogloss:0.0194602\n",
      "[59]\ttrain-mlogloss:0.0066232\ttest-mlogloss:0.0189488\n",
      "[60]\ttrain-mlogloss:0.0061858\ttest-mlogloss:0.0184448\n",
      "[61]\ttrain-mlogloss:0.0057946\ttest-mlogloss:0.0179956\n",
      "[62]\ttrain-mlogloss:0.0054408\ttest-mlogloss:0.0176192\n",
      "[63]\ttrain-mlogloss:0.0051052\ttest-mlogloss:0.0172244\n",
      "[64]\ttrain-mlogloss:0.0048\ttest-mlogloss:0.0168492\n",
      "[65]\ttrain-mlogloss:0.0045192\ttest-mlogloss:0.0165282\n",
      "[66]\ttrain-mlogloss:0.0042658\ttest-mlogloss:0.0162244\n",
      "[67]\ttrain-mlogloss:0.0040282\ttest-mlogloss:0.0159298\n",
      "[68]\ttrain-mlogloss:0.0038102\ttest-mlogloss:0.0156408\n",
      "[69]\ttrain-mlogloss:0.003612\ttest-mlogloss:0.0153782\n",
      "[70]\ttrain-mlogloss:0.0034302\ttest-mlogloss:0.0151734\n",
      "[71]\ttrain-mlogloss:0.0032614\ttest-mlogloss:0.0149838\n",
      "[72]\ttrain-mlogloss:0.0031036\ttest-mlogloss:0.014789\n",
      "[73]\ttrain-mlogloss:0.0029614\ttest-mlogloss:0.014607\n",
      "[74]\ttrain-mlogloss:0.0028292\ttest-mlogloss:0.0144374\n",
      "[75]\ttrain-mlogloss:0.0027066\ttest-mlogloss:0.0143094\n",
      "[76]\ttrain-mlogloss:0.0025922\ttest-mlogloss:0.0141942\n",
      "[77]\ttrain-mlogloss:0.0024852\ttest-mlogloss:0.0140558\n",
      "[78]\ttrain-mlogloss:0.0023882\ttest-mlogloss:0.0139486\n",
      "[79]\ttrain-mlogloss:0.0022994\ttest-mlogloss:0.0138654\n",
      "[80]\ttrain-mlogloss:0.0022148\ttest-mlogloss:0.0137502\n",
      "[81]\ttrain-mlogloss:0.0021338\ttest-mlogloss:0.0136284\n",
      "[82]\ttrain-mlogloss:0.0020586\ttest-mlogloss:0.0135012\n",
      "[83]\ttrain-mlogloss:0.001988\ttest-mlogloss:0.0134116\n",
      "[84]\ttrain-mlogloss:0.0019208\ttest-mlogloss:0.013306\n",
      "[85]\ttrain-mlogloss:0.0018584\ttest-mlogloss:0.0131956\n",
      "[86]\ttrain-mlogloss:0.001801\ttest-mlogloss:0.0131194\n",
      "[87]\ttrain-mlogloss:0.001746\ttest-mlogloss:0.013052\n",
      "[88]\ttrain-mlogloss:0.0016938\ttest-mlogloss:0.0129952\n",
      "[89]\ttrain-mlogloss:0.0016454\ttest-mlogloss:0.0129374\n",
      "[90]\ttrain-mlogloss:0.0015998\ttest-mlogloss:0.0128722\n",
      "[91]\ttrain-mlogloss:0.001558\ttest-mlogloss:0.0127954\n",
      "[92]\ttrain-mlogloss:0.001518\ttest-mlogloss:0.0127408\n",
      "[93]\ttrain-mlogloss:0.0014812\ttest-mlogloss:0.0127004\n",
      "[94]\ttrain-mlogloss:0.001447\ttest-mlogloss:0.0126538\n",
      "[95]\ttrain-mlogloss:0.001413\ttest-mlogloss:0.0126208\n",
      "[96]\ttrain-mlogloss:0.0013822\ttest-mlogloss:0.0125718\n",
      "[97]\ttrain-mlogloss:0.0013532\ttest-mlogloss:0.0125186\n",
      "[98]\ttrain-mlogloss:0.0013258\ttest-mlogloss:0.012494\n",
      "[99]\ttrain-mlogloss:0.001299\ttest-mlogloss:0.012429\n",
      "[100]\ttrain-mlogloss:0.0012732\ttest-mlogloss:0.0123874\n",
      "[101]\ttrain-mlogloss:0.0012494\ttest-mlogloss:0.0123554\n",
      "[102]\ttrain-mlogloss:0.0012268\ttest-mlogloss:0.0123458\n",
      "[103]\ttrain-mlogloss:0.001204\ttest-mlogloss:0.0123196\n",
      "[104]\ttrain-mlogloss:0.0011828\ttest-mlogloss:0.0123042\n",
      "[105]\ttrain-mlogloss:0.0011628\ttest-mlogloss:0.0122962\n",
      "[106]\ttrain-mlogloss:0.0011432\ttest-mlogloss:0.0122616\n",
      "[107]\ttrain-mlogloss:0.0011262\ttest-mlogloss:0.0122532\n",
      "[108]\ttrain-mlogloss:0.0011094\ttest-mlogloss:0.0122442\n",
      "[109]\ttrain-mlogloss:0.0010924\ttest-mlogloss:0.0122366\n",
      "[110]\ttrain-mlogloss:0.0010778\ttest-mlogloss:0.012225\n",
      "[111]\ttrain-mlogloss:0.0010636\ttest-mlogloss:0.0121864\n",
      "[112]\ttrain-mlogloss:0.0010514\ttest-mlogloss:0.012155\n",
      "[113]\ttrain-mlogloss:0.0010394\ttest-mlogloss:0.0121302\n",
      "[114]\ttrain-mlogloss:0.0010266\ttest-mlogloss:0.0121204\n",
      "[115]\ttrain-mlogloss:0.0010146\ttest-mlogloss:0.012107\n",
      "[116]\ttrain-mlogloss:0.0010044\ttest-mlogloss:0.0120992\n",
      "[117]\ttrain-mlogloss:0.000994\ttest-mlogloss:0.012097\n",
      "[118]\ttrain-mlogloss:0.0009844\ttest-mlogloss:0.0120848\n",
      "[119]\ttrain-mlogloss:0.000975\ttest-mlogloss:0.012079\n",
      "[120]\ttrain-mlogloss:0.0009654\ttest-mlogloss:0.0120638\n",
      "[121]\ttrain-mlogloss:0.0009568\ttest-mlogloss:0.012059\n",
      "[122]\ttrain-mlogloss:0.0009484\ttest-mlogloss:0.0120474\n",
      "[123]\ttrain-mlogloss:0.00094\ttest-mlogloss:0.012036\n",
      "[124]\ttrain-mlogloss:0.000933\ttest-mlogloss:0.0120176\n",
      "[125]\ttrain-mlogloss:0.0009248\ttest-mlogloss:0.012008\n",
      "[126]\ttrain-mlogloss:0.0009176\ttest-mlogloss:0.0120022\n",
      "[127]\ttrain-mlogloss:0.0009112\ttest-mlogloss:0.012003\n",
      "[128]\ttrain-mlogloss:0.0009054\ttest-mlogloss:0.0119806\n",
      "[129]\ttrain-mlogloss:0.000899\ttest-mlogloss:0.0119596\n",
      "[130]\ttrain-mlogloss:0.000893\ttest-mlogloss:0.0119468\n",
      "[131]\ttrain-mlogloss:0.0008868\ttest-mlogloss:0.0119496\n",
      "[132]\ttrain-mlogloss:0.0008814\ttest-mlogloss:0.0119404\n",
      "[133]\ttrain-mlogloss:0.0008762\ttest-mlogloss:0.0119172\n",
      "[134]\ttrain-mlogloss:0.0008706\ttest-mlogloss:0.0119134\n",
      "[135]\ttrain-mlogloss:0.0008654\ttest-mlogloss:0.0118998\n",
      "[136]\ttrain-mlogloss:0.0008602\ttest-mlogloss:0.0118986\n",
      "[137]\ttrain-mlogloss:0.0008558\ttest-mlogloss:0.0118874\n",
      "[138]\ttrain-mlogloss:0.0008512\ttest-mlogloss:0.0118796\n",
      "[139]\ttrain-mlogloss:0.0008472\ttest-mlogloss:0.011873\n",
      "[140]\ttrain-mlogloss:0.0008432\ttest-mlogloss:0.0118866\n",
      "[141]\ttrain-mlogloss:0.0008392\ttest-mlogloss:0.0118758\n",
      "[142]\ttrain-mlogloss:0.0008348\ttest-mlogloss:0.011872\n",
      "[143]\ttrain-mlogloss:0.000831\ttest-mlogloss:0.011867\n",
      "[144]\ttrain-mlogloss:0.0008274\ttest-mlogloss:0.011866\n",
      "[145]\ttrain-mlogloss:0.0008236\ttest-mlogloss:0.0118632\n",
      "[146]\ttrain-mlogloss:0.00082\ttest-mlogloss:0.0118714\n",
      "[147]\ttrain-mlogloss:0.0008166\ttest-mlogloss:0.011869\n",
      "[148]\ttrain-mlogloss:0.000813\ttest-mlogloss:0.0118652\n",
      "Stopping. Best iteration:\n",
      "[145]\ttrain-mlogloss:0.0008236+1.08554e-05\ttest-mlogloss:0.0118632+0.00403778\n",
      "\n",
      "     test-mlogloss-mean  test-mlogloss-std  train-mlogloss-mean  \\\n",
      "0              1.841367           0.002341             1.838736   \n",
      "1              1.551989           0.002449             1.547366   \n",
      "2              1.340038           0.003013             1.333800   \n",
      "3              1.173339           0.003088             1.165542   \n",
      "4              1.036389           0.003105             1.027535   \n",
      "5              0.921146           0.003202             0.911387   \n",
      "6              0.822702           0.003449             0.812091   \n",
      "7              0.737310           0.003437             0.726076   \n",
      "8              0.663016           0.003288             0.651017   \n",
      "9              0.597296           0.003382             0.584863   \n",
      "10             0.539375           0.003454             0.526404   \n",
      "11             0.487830           0.003489             0.474340   \n",
      "12             0.441870           0.003521             0.427926   \n",
      "13             0.400840           0.003626             0.386540   \n",
      "14             0.363964           0.003475             0.349427   \n",
      "15             0.330723           0.003627             0.316080   \n",
      "16             0.300930           0.003631             0.286152   \n",
      "17             0.274146           0.003681             0.259172   \n",
      "18             0.250031           0.003773             0.234889   \n",
      "19             0.228064           0.003683             0.212927   \n",
      "20             0.208383           0.003710             0.193152   \n",
      "21             0.190525           0.003646             0.175278   \n",
      "22             0.174305           0.003726             0.159047   \n",
      "23             0.159617           0.003802             0.144401   \n",
      "24             0.146359           0.003882             0.131144   \n",
      "25             0.134185           0.003860             0.119132   \n",
      "26             0.123379           0.003862             0.108293   \n",
      "27             0.113506           0.003884             0.098456   \n",
      "28             0.104556           0.003881             0.089546   \n",
      "29             0.096457           0.003884             0.081494   \n",
      "..                  ...                ...                  ...   \n",
      "116            0.012099           0.003982             0.001004   \n",
      "117            0.012097           0.003992             0.000994   \n",
      "118            0.012085           0.003992             0.000984   \n",
      "119            0.012079           0.003992             0.000975   \n",
      "120            0.012064           0.003991             0.000965   \n",
      "121            0.012059           0.003995             0.000957   \n",
      "122            0.012047           0.003995             0.000948   \n",
      "123            0.012036           0.003999             0.000940   \n",
      "124            0.012018           0.004005             0.000933   \n",
      "125            0.012008           0.004001             0.000925   \n",
      "126            0.012002           0.003999             0.000918   \n",
      "127            0.012003           0.004006             0.000911   \n",
      "128            0.011981           0.003995             0.000905   \n",
      "129            0.011960           0.004002             0.000899   \n",
      "130            0.011947           0.004000             0.000893   \n",
      "131            0.011950           0.004010             0.000887   \n",
      "132            0.011940           0.004008             0.000881   \n",
      "133            0.011917           0.004009             0.000876   \n",
      "134            0.011913           0.004014             0.000871   \n",
      "135            0.011900           0.004011             0.000865   \n",
      "136            0.011899           0.004014             0.000860   \n",
      "137            0.011887           0.004013             0.000856   \n",
      "138            0.011880           0.004019             0.000851   \n",
      "139            0.011873           0.004022             0.000847   \n",
      "140            0.011887           0.004030             0.000843   \n",
      "141            0.011876           0.004026             0.000839   \n",
      "142            0.011872           0.004034             0.000835   \n",
      "143            0.011867           0.004032             0.000831   \n",
      "144            0.011866           0.004036             0.000827   \n",
      "145            0.011863           0.004038             0.000824   \n",
      "\n",
      "     train-mlogloss-std  \n",
      "0              0.002714  \n",
      "1              0.001971  \n",
      "2              0.001480  \n",
      "3              0.001306  \n",
      "4              0.001263  \n",
      "5              0.001340  \n",
      "6              0.001289  \n",
      "7              0.001162  \n",
      "8              0.001069  \n",
      "9              0.001236  \n",
      "10             0.001162  \n",
      "11             0.001090  \n",
      "12             0.001072  \n",
      "13             0.001158  \n",
      "14             0.001112  \n",
      "15             0.001067  \n",
      "16             0.001040  \n",
      "17             0.001024  \n",
      "18             0.000959  \n",
      "19             0.000923  \n",
      "20             0.000839  \n",
      "21             0.000780  \n",
      "22             0.000706  \n",
      "23             0.000627  \n",
      "24             0.000603  \n",
      "25             0.000543  \n",
      "26             0.000515  \n",
      "27             0.000492  \n",
      "28             0.000434  \n",
      "29             0.000378  \n",
      "..                  ...  \n",
      "116            0.000013  \n",
      "117            0.000013  \n",
      "118            0.000014  \n",
      "119            0.000013  \n",
      "120            0.000012  \n",
      "121            0.000012  \n",
      "122            0.000012  \n",
      "123            0.000012  \n",
      "124            0.000012  \n",
      "125            0.000012  \n",
      "126            0.000012  \n",
      "127            0.000012  \n",
      "128            0.000012  \n",
      "129            0.000012  \n",
      "130            0.000011  \n",
      "131            0.000012  \n",
      "132            0.000012  \n",
      "133            0.000011  \n",
      "134            0.000012  \n",
      "135            0.000012  \n",
      "136            0.000012  \n",
      "137            0.000012  \n",
      "138            0.000012  \n",
      "139            0.000011  \n",
      "140            0.000012  \n",
      "141            0.000011  \n",
      "142            0.000011  \n",
      "143            0.000011  \n",
      "144            0.000011  \n",
      "145            0.000011  \n",
      "\n",
      "[146 rows x 4 columns]\n",
      "\n",
      "Model Report\n",
      "Accuracy : 1\n",
      "Log Loss Score (Train): 0.000665\n"
     ]
    }
   ],
   "source": [
    "# choose the best n_estimators\n",
    "xgb1 = XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'multi:softprob',\n",
    " num_class = 10,\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "modelfit(xgb1, train, predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/xgboost/python-package/xgboost/sklearn.py:161: DeprecationWarning: The seed parameter is deprecated as of version .6.Please use random_state instead.seed is deprecated.\n",
      "  'seed is deprecated.', DeprecationWarning)\n",
      "/home/peter/xgboost/python-package/xgboost/sklearn.py:171: DeprecationWarning: The nthread parameter is deprecated as of version .6.Please use n_jobs instead.nthread is deprecated.\n",
      "  'nthread is deprecated.', DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.99724, std: 0.00065, params: {'max_depth': 3, 'min_child_weight': 1},\n",
       "  mean: 0.99687, std: 0.00128, params: {'max_depth': 3, 'min_child_weight': 3},\n",
       "  mean: 0.99641, std: 0.00128, params: {'max_depth': 3, 'min_child_weight': 5},\n",
       "  mean: 0.99696, std: 0.00069, params: {'max_depth': 5, 'min_child_weight': 1},\n",
       "  mean: 0.99641, std: 0.00106, params: {'max_depth': 5, 'min_child_weight': 3},\n",
       "  mean: 0.99650, std: 0.00135, params: {'max_depth': 5, 'min_child_weight': 5},\n",
       "  mean: 0.99678, std: 0.00092, params: {'max_depth': 7, 'min_child_weight': 1},\n",
       "  mean: 0.99650, std: 0.00122, params: {'max_depth': 7, 'min_child_weight': 3},\n",
       "  mean: 0.99614, std: 0.00138, params: {'max_depth': 7, 'min_child_weight': 5},\n",
       "  mean: 0.99669, std: 0.00102, params: {'max_depth': 9, 'min_child_weight': 1},\n",
       "  mean: 0.99669, std: 0.00128, params: {'max_depth': 9, 'min_child_weight': 3},\n",
       "  mean: 0.99623, std: 0.00147, params: {'max_depth': 9, 'min_child_weight': 5}],\n",
       " {'max_depth': 3, 'min_child_weight': 1},\n",
       " 0.9972395121199924)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tune max_depth and min_child_weight use the formal result of best n_estimators=145\n",
    "predictors = [x for x in train.columns if x not in target]\n",
    "param_test1 = {\n",
    " 'max_depth' : [3, 5, 7 , 9] ,\n",
    " 'min_child_weight':[1, 3, 5]\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=145, max_depth=5,\n",
    " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'multi:softprob', num_class = 10, nthread=4, scale_pos_weight=1, seed=27), \n",
    " param_grid = param_test1, n_jobs=1,iid=False, cv=5)\n",
    "gsearch1.fit(train[predictors],train[target])\n",
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/xgboost/python-package/xgboost/sklearn.py:161: DeprecationWarning: The seed parameter is deprecated as of version .6.Please use random_state instead.seed is deprecated.\n",
      "  'seed is deprecated.', DeprecationWarning)\n",
      "/home/peter/xgboost/python-package/xgboost/sklearn.py:171: DeprecationWarning: The nthread parameter is deprecated as of version .6.Please use n_jobs instead.nthread is deprecated.\n",
      "  'nthread is deprecated.', DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.99660, std: 0.00090, params: {'max_depth': 2, 'min_child_weight': 1},\n",
       "  mean: 0.99669, std: 0.00089, params: {'max_depth': 2, 'min_child_weight': 2},\n",
       "  mean: 0.99660, std: 0.00090, params: {'max_depth': 2, 'min_child_weight': 3},\n",
       "  mean: 0.99724, std: 0.00065, params: {'max_depth': 3, 'min_child_weight': 1},\n",
       "  mean: 0.99678, std: 0.00120, params: {'max_depth': 3, 'min_child_weight': 2},\n",
       "  mean: 0.99687, std: 0.00128, params: {'max_depth': 3, 'min_child_weight': 3},\n",
       "  mean: 0.99669, std: 0.00094, params: {'max_depth': 4, 'min_child_weight': 1},\n",
       "  mean: 0.99678, std: 0.00127, params: {'max_depth': 4, 'min_child_weight': 2},\n",
       "  mean: 0.99650, std: 0.00129, params: {'max_depth': 4, 'min_child_weight': 3}],\n",
       " {'max_depth': 3, 'min_child_weight': 1},\n",
       " 0.9972395121199924)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# continue to find the optimal value\n",
    "predictors = [x for x in train.columns if x not in target]\n",
    "param_test1 = {\n",
    " 'max_depth' : [2, 3 , 4] ,\n",
    " 'min_child_weight':[1, 2, 3]\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=145, max_depth=5,\n",
    " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'multi:softprob', num_class = 10, nthread=4, scale_pos_weight=1, seed=27), \n",
    " param_grid = param_test1, n_jobs=1,iid=False, cv=5)\n",
    "gsearch1.fit(train[predictors],train[target])\n",
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/xgboost/python-package/xgboost/sklearn.py:161: DeprecationWarning: The seed parameter is deprecated as of version .6.Please use random_state instead.seed is deprecated.\n",
      "  'seed is deprecated.', DeprecationWarning)\n",
      "/home/peter/xgboost/python-package/xgboost/sklearn.py:171: DeprecationWarning: The nthread parameter is deprecated as of version .6.Please use n_jobs instead.nthread is deprecated.\n",
      "  'nthread is deprecated.', DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.99724, std: 0.00065, params: {'gamma': 0.0},\n",
       "  mean: 0.99696, std: 0.00062, params: {'gamma': 0.1},\n",
       "  mean: 0.99669, std: 0.00089, params: {'gamma': 0.2},\n",
       "  mean: 0.99678, std: 0.00092, params: {'gamma': 0.3},\n",
       "  mean: 0.99660, std: 0.00080, params: {'gamma': 0.4}],\n",
       " {'gamma': 0.0},\n",
       " 0.9972395121199924)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tune gamma with the value get before\n",
    "predictors = [x for x in train.columns if x not in target]\n",
    "param_test3 = {\n",
    " 'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "gsearch3 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=145, max_depth=3,\n",
    " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'multi:softprob', num_class = 10, nthread=4, scale_pos_weight=1, seed=27), \n",
    " param_grid = param_test3,n_jobs=1,iid=False, cv=5)\n",
    "gsearch3.fit(train[predictors],train[target])\n",
    "gsearch3.grid_scores_, gsearch3.best_params_, gsearch3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.85137\ttest-mlogloss:1.8535\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 3 rounds.\n",
      "[1]\ttrain-mlogloss:1.56466\ttest-mlogloss:1.56825\n",
      "[2]\ttrain-mlogloss:1.35123\ttest-mlogloss:1.35598\n",
      "[3]\ttrain-mlogloss:1.18356\ttest-mlogloss:1.18905\n",
      "[4]\ttrain-mlogloss:1.046\ttest-mlogloss:1.05221\n",
      "[5]\ttrain-mlogloss:0.930168\ttest-mlogloss:0.936988\n",
      "[6]\ttrain-mlogloss:0.831311\ttest-mlogloss:0.838819\n",
      "[7]\ttrain-mlogloss:0.745389\ttest-mlogloss:0.753296\n",
      "[8]\ttrain-mlogloss:0.670433\ttest-mlogloss:0.678771\n",
      "[9]\ttrain-mlogloss:0.604452\ttest-mlogloss:0.613264\n",
      "[10]\ttrain-mlogloss:0.545726\ttest-mlogloss:0.554717\n",
      "[11]\ttrain-mlogloss:0.493578\ttest-mlogloss:0.502866\n",
      "[12]\ttrain-mlogloss:0.446997\ttest-mlogloss:0.45661\n",
      "[13]\ttrain-mlogloss:0.405531\ttest-mlogloss:0.41545\n",
      "[14]\ttrain-mlogloss:0.368166\ttest-mlogloss:0.378257\n",
      "[15]\ttrain-mlogloss:0.334641\ttest-mlogloss:0.344909\n",
      "[16]\ttrain-mlogloss:0.304485\ttest-mlogloss:0.314962\n",
      "[17]\ttrain-mlogloss:0.277244\ttest-mlogloss:0.288005\n",
      "[18]\ttrain-mlogloss:0.252693\ttest-mlogloss:0.263593\n",
      "[19]\ttrain-mlogloss:0.230314\ttest-mlogloss:0.241274\n",
      "[20]\ttrain-mlogloss:0.210099\ttest-mlogloss:0.221219\n",
      "[21]\ttrain-mlogloss:0.191829\ttest-mlogloss:0.203076\n",
      "[22]\ttrain-mlogloss:0.1752\ttest-mlogloss:0.186514\n",
      "[23]\ttrain-mlogloss:0.160057\ttest-mlogloss:0.171401\n",
      "[24]\ttrain-mlogloss:0.146336\ttest-mlogloss:0.157687\n",
      "[25]\ttrain-mlogloss:0.133819\ttest-mlogloss:0.145241\n",
      "[26]\ttrain-mlogloss:0.122552\ttest-mlogloss:0.134135\n",
      "[27]\ttrain-mlogloss:0.112217\ttest-mlogloss:0.123795\n",
      "[28]\ttrain-mlogloss:0.102864\ttest-mlogloss:0.114436\n",
      "[29]\ttrain-mlogloss:0.0942832\ttest-mlogloss:0.105902\n",
      "[30]\ttrain-mlogloss:0.0864984\ttest-mlogloss:0.0980456\n",
      "[31]\ttrain-mlogloss:0.0793904\ttest-mlogloss:0.0909474\n",
      "[32]\ttrain-mlogloss:0.0728904\ttest-mlogloss:0.084408\n",
      "[33]\ttrain-mlogloss:0.0669742\ttest-mlogloss:0.0784116\n",
      "[34]\ttrain-mlogloss:0.061568\ttest-mlogloss:0.072968\n",
      "[35]\ttrain-mlogloss:0.0566604\ttest-mlogloss:0.0680536\n",
      "[36]\ttrain-mlogloss:0.052205\ttest-mlogloss:0.0636536\n",
      "[37]\ttrain-mlogloss:0.0481298\ttest-mlogloss:0.059572\n",
      "[38]\ttrain-mlogloss:0.0444014\ttest-mlogloss:0.0558568\n",
      "[39]\ttrain-mlogloss:0.040983\ttest-mlogloss:0.0524576\n",
      "[40]\ttrain-mlogloss:0.0378824\ttest-mlogloss:0.0493454\n",
      "[41]\ttrain-mlogloss:0.0350222\ttest-mlogloss:0.0464918\n",
      "[42]\ttrain-mlogloss:0.0324092\ttest-mlogloss:0.0438616\n",
      "[43]\ttrain-mlogloss:0.0300044\ttest-mlogloss:0.0414846\n",
      "[44]\ttrain-mlogloss:0.0278076\ttest-mlogloss:0.0392632\n",
      "[45]\ttrain-mlogloss:0.0257584\ttest-mlogloss:0.0371934\n",
      "[46]\ttrain-mlogloss:0.0239224\ttest-mlogloss:0.035337\n",
      "[47]\ttrain-mlogloss:0.0222096\ttest-mlogloss:0.033614\n",
      "[48]\ttrain-mlogloss:0.0206572\ttest-mlogloss:0.0320746\n",
      "[49]\ttrain-mlogloss:0.019218\ttest-mlogloss:0.030575\n",
      "[50]\ttrain-mlogloss:0.017896\ttest-mlogloss:0.0293012\n",
      "[51]\ttrain-mlogloss:0.0167026\ttest-mlogloss:0.0280902\n",
      "[52]\ttrain-mlogloss:0.0156094\ttest-mlogloss:0.0269758\n",
      "[53]\ttrain-mlogloss:0.0145934\ttest-mlogloss:0.0259872\n",
      "[54]\ttrain-mlogloss:0.0136442\ttest-mlogloss:0.0250218\n",
      "[55]\ttrain-mlogloss:0.0127728\ttest-mlogloss:0.024157\n",
      "[56]\ttrain-mlogloss:0.0119818\ttest-mlogloss:0.023332\n",
      "[57]\ttrain-mlogloss:0.0112364\ttest-mlogloss:0.022581\n",
      "[58]\ttrain-mlogloss:0.0105514\ttest-mlogloss:0.0218462\n",
      "[59]\ttrain-mlogloss:0.0099186\ttest-mlogloss:0.0211968\n",
      "[60]\ttrain-mlogloss:0.009325\ttest-mlogloss:0.0205774\n",
      "[61]\ttrain-mlogloss:0.0087778\ttest-mlogloss:0.0200224\n",
      "[62]\ttrain-mlogloss:0.0082842\ttest-mlogloss:0.0195268\n",
      "[63]\ttrain-mlogloss:0.0078128\ttest-mlogloss:0.0190754\n",
      "[64]\ttrain-mlogloss:0.0073802\ttest-mlogloss:0.0186268\n",
      "[65]\ttrain-mlogloss:0.0069768\ttest-mlogloss:0.0181984\n",
      "[66]\ttrain-mlogloss:0.0066008\ttest-mlogloss:0.0177846\n",
      "[67]\ttrain-mlogloss:0.006257\ttest-mlogloss:0.0174004\n",
      "[68]\ttrain-mlogloss:0.0059366\ttest-mlogloss:0.0170676\n",
      "[69]\ttrain-mlogloss:0.0056428\ttest-mlogloss:0.0167378\n",
      "[70]\ttrain-mlogloss:0.0053696\ttest-mlogloss:0.0164686\n",
      "[71]\ttrain-mlogloss:0.0051094\ttest-mlogloss:0.0161866\n",
      "[72]\ttrain-mlogloss:0.0048678\ttest-mlogloss:0.0159112\n",
      "[73]\ttrain-mlogloss:0.0046422\ttest-mlogloss:0.015705\n",
      "[74]\ttrain-mlogloss:0.0044312\ttest-mlogloss:0.0154742\n",
      "[75]\ttrain-mlogloss:0.0042336\ttest-mlogloss:0.0153062\n",
      "[76]\ttrain-mlogloss:0.0040502\ttest-mlogloss:0.0151152\n",
      "[77]\ttrain-mlogloss:0.0038778\ttest-mlogloss:0.0149332\n",
      "[78]\ttrain-mlogloss:0.0037234\ttest-mlogloss:0.0147668\n",
      "[79]\ttrain-mlogloss:0.0035712\ttest-mlogloss:0.0146506\n",
      "[80]\ttrain-mlogloss:0.0034276\ttest-mlogloss:0.0144986\n",
      "[81]\ttrain-mlogloss:0.0032926\ttest-mlogloss:0.0143654\n",
      "[82]\ttrain-mlogloss:0.0031656\ttest-mlogloss:0.0142268\n",
      "[83]\ttrain-mlogloss:0.0030446\ttest-mlogloss:0.014098\n",
      "[84]\ttrain-mlogloss:0.0029356\ttest-mlogloss:0.0139906\n",
      "[85]\ttrain-mlogloss:0.002832\ttest-mlogloss:0.0138708\n",
      "[86]\ttrain-mlogloss:0.0027334\ttest-mlogloss:0.0137294\n",
      "[87]\ttrain-mlogloss:0.0026368\ttest-mlogloss:0.0136462\n",
      "[88]\ttrain-mlogloss:0.00255\ttest-mlogloss:0.013585\n",
      "[89]\ttrain-mlogloss:0.00247\ttest-mlogloss:0.0135216\n",
      "[90]\ttrain-mlogloss:0.0023868\ttest-mlogloss:0.013421\n",
      "[91]\ttrain-mlogloss:0.002311\ttest-mlogloss:0.0133246\n",
      "[92]\ttrain-mlogloss:0.0022434\ttest-mlogloss:0.0132806\n",
      "[93]\ttrain-mlogloss:0.002179\ttest-mlogloss:0.0132082\n",
      "[94]\ttrain-mlogloss:0.0021158\ttest-mlogloss:0.013152\n",
      "[95]\ttrain-mlogloss:0.0020554\ttest-mlogloss:0.0130918\n",
      "[96]\ttrain-mlogloss:0.0019962\ttest-mlogloss:0.013023\n",
      "[97]\ttrain-mlogloss:0.0019434\ttest-mlogloss:0.0129558\n",
      "[98]\ttrain-mlogloss:0.0018948\ttest-mlogloss:0.0129162\n",
      "[99]\ttrain-mlogloss:0.0018478\ttest-mlogloss:0.0128174\n",
      "[100]\ttrain-mlogloss:0.0018018\ttest-mlogloss:0.0127708\n",
      "[101]\ttrain-mlogloss:0.0017586\ttest-mlogloss:0.012749\n",
      "[102]\ttrain-mlogloss:0.0017162\ttest-mlogloss:0.0127048\n",
      "[103]\ttrain-mlogloss:0.0016778\ttest-mlogloss:0.0126632\n",
      "[104]\ttrain-mlogloss:0.0016422\ttest-mlogloss:0.0126098\n",
      "[105]\ttrain-mlogloss:0.0016058\ttest-mlogloss:0.0125782\n",
      "[106]\ttrain-mlogloss:0.0015714\ttest-mlogloss:0.0125182\n",
      "[107]\ttrain-mlogloss:0.0015404\ttest-mlogloss:0.0124904\n",
      "[108]\ttrain-mlogloss:0.0015104\ttest-mlogloss:0.0124604\n",
      "[109]\ttrain-mlogloss:0.0014794\ttest-mlogloss:0.0124414\n",
      "[110]\ttrain-mlogloss:0.0014518\ttest-mlogloss:0.0123952\n",
      "[111]\ttrain-mlogloss:0.0014254\ttest-mlogloss:0.01235\n",
      "[112]\ttrain-mlogloss:0.0013998\ttest-mlogloss:0.012323\n",
      "[113]\ttrain-mlogloss:0.0013762\ttest-mlogloss:0.0123018\n",
      "[114]\ttrain-mlogloss:0.001353\ttest-mlogloss:0.0122972\n",
      "[115]\ttrain-mlogloss:0.0013308\ttest-mlogloss:0.0122798\n",
      "[116]\ttrain-mlogloss:0.0013098\ttest-mlogloss:0.0122368\n",
      "[117]\ttrain-mlogloss:0.0012896\ttest-mlogloss:0.0122108\n",
      "[118]\ttrain-mlogloss:0.0012702\ttest-mlogloss:0.0121846\n",
      "[119]\ttrain-mlogloss:0.001252\ttest-mlogloss:0.012162\n",
      "[120]\ttrain-mlogloss:0.0012346\ttest-mlogloss:0.0121314\n",
      "[121]\ttrain-mlogloss:0.001218\ttest-mlogloss:0.0121128\n",
      "[122]\ttrain-mlogloss:0.0012024\ttest-mlogloss:0.0120896\n",
      "[123]\ttrain-mlogloss:0.0011862\ttest-mlogloss:0.0120588\n",
      "[124]\ttrain-mlogloss:0.0011716\ttest-mlogloss:0.0120398\n",
      "[125]\ttrain-mlogloss:0.0011572\ttest-mlogloss:0.0120286\n",
      "[126]\ttrain-mlogloss:0.0011442\ttest-mlogloss:0.0120156\n",
      "[127]\ttrain-mlogloss:0.0011298\ttest-mlogloss:0.0120074\n",
      "[128]\ttrain-mlogloss:0.0011182\ttest-mlogloss:0.0119892\n",
      "[129]\ttrain-mlogloss:0.0011066\ttest-mlogloss:0.011974\n",
      "[130]\ttrain-mlogloss:0.0010946\ttest-mlogloss:0.0119498\n",
      "[131]\ttrain-mlogloss:0.0010834\ttest-mlogloss:0.0119386\n",
      "[132]\ttrain-mlogloss:0.0010734\ttest-mlogloss:0.0119238\n",
      "[133]\ttrain-mlogloss:0.0010622\ttest-mlogloss:0.0118992\n",
      "[134]\ttrain-mlogloss:0.0010522\ttest-mlogloss:0.0118922\n",
      "[135]\ttrain-mlogloss:0.0010426\ttest-mlogloss:0.0118646\n",
      "[136]\ttrain-mlogloss:0.0010326\ttest-mlogloss:0.011855\n",
      "[137]\ttrain-mlogloss:0.0010238\ttest-mlogloss:0.0118442\n",
      "[138]\ttrain-mlogloss:0.0010154\ttest-mlogloss:0.0118318\n",
      "[139]\ttrain-mlogloss:0.0010082\ttest-mlogloss:0.011823\n",
      "[140]\ttrain-mlogloss:0.0010006\ttest-mlogloss:0.0118186\n",
      "[141]\ttrain-mlogloss:0.0009926\ttest-mlogloss:0.0118048\n",
      "[142]\ttrain-mlogloss:0.0009846\ttest-mlogloss:0.0118066\n",
      "[143]\ttrain-mlogloss:0.000977\ttest-mlogloss:0.0117822\n",
      "[144]\ttrain-mlogloss:0.0009704\ttest-mlogloss:0.0117764\n",
      "[145]\ttrain-mlogloss:0.0009632\ttest-mlogloss:0.0117528\n",
      "[146]\ttrain-mlogloss:0.0009562\ttest-mlogloss:0.0117434\n",
      "[147]\ttrain-mlogloss:0.0009488\ttest-mlogloss:0.0117286\n",
      "[148]\ttrain-mlogloss:0.000942\ttest-mlogloss:0.011711\n",
      "[149]\ttrain-mlogloss:0.0009356\ttest-mlogloss:0.0116968\n",
      "[150]\ttrain-mlogloss:0.0009296\ttest-mlogloss:0.0116912\n",
      "[151]\ttrain-mlogloss:0.0009242\ttest-mlogloss:0.0116972\n",
      "[152]\ttrain-mlogloss:0.0009184\ttest-mlogloss:0.0116776\n",
      "[153]\ttrain-mlogloss:0.0009124\ttest-mlogloss:0.0116718\n",
      "[154]\ttrain-mlogloss:0.0009066\ttest-mlogloss:0.0116688\n",
      "[155]\ttrain-mlogloss:0.0009014\ttest-mlogloss:0.0116484\n",
      "[156]\ttrain-mlogloss:0.0008962\ttest-mlogloss:0.0116278\n",
      "[157]\ttrain-mlogloss:0.000891\ttest-mlogloss:0.0116194\n",
      "[158]\ttrain-mlogloss:0.0008866\ttest-mlogloss:0.0116246\n",
      "[159]\ttrain-mlogloss:0.0008816\ttest-mlogloss:0.0115992\n",
      "[160]\ttrain-mlogloss:0.000877\ttest-mlogloss:0.011592\n",
      "[161]\ttrain-mlogloss:0.0008722\ttest-mlogloss:0.0115806\n",
      "[162]\ttrain-mlogloss:0.000868\ttest-mlogloss:0.0115732\n",
      "[163]\ttrain-mlogloss:0.0008634\ttest-mlogloss:0.0115556\n",
      "[164]\ttrain-mlogloss:0.000859\ttest-mlogloss:0.0115404\n",
      "[165]\ttrain-mlogloss:0.0008546\ttest-mlogloss:0.0115254\n",
      "[166]\ttrain-mlogloss:0.0008504\ttest-mlogloss:0.0115208\n",
      "[167]\ttrain-mlogloss:0.0008462\ttest-mlogloss:0.0115168\n",
      "[168]\ttrain-mlogloss:0.0008422\ttest-mlogloss:0.0115186\n",
      "[169]\ttrain-mlogloss:0.0008384\ttest-mlogloss:0.0115144\n",
      "[170]\ttrain-mlogloss:0.0008344\ttest-mlogloss:0.011527\n",
      "[171]\ttrain-mlogloss:0.0008302\ttest-mlogloss:0.0115136\n",
      "[172]\ttrain-mlogloss:0.0008268\ttest-mlogloss:0.0115038\n",
      "[173]\ttrain-mlogloss:0.0008226\ttest-mlogloss:0.0114924\n",
      "[174]\ttrain-mlogloss:0.0008194\ttest-mlogloss:0.0114814\n",
      "[175]\ttrain-mlogloss:0.0008154\ttest-mlogloss:0.0114736\n",
      "[176]\ttrain-mlogloss:0.0008122\ttest-mlogloss:0.011468\n",
      "[177]\ttrain-mlogloss:0.0008086\ttest-mlogloss:0.0114672\n",
      "[178]\ttrain-mlogloss:0.0008058\ttest-mlogloss:0.0114736\n",
      "[179]\ttrain-mlogloss:0.000802\ttest-mlogloss:0.0114792\n",
      "[180]\ttrain-mlogloss:0.0007988\ttest-mlogloss:0.0114838\n",
      "Stopping. Best iteration:\n",
      "[177]\ttrain-mlogloss:0.0008086+8.73155e-06\ttest-mlogloss:0.0114672+0.00426313\n",
      "\n",
      "     test-mlogloss-mean  test-mlogloss-std  train-mlogloss-mean  \\\n",
      "0              1.853504           0.004104             1.851365   \n",
      "1              1.568252           0.004606             1.564656   \n",
      "2              1.355979           0.004992             1.351233   \n",
      "3              1.189052           0.004722             1.183563   \n",
      "4              1.052214           0.005157             1.045997   \n",
      "5              0.936988           0.005105             0.930168   \n",
      "6              0.838819           0.005036             0.831311   \n",
      "7              0.753296           0.005014             0.745389   \n",
      "8              0.678771           0.005059             0.670433   \n",
      "9              0.613264           0.004996             0.604452   \n",
      "10             0.554717           0.004847             0.545726   \n",
      "11             0.502866           0.004744             0.493578   \n",
      "12             0.456610           0.004558             0.446997   \n",
      "13             0.415450           0.004548             0.405531   \n",
      "14             0.378257           0.004371             0.368166   \n",
      "15             0.344909           0.004446             0.334641   \n",
      "16             0.314962           0.004359             0.304485   \n",
      "17             0.288005           0.004246             0.277244   \n",
      "18             0.263593           0.004245             0.252693   \n",
      "19             0.241274           0.004231             0.230314   \n",
      "20             0.221219           0.004224             0.210099   \n",
      "21             0.203076           0.004072             0.191829   \n",
      "22             0.186514           0.003975             0.175200   \n",
      "23             0.171401           0.004014             0.160057   \n",
      "24             0.157687           0.004012             0.146336   \n",
      "25             0.145241           0.003980             0.133819   \n",
      "26             0.134135           0.003938             0.122552   \n",
      "27             0.123795           0.003838             0.112217   \n",
      "28             0.114436           0.003864             0.102864   \n",
      "29             0.105902           0.003827             0.094283   \n",
      "..                  ...                ...                  ...   \n",
      "148            0.011711           0.004275             0.000942   \n",
      "149            0.011697           0.004270             0.000936   \n",
      "150            0.011691           0.004278             0.000930   \n",
      "151            0.011697           0.004280             0.000924   \n",
      "152            0.011678           0.004274             0.000918   \n",
      "153            0.011672           0.004269             0.000912   \n",
      "154            0.011669           0.004274             0.000907   \n",
      "155            0.011648           0.004268             0.000901   \n",
      "156            0.011628           0.004265             0.000896   \n",
      "157            0.011619           0.004271             0.000891   \n",
      "158            0.011625           0.004279             0.000887   \n",
      "159            0.011599           0.004272             0.000882   \n",
      "160            0.011592           0.004270             0.000877   \n",
      "161            0.011581           0.004273             0.000872   \n",
      "162            0.011573           0.004271             0.000868   \n",
      "163            0.011556           0.004268             0.000863   \n",
      "164            0.011540           0.004267             0.000859   \n",
      "165            0.011525           0.004262             0.000855   \n",
      "166            0.011521           0.004265             0.000850   \n",
      "167            0.011517           0.004269             0.000846   \n",
      "168            0.011519           0.004272             0.000842   \n",
      "169            0.011514           0.004269             0.000838   \n",
      "170            0.011527           0.004263             0.000834   \n",
      "171            0.011514           0.004259             0.000830   \n",
      "172            0.011504           0.004268             0.000827   \n",
      "173            0.011492           0.004260             0.000823   \n",
      "174            0.011481           0.004258             0.000819   \n",
      "175            0.011474           0.004263             0.000815   \n",
      "176            0.011468           0.004263             0.000812   \n",
      "177            0.011467           0.004263             0.000809   \n",
      "\n",
      "     train-mlogloss-std  \n",
      "0              0.003608  \n",
      "1              0.002992  \n",
      "2              0.002120  \n",
      "3              0.002065  \n",
      "4              0.001997  \n",
      "5              0.002145  \n",
      "6              0.002012  \n",
      "7              0.001812  \n",
      "8              0.001589  \n",
      "9              0.001802  \n",
      "10             0.001728  \n",
      "11             0.001586  \n",
      "12             0.001473  \n",
      "13             0.001500  \n",
      "14             0.001346  \n",
      "15             0.001302  \n",
      "16             0.001110  \n",
      "17             0.001102  \n",
      "18             0.000877  \n",
      "19             0.000946  \n",
      "20             0.000828  \n",
      "21             0.000730  \n",
      "22             0.000653  \n",
      "23             0.000563  \n",
      "24             0.000531  \n",
      "25             0.000490  \n",
      "26             0.000533  \n",
      "27             0.000543  \n",
      "28             0.000499  \n",
      "29             0.000426  \n",
      "..                  ...  \n",
      "148            0.000014  \n",
      "149            0.000013  \n",
      "150            0.000013  \n",
      "151            0.000012  \n",
      "152            0.000013  \n",
      "153            0.000012  \n",
      "154            0.000012  \n",
      "155            0.000011  \n",
      "156            0.000011  \n",
      "157            0.000011  \n",
      "158            0.000011  \n",
      "159            0.000012  \n",
      "160            0.000012  \n",
      "161            0.000011  \n",
      "162            0.000011  \n",
      "163            0.000011  \n",
      "164            0.000011  \n",
      "165            0.000011  \n",
      "166            0.000011  \n",
      "167            0.000011  \n",
      "168            0.000011  \n",
      "169            0.000010  \n",
      "170            0.000010  \n",
      "171            0.000010  \n",
      "172            0.000010  \n",
      "173            0.000010  \n",
      "174            0.000009  \n",
      "175            0.000009  \n",
      "176            0.000009  \n",
      "177            0.000009  \n",
      "\n",
      "[178 rows x 4 columns]\n",
      "\n",
      "Model Report\n",
      "Accuracy : 1\n",
      "Log Loss Score (Train): 0.000666\n"
     ]
    }
   ],
   "source": [
    "# re-calibrate the number of boosting rounds for the updated parameter\n",
    "xgb2 = XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=3,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'multi:softprob',\n",
    " num_class = 10,\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "modelfit(xgb2, train, predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/xgboost/python-package/xgboost/sklearn.py:161: DeprecationWarning: The seed parameter is deprecated as of version .6.Please use random_state instead.seed is deprecated.\n",
      "  'seed is deprecated.', DeprecationWarning)\n",
      "/home/peter/xgboost/python-package/xgboost/sklearn.py:171: DeprecationWarning: The nthread parameter is deprecated as of version .6.Please use n_jobs instead.nthread is deprecated.\n",
      "  'nthread is deprecated.', DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.99687, std: 0.00098, params: {'colsample_bytree': 0.6, 'subsample': 0.6},\n",
       "  mean: 0.99706, std: 0.00055, params: {'colsample_bytree': 0.6, 'subsample': 0.7},\n",
       "  mean: 0.99706, std: 0.00055, params: {'colsample_bytree': 0.6, 'subsample': 0.8},\n",
       "  mean: 0.99724, std: 0.00058, params: {'colsample_bytree': 0.6, 'subsample': 0.9},\n",
       "  mean: 0.99706, std: 0.00095, params: {'colsample_bytree': 0.7, 'subsample': 0.6},\n",
       "  mean: 0.99696, std: 0.00075, params: {'colsample_bytree': 0.7, 'subsample': 0.7},\n",
       "  mean: 0.99715, std: 0.00054, params: {'colsample_bytree': 0.7, 'subsample': 0.8},\n",
       "  mean: 0.99696, std: 0.00075, params: {'colsample_bytree': 0.7, 'subsample': 0.9},\n",
       "  mean: 0.99660, std: 0.00095, params: {'colsample_bytree': 0.8, 'subsample': 0.6},\n",
       "  mean: 0.99678, std: 0.00087, params: {'colsample_bytree': 0.8, 'subsample': 0.7},\n",
       "  mean: 0.99696, std: 0.00069, params: {'colsample_bytree': 0.8, 'subsample': 0.8},\n",
       "  mean: 0.99696, std: 0.00047, params: {'colsample_bytree': 0.8, 'subsample': 0.9},\n",
       "  mean: 0.99678, std: 0.00082, params: {'colsample_bytree': 0.9, 'subsample': 0.6},\n",
       "  mean: 0.99660, std: 0.00080, params: {'colsample_bytree': 0.9, 'subsample': 0.7},\n",
       "  mean: 0.99706, std: 0.00069, params: {'colsample_bytree': 0.9, 'subsample': 0.8},\n",
       "  mean: 0.99715, std: 0.00034, params: {'colsample_bytree': 0.9, 'subsample': 0.9}],\n",
       " {'colsample_bytree': 0.6, 'subsample': 0.9},\n",
       " 0.9972401047879668)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tune subsample and colsample_bytree\n",
    "predictors = [x for x in train.columns if x not in target]\n",
    "param_test4 = {\n",
    " 'subsample':[i/10.0 for i in range(6,10)],\n",
    " 'colsample_bytree':[i/10.0 for i in range(6,10)]\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators= 177,max_depth=5,\n",
    " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'multi:softprob', num_class = 10, nthread=4, scale_pos_weight=1, seed=27), \n",
    " param_grid = param_test4 , n_jobs=1,iid=False, cv=5)\n",
    "gsearch1.fit(train[predictors],train[target])\n",
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/xgboost/python-package/xgboost/sklearn.py:161: DeprecationWarning: The seed parameter is deprecated as of version .6.Please use random_state instead.seed is deprecated.\n",
      "  'seed is deprecated.', DeprecationWarning)\n",
      "/home/peter/xgboost/python-package/xgboost/sklearn.py:171: DeprecationWarning: The nthread parameter is deprecated as of version .6.Please use n_jobs instead.nthread is deprecated.\n",
      "  'nthread is deprecated.', DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.99678, std: 0.00109, params: {'colsample_bytree': 0.85, 'subsample': 0.55},\n",
       "  mean: 0.99650, std: 0.00119, params: {'colsample_bytree': 0.85, 'subsample': 0.6},\n",
       "  mean: 0.99678, std: 0.00087, params: {'colsample_bytree': 0.85, 'subsample': 0.65},\n",
       "  mean: 0.99678, std: 0.00082, params: {'colsample_bytree': 0.9, 'subsample': 0.55},\n",
       "  mean: 0.99678, std: 0.00082, params: {'colsample_bytree': 0.9, 'subsample': 0.6},\n",
       "  mean: 0.99687, std: 0.00084, params: {'colsample_bytree': 0.9, 'subsample': 0.65},\n",
       "  mean: 0.99660, std: 0.00107, params: {'colsample_bytree': 0.95, 'subsample': 0.55},\n",
       "  mean: 0.99678, std: 0.00077, params: {'colsample_bytree': 0.95, 'subsample': 0.6},\n",
       "  mean: 0.99669, std: 0.00089, params: {'colsample_bytree': 0.95, 'subsample': 0.65}],\n",
       " {'colsample_bytree': 0.9, 'subsample': 0.65},\n",
       " 0.9968715268394035)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tune subsample and colsample_bytree again\n",
    "predictors = [x for x in train.columns if x not in target]\n",
    "param_test5 = {\n",
    " 'subsample':[i/100.0 for i in range(55,70,5)],\n",
    " 'colsample_bytree':[i/100.0 for i in range(85,100,5)]\n",
    "}\n",
    "gsearch5 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators= 177,max_depth=5,\n",
    " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'multi:softprob', num_class = 10, nthread=4, scale_pos_weight=1, seed=27), \n",
    " param_grid = param_test5 , n_jobs=1,iid=False, cv=5)\n",
    "gsearch5.fit(train[predictors],train[target])\n",
    "gsearch5.grid_scores_, gsearch5.best_params_, gsearch5.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/xgboost/python-package/xgboost/sklearn.py:161: DeprecationWarning: The seed parameter is deprecated as of version .6.Please use random_state instead.seed is deprecated.\n",
      "  'seed is deprecated.', DeprecationWarning)\n",
      "/home/peter/xgboost/python-package/xgboost/sklearn.py:171: DeprecationWarning: The nthread parameter is deprecated as of version .6.Please use n_jobs instead.nthread is deprecated.\n",
      "  'nthread is deprecated.', DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.99687, std: 0.00084, params: {'reg_alpha': 1e-05},\n",
       "  mean: 0.99678, std: 0.00101, params: {'reg_alpha': 0.01},\n",
       "  mean: 0.99669, std: 0.00102, params: {'reg_alpha': 0.1},\n",
       "  mean: 0.99660, std: 0.00103, params: {'reg_alpha': 1},\n",
       "  mean: 0.98859, std: 0.00158, params: {'reg_alpha': 100}],\n",
       " {'reg_alpha': 1e-05},\n",
       " 0.9968715268394035)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tuning Regularization Parameters\n",
    "predictors = [x for x in train.columns if x not in target]\n",
    "param_test5 = {\n",
    " 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n",
    "}\n",
    "gsearch5 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators= 177,max_depth=5,\n",
    " min_child_weight=1, gamma=0, subsample=0.65, colsample_bytree=0.9,\n",
    " objective= 'multi:softprob', num_class = 10, nthread=4, scale_pos_weight=1, seed=27), \n",
    " param_grid = param_test5 , n_jobs=1,iid=False, cv=5)\n",
    "gsearch5.fit(train[predictors],train[target])\n",
    "gsearch5.grid_scores_, gsearch5.best_params_, gsearch5.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/xgboost/python-package/xgboost/sklearn.py:161: DeprecationWarning: The seed parameter is deprecated as of version .6.Please use random_state instead.seed is deprecated.\n",
      "  'seed is deprecated.', DeprecationWarning)\n",
      "/home/peter/xgboost/python-package/xgboost/sklearn.py:171: DeprecationWarning: The nthread parameter is deprecated as of version .6.Please use n_jobs instead.nthread is deprecated.\n",
      "  'nthread is deprecated.', DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.99687, std: 0.00084, params: {'reg_alpha': 0},\n",
       "  mean: 0.99687, std: 0.00084, params: {'reg_alpha': 1e-06},\n",
       "  mean: 0.99678, std: 0.00101, params: {'reg_alpha': 5e-06},\n",
       "  mean: 0.99687, std: 0.00084, params: {'reg_alpha': 1e-05},\n",
       "  mean: 0.99687, std: 0.00084, params: {'reg_alpha': 5e-05}],\n",
       " {'reg_alpha': 0},\n",
       " 0.9968715268394035)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tuning Regularization Parameters\n",
    "predictors = [x for x in train.columns if x not in target]\n",
    "param_test5 = {\n",
    " 'reg_alpha':[0, 1e-6, 5e-6, 1e-5, 5e-5]\n",
    "}\n",
    "gsearch5 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators= 177,max_depth=5,\n",
    " min_child_weight=1, gamma=0, subsample=0.65, colsample_bytree=0.9,\n",
    " objective= 'multi:softprob', num_class = 10, nthread=4, scale_pos_weight=1, seed=27), \n",
    " param_grid = param_test5 , n_jobs=1,iid=False, cv=5)\n",
    "gsearch5.fit(train[predictors],train[target])\n",
    "gsearch5.grid_scores_, gsearch5.best_params_, gsearch5.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.85114\ttest-mlogloss:1.85287\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 3 rounds.\n",
      "[1]\ttrain-mlogloss:1.56536\ttest-mlogloss:1.56829\n",
      "[2]\ttrain-mlogloss:1.35198\ttest-mlogloss:1.35577\n",
      "[3]\ttrain-mlogloss:1.18461\ttest-mlogloss:1.18935\n",
      "[4]\ttrain-mlogloss:1.047\ttest-mlogloss:1.05254\n",
      "[5]\ttrain-mlogloss:0.931335\ttest-mlogloss:0.937618\n",
      "[6]\ttrain-mlogloss:0.832657\ttest-mlogloss:0.83939\n",
      "[7]\ttrain-mlogloss:0.74702\ttest-mlogloss:0.754439\n",
      "[8]\ttrain-mlogloss:0.672019\ttest-mlogloss:0.679971\n",
      "[9]\ttrain-mlogloss:0.606046\ttest-mlogloss:0.614451\n",
      "[10]\ttrain-mlogloss:0.547283\ttest-mlogloss:0.55602\n",
      "[11]\ttrain-mlogloss:0.495066\ttest-mlogloss:0.504015\n",
      "[12]\ttrain-mlogloss:0.448499\ttest-mlogloss:0.457711\n",
      "[13]\ttrain-mlogloss:0.406859\ttest-mlogloss:0.416326\n",
      "[14]\ttrain-mlogloss:0.369493\ttest-mlogloss:0.379201\n",
      "[15]\ttrain-mlogloss:0.335813\ttest-mlogloss:0.34568\n",
      "[16]\ttrain-mlogloss:0.305589\ttest-mlogloss:0.315775\n",
      "[17]\ttrain-mlogloss:0.278399\ttest-mlogloss:0.288729\n",
      "[18]\ttrain-mlogloss:0.253815\ttest-mlogloss:0.264142\n",
      "[19]\ttrain-mlogloss:0.231422\ttest-mlogloss:0.241822\n",
      "[20]\ttrain-mlogloss:0.211256\ttest-mlogloss:0.221801\n",
      "[21]\ttrain-mlogloss:0.192886\ttest-mlogloss:0.20368\n",
      "[22]\ttrain-mlogloss:0.176176\ttest-mlogloss:0.187138\n",
      "[23]\ttrain-mlogloss:0.161025\ttest-mlogloss:0.172119\n",
      "[24]\ttrain-mlogloss:0.147298\ttest-mlogloss:0.158512\n",
      "[25]\ttrain-mlogloss:0.13478\ttest-mlogloss:0.146155\n",
      "[26]\ttrain-mlogloss:0.123479\ttest-mlogloss:0.134963\n",
      "[27]\ttrain-mlogloss:0.113134\ttest-mlogloss:0.124662\n",
      "[28]\ttrain-mlogloss:0.103752\ttest-mlogloss:0.115343\n",
      "[29]\ttrain-mlogloss:0.0951504\ttest-mlogloss:0.106811\n",
      "[30]\ttrain-mlogloss:0.0873476\ttest-mlogloss:0.0989768\n",
      "[31]\ttrain-mlogloss:0.080224\ttest-mlogloss:0.091835\n",
      "[32]\ttrain-mlogloss:0.0737148\ttest-mlogloss:0.0853756\n",
      "[33]\ttrain-mlogloss:0.0678034\ttest-mlogloss:0.0794844\n",
      "[34]\ttrain-mlogloss:0.062378\ttest-mlogloss:0.074005\n",
      "[35]\ttrain-mlogloss:0.0575036\ttest-mlogloss:0.0691312\n",
      "[36]\ttrain-mlogloss:0.0530314\ttest-mlogloss:0.064712\n",
      "[37]\ttrain-mlogloss:0.04895\ttest-mlogloss:0.0607324\n",
      "[38]\ttrain-mlogloss:0.045182\ttest-mlogloss:0.0569182\n",
      "[39]\ttrain-mlogloss:0.0417576\ttest-mlogloss:0.0535232\n",
      "[40]\ttrain-mlogloss:0.038611\ttest-mlogloss:0.0502962\n",
      "[41]\ttrain-mlogloss:0.0357604\ttest-mlogloss:0.0474762\n",
      "[42]\ttrain-mlogloss:0.0331328\ttest-mlogloss:0.0448236\n",
      "[43]\ttrain-mlogloss:0.0307164\ttest-mlogloss:0.0424098\n",
      "[44]\ttrain-mlogloss:0.0285532\ttest-mlogloss:0.0402866\n",
      "[45]\ttrain-mlogloss:0.026509\ttest-mlogloss:0.0382058\n",
      "[46]\ttrain-mlogloss:0.0246618\ttest-mlogloss:0.0363452\n",
      "[47]\ttrain-mlogloss:0.023001\ttest-mlogloss:0.0346438\n",
      "[48]\ttrain-mlogloss:0.02143\ttest-mlogloss:0.0330662\n",
      "[49]\ttrain-mlogloss:0.019984\ttest-mlogloss:0.0316196\n",
      "[50]\ttrain-mlogloss:0.0186508\ttest-mlogloss:0.0303094\n",
      "[51]\ttrain-mlogloss:0.0174588\ttest-mlogloss:0.0291292\n",
      "[52]\ttrain-mlogloss:0.0163416\ttest-mlogloss:0.0280126\n",
      "[53]\ttrain-mlogloss:0.0153096\ttest-mlogloss:0.0269888\n",
      "[54]\ttrain-mlogloss:0.0143472\ttest-mlogloss:0.0260626\n",
      "[55]\ttrain-mlogloss:0.0134782\ttest-mlogloss:0.025156\n",
      "[56]\ttrain-mlogloss:0.0126776\ttest-mlogloss:0.0243316\n",
      "[57]\ttrain-mlogloss:0.0119348\ttest-mlogloss:0.0236334\n",
      "[58]\ttrain-mlogloss:0.01125\ttest-mlogloss:0.0229444\n",
      "[59]\ttrain-mlogloss:0.0106078\ttest-mlogloss:0.0223114\n",
      "[60]\ttrain-mlogloss:0.0100086\ttest-mlogloss:0.0216688\n",
      "[61]\ttrain-mlogloss:0.0094524\ttest-mlogloss:0.021084\n",
      "[62]\ttrain-mlogloss:0.0089396\ttest-mlogloss:0.0206056\n",
      "[63]\ttrain-mlogloss:0.0084702\ttest-mlogloss:0.0201558\n",
      "[64]\ttrain-mlogloss:0.0080318\ttest-mlogloss:0.0196964\n",
      "[65]\ttrain-mlogloss:0.0076092\ttest-mlogloss:0.0192138\n",
      "[66]\ttrain-mlogloss:0.007218\ttest-mlogloss:0.0188312\n",
      "[67]\ttrain-mlogloss:0.0068728\ttest-mlogloss:0.018483\n",
      "[68]\ttrain-mlogloss:0.006546\ttest-mlogloss:0.0181618\n",
      "[69]\ttrain-mlogloss:0.0062362\ttest-mlogloss:0.0178712\n",
      "[70]\ttrain-mlogloss:0.0059484\ttest-mlogloss:0.0175812\n",
      "[71]\ttrain-mlogloss:0.0056866\ttest-mlogloss:0.0173266\n",
      "[72]\ttrain-mlogloss:0.0054288\ttest-mlogloss:0.0170874\n",
      "[73]\ttrain-mlogloss:0.005196\ttest-mlogloss:0.0168648\n",
      "[74]\ttrain-mlogloss:0.0049826\ttest-mlogloss:0.0165992\n",
      "[75]\ttrain-mlogloss:0.0047744\ttest-mlogloss:0.0164056\n",
      "[76]\ttrain-mlogloss:0.0045846\ttest-mlogloss:0.0162188\n",
      "[77]\ttrain-mlogloss:0.0044142\ttest-mlogloss:0.0160652\n",
      "[78]\ttrain-mlogloss:0.0042542\ttest-mlogloss:0.0159018\n",
      "[79]\ttrain-mlogloss:0.0040954\ttest-mlogloss:0.0157694\n",
      "[80]\ttrain-mlogloss:0.0039452\ttest-mlogloss:0.0156218\n",
      "[81]\ttrain-mlogloss:0.0038066\ttest-mlogloss:0.0154804\n",
      "[82]\ttrain-mlogloss:0.0036722\ttest-mlogloss:0.0153048\n",
      "[83]\ttrain-mlogloss:0.0035496\ttest-mlogloss:0.0151598\n",
      "[84]\ttrain-mlogloss:0.0034308\ttest-mlogloss:0.015044\n",
      "[85]\ttrain-mlogloss:0.0033164\ttest-mlogloss:0.0148876\n",
      "[86]\ttrain-mlogloss:0.0032108\ttest-mlogloss:0.014782\n",
      "[87]\ttrain-mlogloss:0.003109\ttest-mlogloss:0.0146832\n",
      "[88]\ttrain-mlogloss:0.0030098\ttest-mlogloss:0.0145858\n",
      "[89]\ttrain-mlogloss:0.002916\ttest-mlogloss:0.0144652\n",
      "[90]\ttrain-mlogloss:0.002831\ttest-mlogloss:0.014365\n",
      "[91]\ttrain-mlogloss:0.002747\ttest-mlogloss:0.0142578\n",
      "[92]\ttrain-mlogloss:0.0026712\ttest-mlogloss:0.0141836\n",
      "[93]\ttrain-mlogloss:0.0026002\ttest-mlogloss:0.014087\n",
      "[94]\ttrain-mlogloss:0.0025286\ttest-mlogloss:0.0140072\n",
      "[95]\ttrain-mlogloss:0.002464\ttest-mlogloss:0.013934\n",
      "[96]\ttrain-mlogloss:0.0023992\ttest-mlogloss:0.0138772\n",
      "[97]\ttrain-mlogloss:0.0023412\ttest-mlogloss:0.0138122\n",
      "[98]\ttrain-mlogloss:0.0022874\ttest-mlogloss:0.0137618\n",
      "[99]\ttrain-mlogloss:0.0022334\ttest-mlogloss:0.013681\n",
      "[100]\ttrain-mlogloss:0.0021818\ttest-mlogloss:0.0136348\n",
      "[101]\ttrain-mlogloss:0.0021312\ttest-mlogloss:0.0135674\n",
      "[102]\ttrain-mlogloss:0.002082\ttest-mlogloss:0.0135002\n",
      "[103]\ttrain-mlogloss:0.0020352\ttest-mlogloss:0.0134554\n",
      "[104]\ttrain-mlogloss:0.0019918\ttest-mlogloss:0.0134166\n",
      "[105]\ttrain-mlogloss:0.0019514\ttest-mlogloss:0.0133754\n",
      "[106]\ttrain-mlogloss:0.001915\ttest-mlogloss:0.0133024\n",
      "[107]\ttrain-mlogloss:0.0018784\ttest-mlogloss:0.0132604\n",
      "[108]\ttrain-mlogloss:0.0018444\ttest-mlogloss:0.0132324\n",
      "[109]\ttrain-mlogloss:0.0018102\ttest-mlogloss:0.013207\n",
      "[110]\ttrain-mlogloss:0.0017804\ttest-mlogloss:0.0131588\n",
      "[111]\ttrain-mlogloss:0.0017528\ttest-mlogloss:0.0131076\n",
      "[112]\ttrain-mlogloss:0.001725\ttest-mlogloss:0.0130624\n",
      "[113]\ttrain-mlogloss:0.0016992\ttest-mlogloss:0.0130286\n",
      "[114]\ttrain-mlogloss:0.0016718\ttest-mlogloss:0.0129876\n",
      "[115]\ttrain-mlogloss:0.0016458\ttest-mlogloss:0.0129572\n",
      "[116]\ttrain-mlogloss:0.0016198\ttest-mlogloss:0.0129338\n",
      "[117]\ttrain-mlogloss:0.0015946\ttest-mlogloss:0.0128874\n",
      "[118]\ttrain-mlogloss:0.0015718\ttest-mlogloss:0.0128426\n",
      "[119]\ttrain-mlogloss:0.0015516\ttest-mlogloss:0.0128246\n",
      "[120]\ttrain-mlogloss:0.0015302\ttest-mlogloss:0.0127758\n",
      "[121]\ttrain-mlogloss:0.0015096\ttest-mlogloss:0.0127434\n",
      "[122]\ttrain-mlogloss:0.0014908\ttest-mlogloss:0.0127324\n",
      "[123]\ttrain-mlogloss:0.0014702\ttest-mlogloss:0.0127312\n",
      "[124]\ttrain-mlogloss:0.001452\ttest-mlogloss:0.0127114\n",
      "[125]\ttrain-mlogloss:0.0014356\ttest-mlogloss:0.0126794\n",
      "[126]\ttrain-mlogloss:0.0014202\ttest-mlogloss:0.0126754\n",
      "[127]\ttrain-mlogloss:0.001404\ttest-mlogloss:0.0126532\n",
      "[128]\ttrain-mlogloss:0.0013882\ttest-mlogloss:0.012635\n",
      "[129]\ttrain-mlogloss:0.0013732\ttest-mlogloss:0.0126196\n",
      "[130]\ttrain-mlogloss:0.0013584\ttest-mlogloss:0.0125988\n",
      "[131]\ttrain-mlogloss:0.0013442\ttest-mlogloss:0.0125756\n",
      "[132]\ttrain-mlogloss:0.0013322\ttest-mlogloss:0.012572\n",
      "[133]\ttrain-mlogloss:0.001319\ttest-mlogloss:0.0125766\n",
      "[134]\ttrain-mlogloss:0.0013066\ttest-mlogloss:0.0125608\n",
      "[135]\ttrain-mlogloss:0.001294\ttest-mlogloss:0.0125416\n",
      "[136]\ttrain-mlogloss:0.001281\ttest-mlogloss:0.012531\n",
      "[137]\ttrain-mlogloss:0.00127\ttest-mlogloss:0.0124982\n",
      "[138]\ttrain-mlogloss:0.0012586\ttest-mlogloss:0.0124778\n",
      "[139]\ttrain-mlogloss:0.0012484\ttest-mlogloss:0.012458\n",
      "[140]\ttrain-mlogloss:0.0012386\ttest-mlogloss:0.0124224\n",
      "[141]\ttrain-mlogloss:0.0012276\ttest-mlogloss:0.0123992\n",
      "[142]\ttrain-mlogloss:0.0012182\ttest-mlogloss:0.0123912\n",
      "[143]\ttrain-mlogloss:0.0012082\ttest-mlogloss:0.0123682\n",
      "[144]\ttrain-mlogloss:0.0011994\ttest-mlogloss:0.0123464\n",
      "[145]\ttrain-mlogloss:0.0011898\ttest-mlogloss:0.0123188\n",
      "[146]\ttrain-mlogloss:0.0011814\ttest-mlogloss:0.0123132\n",
      "[147]\ttrain-mlogloss:0.0011722\ttest-mlogloss:0.012307\n",
      "[148]\ttrain-mlogloss:0.0011632\ttest-mlogloss:0.0123038\n",
      "[149]\ttrain-mlogloss:0.0011546\ttest-mlogloss:0.0122988\n",
      "[150]\ttrain-mlogloss:0.0011458\ttest-mlogloss:0.0122988\n",
      "[151]\ttrain-mlogloss:0.001138\ttest-mlogloss:0.0122904\n",
      "[152]\ttrain-mlogloss:0.0011304\ttest-mlogloss:0.0122736\n",
      "[153]\ttrain-mlogloss:0.001123\ttest-mlogloss:0.0122644\n",
      "[154]\ttrain-mlogloss:0.0011156\ttest-mlogloss:0.0122588\n",
      "[155]\ttrain-mlogloss:0.0011084\ttest-mlogloss:0.0122428\n",
      "[156]\ttrain-mlogloss:0.001102\ttest-mlogloss:0.0122388\n",
      "[157]\ttrain-mlogloss:0.001095\ttest-mlogloss:0.0122416\n",
      "[158]\ttrain-mlogloss:0.001088\ttest-mlogloss:0.0122312\n",
      "[159]\ttrain-mlogloss:0.0010818\ttest-mlogloss:0.0122162\n",
      "[160]\ttrain-mlogloss:0.0010764\ttest-mlogloss:0.012221\n",
      "[161]\ttrain-mlogloss:0.00107\ttest-mlogloss:0.012204\n",
      "[162]\ttrain-mlogloss:0.0010644\ttest-mlogloss:0.0122048\n",
      "[163]\ttrain-mlogloss:0.0010582\ttest-mlogloss:0.012201\n",
      "[164]\ttrain-mlogloss:0.001053\ttest-mlogloss:0.0121826\n",
      "[165]\ttrain-mlogloss:0.0010472\ttest-mlogloss:0.0121736\n",
      "[166]\ttrain-mlogloss:0.0010418\ttest-mlogloss:0.0121618\n",
      "[167]\ttrain-mlogloss:0.001037\ttest-mlogloss:0.0121534\n",
      "[168]\ttrain-mlogloss:0.0010316\ttest-mlogloss:0.0121334\n",
      "[169]\ttrain-mlogloss:0.0010266\ttest-mlogloss:0.0121438\n",
      "[170]\ttrain-mlogloss:0.0010212\ttest-mlogloss:0.0121478\n",
      "[171]\ttrain-mlogloss:0.0010156\ttest-mlogloss:0.0121384\n",
      "Stopping. Best iteration:\n",
      "[168]\ttrain-mlogloss:0.0010316+1.26744e-05\ttest-mlogloss:0.0121334+0.00432392\n",
      "\n",
      "     test-mlogloss-mean  test-mlogloss-std  train-mlogloss-mean  \\\n",
      "0              1.852870           0.003006             1.851142   \n",
      "1              1.568285           0.003214             1.565364   \n",
      "2              1.355769           0.003452             1.351985   \n",
      "3              1.189355           0.004538             1.184608   \n",
      "4              1.052536           0.004795             1.047001   \n",
      "5              0.937618           0.004849             0.931335   \n",
      "6              0.839390           0.004303             0.832657   \n",
      "7              0.754439           0.004457             0.747020   \n",
      "8              0.679971           0.004674             0.672019   \n",
      "9              0.614451           0.005011             0.606046   \n",
      "10             0.556020           0.004807             0.547283   \n",
      "11             0.504015           0.004815             0.495066   \n",
      "12             0.457711           0.004747             0.448499   \n",
      "13             0.416326           0.004829             0.406859   \n",
      "14             0.379201           0.004834             0.369493   \n",
      "15             0.345680           0.004889             0.335813   \n",
      "16             0.315775           0.004911             0.305589   \n",
      "17             0.288729           0.004862             0.278399   \n",
      "18             0.264142           0.004860             0.253815   \n",
      "19             0.241822           0.004802             0.231422   \n",
      "20             0.221801           0.004595             0.211256   \n",
      "21             0.203680           0.004543             0.192886   \n",
      "22             0.187138           0.004561             0.176176   \n",
      "23             0.172119           0.004533             0.161025   \n",
      "24             0.158512           0.004425             0.147298   \n",
      "25             0.146155           0.004404             0.134780   \n",
      "26             0.134963           0.004395             0.123479   \n",
      "27             0.124662           0.004292             0.113134   \n",
      "28             0.115343           0.004265             0.103752   \n",
      "29             0.106811           0.004308             0.095150   \n",
      "..                  ...                ...                  ...   \n",
      "139            0.012458           0.004289             0.001248   \n",
      "140            0.012422           0.004276             0.001239   \n",
      "141            0.012399           0.004282             0.001228   \n",
      "142            0.012391           0.004279             0.001218   \n",
      "143            0.012368           0.004278             0.001208   \n",
      "144            0.012346           0.004283             0.001199   \n",
      "145            0.012319           0.004275             0.001190   \n",
      "146            0.012313           0.004275             0.001181   \n",
      "147            0.012307           0.004284             0.001172   \n",
      "148            0.012304           0.004289             0.001163   \n",
      "149            0.012299           0.004289             0.001155   \n",
      "150            0.012299           0.004300             0.001146   \n",
      "151            0.012290           0.004299             0.001138   \n",
      "152            0.012274           0.004295             0.001130   \n",
      "153            0.012264           0.004287             0.001123   \n",
      "154            0.012259           0.004290             0.001116   \n",
      "155            0.012243           0.004288             0.001108   \n",
      "156            0.012239           0.004292             0.001102   \n",
      "157            0.012242           0.004298             0.001095   \n",
      "158            0.012231           0.004294             0.001088   \n",
      "159            0.012216           0.004304             0.001082   \n",
      "160            0.012221           0.004317             0.001076   \n",
      "161            0.012204           0.004318             0.001070   \n",
      "162            0.012205           0.004326             0.001064   \n",
      "163            0.012201           0.004321             0.001058   \n",
      "164            0.012183           0.004327             0.001053   \n",
      "165            0.012174           0.004328             0.001047   \n",
      "166            0.012162           0.004333             0.001042   \n",
      "167            0.012153           0.004332             0.001037   \n",
      "168            0.012133           0.004324             0.001032   \n",
      "\n",
      "     train-mlogloss-std  \n",
      "0              0.001654  \n",
      "1              0.002224  \n",
      "2              0.001980  \n",
      "3              0.002132  \n",
      "4              0.001736  \n",
      "5              0.001675  \n",
      "6              0.001297  \n",
      "7              0.001027  \n",
      "8              0.000834  \n",
      "9              0.000791  \n",
      "10             0.000713  \n",
      "11             0.000699  \n",
      "12             0.000596  \n",
      "13             0.000484  \n",
      "14             0.000521  \n",
      "15             0.000441  \n",
      "16             0.000403  \n",
      "17             0.000414  \n",
      "18             0.000381  \n",
      "19             0.000474  \n",
      "20             0.000571  \n",
      "21             0.000618  \n",
      "22             0.000587  \n",
      "23             0.000494  \n",
      "24             0.000511  \n",
      "25             0.000483  \n",
      "26             0.000505  \n",
      "27             0.000484  \n",
      "28             0.000462  \n",
      "29             0.000429  \n",
      "..                  ...  \n",
      "139            0.000014  \n",
      "140            0.000014  \n",
      "141            0.000014  \n",
      "142            0.000013  \n",
      "143            0.000012  \n",
      "144            0.000012  \n",
      "145            0.000013  \n",
      "146            0.000014  \n",
      "147            0.000014  \n",
      "148            0.000014  \n",
      "149            0.000014  \n",
      "150            0.000013  \n",
      "151            0.000013  \n",
      "152            0.000012  \n",
      "153            0.000012  \n",
      "154            0.000012  \n",
      "155            0.000013  \n",
      "156            0.000013  \n",
      "157            0.000013  \n",
      "158            0.000013  \n",
      "159            0.000013  \n",
      "160            0.000014  \n",
      "161            0.000013  \n",
      "162            0.000013  \n",
      "163            0.000013  \n",
      "164            0.000014  \n",
      "165            0.000014  \n",
      "166            0.000013  \n",
      "167            0.000013  \n",
      "168            0.000013  \n",
      "\n",
      "[169 rows x 4 columns]\n",
      "\n",
      "Model Report\n",
      "Accuracy : 1\n",
      "Log Loss Score (Train): 0.000860\n"
     ]
    }
   ],
   "source": [
    "# See the result with new parameter\n",
    "predictors = [x for x in train.columns if x not in target]\n",
    "xgb3 = XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=3,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.65,\n",
    " reg_alpha=0,\n",
    " colsample_bytree=0.9,\n",
    " objective= 'multi:softprob',\n",
    " num_class = 10,\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "modelfit(xgb3, train, predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:2.2561\ttest-mlogloss:2.25627\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 3 rounds.\n",
      "[1]\ttrain-mlogloss:2.21152\ttest-mlogloss:2.21191\n",
      "[2]\ttrain-mlogloss:2.16895\ttest-mlogloss:2.16951\n",
      "[3]\ttrain-mlogloss:2.12832\ttest-mlogloss:2.12915\n",
      "[4]\ttrain-mlogloss:2.08914\ttest-mlogloss:2.09015\n",
      "[5]\ttrain-mlogloss:2.0515\ttest-mlogloss:2.05273\n",
      "[6]\ttrain-mlogloss:2.01539\ttest-mlogloss:2.01684\n",
      "[7]\ttrain-mlogloss:1.98052\ttest-mlogloss:1.98216\n",
      "[8]\ttrain-mlogloss:1.94695\ttest-mlogloss:1.94869\n",
      "[9]\ttrain-mlogloss:1.91451\ttest-mlogloss:1.91639\n",
      "[10]\ttrain-mlogloss:1.88306\ttest-mlogloss:1.88508\n",
      "[11]\ttrain-mlogloss:1.85271\ttest-mlogloss:1.8549\n",
      "[12]\ttrain-mlogloss:1.82326\ttest-mlogloss:1.82562\n",
      "[13]\ttrain-mlogloss:1.79476\ttest-mlogloss:1.79727\n",
      "[14]\ttrain-mlogloss:1.76703\ttest-mlogloss:1.76966\n",
      "[15]\ttrain-mlogloss:1.74007\ttest-mlogloss:1.74283\n",
      "[16]\ttrain-mlogloss:1.71387\ttest-mlogloss:1.71674\n",
      "[17]\ttrain-mlogloss:1.68842\ttest-mlogloss:1.69143\n",
      "[18]\ttrain-mlogloss:1.66356\ttest-mlogloss:1.66665\n",
      "[19]\ttrain-mlogloss:1.63923\ttest-mlogloss:1.64246\n",
      "[20]\ttrain-mlogloss:1.61556\ttest-mlogloss:1.61887\n",
      "[21]\ttrain-mlogloss:1.59246\ttest-mlogloss:1.59592\n",
      "[22]\ttrain-mlogloss:1.57001\ttest-mlogloss:1.5735\n",
      "[23]\ttrain-mlogloss:1.54804\ttest-mlogloss:1.55168\n",
      "[24]\ttrain-mlogloss:1.52654\ttest-mlogloss:1.53031\n",
      "[25]\ttrain-mlogloss:1.50542\ttest-mlogloss:1.50929\n",
      "[26]\ttrain-mlogloss:1.48493\ttest-mlogloss:1.48896\n",
      "[27]\ttrain-mlogloss:1.46482\ttest-mlogloss:1.46897\n",
      "[28]\ttrain-mlogloss:1.44518\ttest-mlogloss:1.44942\n",
      "[29]\ttrain-mlogloss:1.42598\ttest-mlogloss:1.43033\n",
      "[30]\ttrain-mlogloss:1.40722\ttest-mlogloss:1.41169\n",
      "[31]\ttrain-mlogloss:1.38881\ttest-mlogloss:1.39333\n",
      "[32]\ttrain-mlogloss:1.37073\ttest-mlogloss:1.37535\n",
      "[33]\ttrain-mlogloss:1.35304\ttest-mlogloss:1.35771\n",
      "[34]\ttrain-mlogloss:1.33569\ttest-mlogloss:1.34042\n",
      "[35]\ttrain-mlogloss:1.31874\ttest-mlogloss:1.32359\n",
      "[36]\ttrain-mlogloss:1.30212\ttest-mlogloss:1.30709\n",
      "[37]\ttrain-mlogloss:1.28576\ttest-mlogloss:1.29078\n",
      "[38]\ttrain-mlogloss:1.26976\ttest-mlogloss:1.27484\n",
      "[39]\ttrain-mlogloss:1.25404\ttest-mlogloss:1.2592\n",
      "[40]\ttrain-mlogloss:1.23862\ttest-mlogloss:1.24386\n",
      "[41]\ttrain-mlogloss:1.22348\ttest-mlogloss:1.22881\n",
      "[42]\ttrain-mlogloss:1.20857\ttest-mlogloss:1.214\n",
      "[43]\ttrain-mlogloss:1.19401\ttest-mlogloss:1.19952\n",
      "[44]\ttrain-mlogloss:1.17973\ttest-mlogloss:1.18535\n",
      "[45]\ttrain-mlogloss:1.1656\ttest-mlogloss:1.17132\n",
      "[46]\ttrain-mlogloss:1.15178\ttest-mlogloss:1.1576\n",
      "[47]\ttrain-mlogloss:1.13815\ttest-mlogloss:1.14402\n",
      "[48]\ttrain-mlogloss:1.1247\ttest-mlogloss:1.13066\n",
      "[49]\ttrain-mlogloss:1.1115\ttest-mlogloss:1.11751\n",
      "[50]\ttrain-mlogloss:1.09854\ttest-mlogloss:1.10463\n",
      "[51]\ttrain-mlogloss:1.08574\ttest-mlogloss:1.09188\n",
      "[52]\ttrain-mlogloss:1.07317\ttest-mlogloss:1.07936\n",
      "[53]\ttrain-mlogloss:1.06082\ttest-mlogloss:1.06707\n",
      "[54]\ttrain-mlogloss:1.04864\ttest-mlogloss:1.05493\n",
      "[55]\ttrain-mlogloss:1.03667\ttest-mlogloss:1.04302\n",
      "[56]\ttrain-mlogloss:1.02487\ttest-mlogloss:1.03128\n",
      "[57]\ttrain-mlogloss:1.01328\ttest-mlogloss:1.01975\n",
      "[58]\ttrain-mlogloss:1.00185\ttest-mlogloss:1.00836\n",
      "[59]\ttrain-mlogloss:0.990592\ttest-mlogloss:0.997181\n",
      "[60]\ttrain-mlogloss:0.979502\ttest-mlogloss:0.986154\n",
      "[61]\ttrain-mlogloss:0.968601\ttest-mlogloss:0.975324\n",
      "[62]\ttrain-mlogloss:0.957829\ttest-mlogloss:0.964639\n",
      "[63]\ttrain-mlogloss:0.947227\ttest-mlogloss:0.954092\n",
      "[64]\ttrain-mlogloss:0.936761\ttest-mlogloss:0.943698\n",
      "[65]\ttrain-mlogloss:0.926438\ttest-mlogloss:0.933438\n",
      "[66]\ttrain-mlogloss:0.91626\ttest-mlogloss:0.923308\n",
      "[67]\ttrain-mlogloss:0.906262\ttest-mlogloss:0.913392\n",
      "[68]\ttrain-mlogloss:0.896371\ttest-mlogloss:0.903532\n",
      "[69]\ttrain-mlogloss:0.886635\ttest-mlogloss:0.893855\n",
      "[70]\ttrain-mlogloss:0.877071\ttest-mlogloss:0.884363\n",
      "[71]\ttrain-mlogloss:0.867613\ttest-mlogloss:0.87494\n",
      "[72]\ttrain-mlogloss:0.858273\ttest-mlogloss:0.865665\n",
      "[73]\ttrain-mlogloss:0.849053\ttest-mlogloss:0.856511\n",
      "[74]\ttrain-mlogloss:0.839985\ttest-mlogloss:0.847499\n",
      "[75]\ttrain-mlogloss:0.830998\ttest-mlogloss:0.838578\n",
      "[76]\ttrain-mlogloss:0.82214\ttest-mlogloss:0.829763\n",
      "[77]\ttrain-mlogloss:0.813415\ttest-mlogloss:0.821098\n",
      "[78]\ttrain-mlogloss:0.804847\ttest-mlogloss:0.812592\n",
      "[79]\ttrain-mlogloss:0.796355\ttest-mlogloss:0.804174\n",
      "[80]\ttrain-mlogloss:0.787955\ttest-mlogloss:0.795816\n",
      "[81]\ttrain-mlogloss:0.779684\ttest-mlogloss:0.787585\n",
      "[82]\ttrain-mlogloss:0.771523\ttest-mlogloss:0.779465\n",
      "[83]\ttrain-mlogloss:0.763429\ttest-mlogloss:0.771397\n",
      "[84]\ttrain-mlogloss:0.755473\ttest-mlogloss:0.763502\n",
      "[85]\ttrain-mlogloss:0.747598\ttest-mlogloss:0.755661\n",
      "[86]\ttrain-mlogloss:0.739807\ttest-mlogloss:0.747903\n",
      "[87]\ttrain-mlogloss:0.732158\ttest-mlogloss:0.740297\n",
      "[88]\ttrain-mlogloss:0.724626\ttest-mlogloss:0.732814\n",
      "[89]\ttrain-mlogloss:0.717137\ttest-mlogloss:0.725359\n",
      "[90]\ttrain-mlogloss:0.709741\ttest-mlogloss:0.717999\n",
      "[91]\ttrain-mlogloss:0.702435\ttest-mlogloss:0.710732\n",
      "[92]\ttrain-mlogloss:0.695198\ttest-mlogloss:0.703551\n",
      "[93]\ttrain-mlogloss:0.688058\ttest-mlogloss:0.69644\n",
      "[94]\ttrain-mlogloss:0.681018\ttest-mlogloss:0.689454\n",
      "[95]\ttrain-mlogloss:0.674069\ttest-mlogloss:0.682554\n",
      "[96]\ttrain-mlogloss:0.667218\ttest-mlogloss:0.675755\n",
      "[97]\ttrain-mlogloss:0.660425\ttest-mlogloss:0.668992\n",
      "[98]\ttrain-mlogloss:0.653714\ttest-mlogloss:0.662318\n",
      "[99]\ttrain-mlogloss:0.647083\ttest-mlogloss:0.655706\n",
      "[100]\ttrain-mlogloss:0.64051\ttest-mlogloss:0.64917\n",
      "[101]\ttrain-mlogloss:0.634005\ttest-mlogloss:0.642681\n",
      "[102]\ttrain-mlogloss:0.62759\ttest-mlogloss:0.636287\n",
      "[103]\ttrain-mlogloss:0.621266\ttest-mlogloss:0.62999\n",
      "[104]\ttrain-mlogloss:0.615003\ttest-mlogloss:0.623762\n",
      "[105]\ttrain-mlogloss:0.608808\ttest-mlogloss:0.617591\n",
      "[106]\ttrain-mlogloss:0.602699\ttest-mlogloss:0.611504\n",
      "[107]\ttrain-mlogloss:0.596657\ttest-mlogloss:0.605503\n",
      "[108]\ttrain-mlogloss:0.590689\ttest-mlogloss:0.59956\n",
      "[109]\ttrain-mlogloss:0.584783\ttest-mlogloss:0.593683\n",
      "[110]\ttrain-mlogloss:0.578942\ttest-mlogloss:0.587866\n",
      "[111]\ttrain-mlogloss:0.5732\ttest-mlogloss:0.582162\n",
      "[112]\ttrain-mlogloss:0.567498\ttest-mlogloss:0.576494\n",
      "[113]\ttrain-mlogloss:0.561864\ttest-mlogloss:0.570911\n",
      "[114]\ttrain-mlogloss:0.55626\ttest-mlogloss:0.565324\n",
      "[115]\ttrain-mlogloss:0.550767\ttest-mlogloss:0.559854\n",
      "[116]\ttrain-mlogloss:0.545308\ttest-mlogloss:0.554416\n",
      "[117]\ttrain-mlogloss:0.539933\ttest-mlogloss:0.549078\n",
      "[118]\ttrain-mlogloss:0.534632\ttest-mlogloss:0.543797\n",
      "[119]\ttrain-mlogloss:0.529357\ttest-mlogloss:0.538555\n",
      "[120]\ttrain-mlogloss:0.524127\ttest-mlogloss:0.533336\n",
      "[121]\ttrain-mlogloss:0.518975\ttest-mlogloss:0.528213\n",
      "[122]\ttrain-mlogloss:0.513877\ttest-mlogloss:0.523135\n",
      "[123]\ttrain-mlogloss:0.508851\ttest-mlogloss:0.518141\n",
      "[124]\ttrain-mlogloss:0.503876\ttest-mlogloss:0.513192\n",
      "[125]\ttrain-mlogloss:0.498952\ttest-mlogloss:0.508291\n",
      "[126]\ttrain-mlogloss:0.494072\ttest-mlogloss:0.50344\n",
      "[127]\ttrain-mlogloss:0.489267\ttest-mlogloss:0.498661\n",
      "[128]\ttrain-mlogloss:0.484499\ttest-mlogloss:0.493922\n",
      "[129]\ttrain-mlogloss:0.479769\ttest-mlogloss:0.489204\n",
      "[130]\ttrain-mlogloss:0.47511\ttest-mlogloss:0.48456\n",
      "[131]\ttrain-mlogloss:0.470494\ttest-mlogloss:0.479971\n",
      "[132]\ttrain-mlogloss:0.465924\ttest-mlogloss:0.475436\n",
      "[133]\ttrain-mlogloss:0.461409\ttest-mlogloss:0.47096\n",
      "[134]\ttrain-mlogloss:0.456948\ttest-mlogloss:0.466533\n",
      "[135]\ttrain-mlogloss:0.452535\ttest-mlogloss:0.462156\n",
      "[136]\ttrain-mlogloss:0.448177\ttest-mlogloss:0.45782\n",
      "[137]\ttrain-mlogloss:0.443846\ttest-mlogloss:0.453527\n",
      "[138]\ttrain-mlogloss:0.439562\ttest-mlogloss:0.449275\n",
      "[139]\ttrain-mlogloss:0.435323\ttest-mlogloss:0.445051\n",
      "[140]\ttrain-mlogloss:0.431139\ttest-mlogloss:0.440902\n",
      "[141]\ttrain-mlogloss:0.426988\ttest-mlogloss:0.436766\n",
      "[142]\ttrain-mlogloss:0.422887\ttest-mlogloss:0.432687\n",
      "[143]\ttrain-mlogloss:0.418823\ttest-mlogloss:0.428645\n",
      "[144]\ttrain-mlogloss:0.414814\ttest-mlogloss:0.424668\n",
      "[145]\ttrain-mlogloss:0.410847\ttest-mlogloss:0.420724\n",
      "[146]\ttrain-mlogloss:0.40692\ttest-mlogloss:0.416816\n",
      "[147]\ttrain-mlogloss:0.403025\ttest-mlogloss:0.41294\n",
      "[148]\ttrain-mlogloss:0.399185\ttest-mlogloss:0.409128\n",
      "[149]\ttrain-mlogloss:0.395378\ttest-mlogloss:0.405349\n",
      "[150]\ttrain-mlogloss:0.391617\ttest-mlogloss:0.401621\n",
      "[151]\ttrain-mlogloss:0.387887\ttest-mlogloss:0.397902\n",
      "[152]\ttrain-mlogloss:0.384198\ttest-mlogloss:0.394238\n",
      "[153]\ttrain-mlogloss:0.380558\ttest-mlogloss:0.39061\n",
      "[154]\ttrain-mlogloss:0.376935\ttest-mlogloss:0.387002\n",
      "[155]\ttrain-mlogloss:0.373363\ttest-mlogloss:0.383449\n",
      "[156]\ttrain-mlogloss:0.369819\ttest-mlogloss:0.379919\n",
      "[157]\ttrain-mlogloss:0.366318\ttest-mlogloss:0.376433\n",
      "[158]\ttrain-mlogloss:0.362855\ttest-mlogloss:0.372983\n",
      "[159]\ttrain-mlogloss:0.359436\ttest-mlogloss:0.369582\n",
      "[160]\ttrain-mlogloss:0.356049\ttest-mlogloss:0.366212\n",
      "[161]\ttrain-mlogloss:0.352704\ttest-mlogloss:0.362882\n",
      "[162]\ttrain-mlogloss:0.349389\ttest-mlogloss:0.359586\n",
      "[163]\ttrain-mlogloss:0.3461\ttest-mlogloss:0.356302\n",
      "[164]\ttrain-mlogloss:0.34285\ttest-mlogloss:0.353075\n",
      "[165]\ttrain-mlogloss:0.33962\ttest-mlogloss:0.349856\n",
      "[166]\ttrain-mlogloss:0.336432\ttest-mlogloss:0.346689\n",
      "[167]\ttrain-mlogloss:0.333287\ttest-mlogloss:0.343565\n",
      "[168]\ttrain-mlogloss:0.330155\ttest-mlogloss:0.34046\n",
      "[169]\ttrain-mlogloss:0.327056\ttest-mlogloss:0.337388\n",
      "[170]\ttrain-mlogloss:0.323986\ttest-mlogloss:0.334341\n",
      "[171]\ttrain-mlogloss:0.32097\ttest-mlogloss:0.331335\n",
      "[172]\ttrain-mlogloss:0.317971\ttest-mlogloss:0.328343\n",
      "[173]\ttrain-mlogloss:0.314998\ttest-mlogloss:0.325385\n",
      "[174]\ttrain-mlogloss:0.312061\ttest-mlogloss:0.322458\n",
      "[175]\ttrain-mlogloss:0.309147\ttest-mlogloss:0.31956\n",
      "[176]\ttrain-mlogloss:0.306275\ttest-mlogloss:0.316706\n",
      "[177]\ttrain-mlogloss:0.303436\ttest-mlogloss:0.313887\n",
      "[178]\ttrain-mlogloss:0.300611\ttest-mlogloss:0.311083\n",
      "[179]\ttrain-mlogloss:0.297826\ttest-mlogloss:0.308318\n",
      "[180]\ttrain-mlogloss:0.295062\ttest-mlogloss:0.305568\n",
      "[181]\ttrain-mlogloss:0.292321\ttest-mlogloss:0.302844\n",
      "[182]\ttrain-mlogloss:0.289607\ttest-mlogloss:0.300152\n",
      "[183]\ttrain-mlogloss:0.286916\ttest-mlogloss:0.297485\n",
      "[184]\ttrain-mlogloss:0.284254\ttest-mlogloss:0.294833\n",
      "[185]\ttrain-mlogloss:0.281624\ttest-mlogloss:0.292224\n",
      "[186]\ttrain-mlogloss:0.279022\ttest-mlogloss:0.289636\n",
      "[187]\ttrain-mlogloss:0.276447\ttest-mlogloss:0.28708\n",
      "[188]\ttrain-mlogloss:0.273889\ttest-mlogloss:0.28453\n",
      "[189]\ttrain-mlogloss:0.271369\ttest-mlogloss:0.282037\n",
      "[190]\ttrain-mlogloss:0.268872\ttest-mlogloss:0.279538\n",
      "[191]\ttrain-mlogloss:0.266409\ttest-mlogloss:0.277076\n",
      "[192]\ttrain-mlogloss:0.263955\ttest-mlogloss:0.274621\n",
      "[193]\ttrain-mlogloss:0.261533\ttest-mlogloss:0.2722\n",
      "[194]\ttrain-mlogloss:0.259125\ttest-mlogloss:0.269808\n",
      "[195]\ttrain-mlogloss:0.256745\ttest-mlogloss:0.267442\n",
      "[196]\ttrain-mlogloss:0.254388\ttest-mlogloss:0.265092\n",
      "[197]\ttrain-mlogloss:0.252051\ttest-mlogloss:0.262769\n",
      "[198]\ttrain-mlogloss:0.249742\ttest-mlogloss:0.260478\n",
      "[199]\ttrain-mlogloss:0.247454\ttest-mlogloss:0.258209\n",
      "[200]\ttrain-mlogloss:0.245179\ttest-mlogloss:0.255947\n",
      "[201]\ttrain-mlogloss:0.24294\ttest-mlogloss:0.253718\n",
      "[202]\ttrain-mlogloss:0.240726\ttest-mlogloss:0.251519\n",
      "[203]\ttrain-mlogloss:0.238526\ttest-mlogloss:0.249328\n",
      "[204]\ttrain-mlogloss:0.236352\ttest-mlogloss:0.247171\n",
      "[205]\ttrain-mlogloss:0.234196\ttest-mlogloss:0.245022\n",
      "[206]\ttrain-mlogloss:0.232053\ttest-mlogloss:0.242876\n",
      "[207]\ttrain-mlogloss:0.229945\ttest-mlogloss:0.240787\n",
      "[208]\ttrain-mlogloss:0.227851\ttest-mlogloss:0.238713\n",
      "[209]\ttrain-mlogloss:0.225778\ttest-mlogloss:0.236647\n",
      "[210]\ttrain-mlogloss:0.223722\ttest-mlogloss:0.234606\n",
      "[211]\ttrain-mlogloss:0.221688\ttest-mlogloss:0.232585\n",
      "[212]\ttrain-mlogloss:0.219667\ttest-mlogloss:0.230568\n",
      "[213]\ttrain-mlogloss:0.217672\ttest-mlogloss:0.228585\n",
      "[214]\ttrain-mlogloss:0.215701\ttest-mlogloss:0.226615\n",
      "[215]\ttrain-mlogloss:0.213753\ttest-mlogloss:0.224676\n",
      "[216]\ttrain-mlogloss:0.211818\ttest-mlogloss:0.222768\n",
      "[217]\ttrain-mlogloss:0.209903\ttest-mlogloss:0.22086\n",
      "[218]\ttrain-mlogloss:0.208015\ttest-mlogloss:0.21898\n",
      "[219]\ttrain-mlogloss:0.206142\ttest-mlogloss:0.217111\n",
      "[220]\ttrain-mlogloss:0.204274\ttest-mlogloss:0.215249\n",
      "[221]\ttrain-mlogloss:0.202434\ttest-mlogloss:0.213425\n",
      "[222]\ttrain-mlogloss:0.200616\ttest-mlogloss:0.211617\n",
      "[223]\ttrain-mlogloss:0.198811\ttest-mlogloss:0.209817\n",
      "[224]\ttrain-mlogloss:0.197023\ttest-mlogloss:0.208032\n",
      "[225]\ttrain-mlogloss:0.19525\ttest-mlogloss:0.206267\n",
      "[226]\ttrain-mlogloss:0.1935\ttest-mlogloss:0.204525\n",
      "[227]\ttrain-mlogloss:0.191766\ttest-mlogloss:0.202803\n",
      "[228]\ttrain-mlogloss:0.190042\ttest-mlogloss:0.201094\n",
      "[229]\ttrain-mlogloss:0.188337\ttest-mlogloss:0.199396\n",
      "[230]\ttrain-mlogloss:0.18664\ttest-mlogloss:0.197705\n",
      "[231]\ttrain-mlogloss:0.184972\ttest-mlogloss:0.196048\n",
      "[232]\ttrain-mlogloss:0.183312\ttest-mlogloss:0.194405\n",
      "[233]\ttrain-mlogloss:0.181664\ttest-mlogloss:0.192773\n",
      "[234]\ttrain-mlogloss:0.180042\ttest-mlogloss:0.191167\n",
      "[235]\ttrain-mlogloss:0.178427\ttest-mlogloss:0.189554\n",
      "[236]\ttrain-mlogloss:0.176835\ttest-mlogloss:0.187965\n",
      "[237]\ttrain-mlogloss:0.175257\ttest-mlogloss:0.186388\n",
      "[238]\ttrain-mlogloss:0.173683\ttest-mlogloss:0.184825\n",
      "[239]\ttrain-mlogloss:0.172131\ttest-mlogloss:0.183282\n",
      "[240]\ttrain-mlogloss:0.170599\ttest-mlogloss:0.181761\n",
      "[241]\ttrain-mlogloss:0.169079\ttest-mlogloss:0.180249\n",
      "[242]\ttrain-mlogloss:0.167568\ttest-mlogloss:0.178753\n",
      "[243]\ttrain-mlogloss:0.166069\ttest-mlogloss:0.177262\n",
      "[244]\ttrain-mlogloss:0.164583\ttest-mlogloss:0.175789\n",
      "[245]\ttrain-mlogloss:0.163117\ttest-mlogloss:0.17433\n",
      "[246]\ttrain-mlogloss:0.161663\ttest-mlogloss:0.172882\n",
      "[247]\ttrain-mlogloss:0.160222\ttest-mlogloss:0.171453\n",
      "[248]\ttrain-mlogloss:0.158804\ttest-mlogloss:0.170041\n",
      "[249]\ttrain-mlogloss:0.157396\ttest-mlogloss:0.168637\n",
      "[250]\ttrain-mlogloss:0.155995\ttest-mlogloss:0.16724\n",
      "[251]\ttrain-mlogloss:0.154612\ttest-mlogloss:0.165863\n",
      "[252]\ttrain-mlogloss:0.153245\ttest-mlogloss:0.164504\n",
      "[253]\ttrain-mlogloss:0.151884\ttest-mlogloss:0.163146\n",
      "[254]\ttrain-mlogloss:0.150541\ttest-mlogloss:0.161812\n",
      "[255]\ttrain-mlogloss:0.149209\ttest-mlogloss:0.160487\n",
      "[256]\ttrain-mlogloss:0.14789\ttest-mlogloss:0.159182\n",
      "[257]\ttrain-mlogloss:0.146582\ttest-mlogloss:0.157882\n",
      "[258]\ttrain-mlogloss:0.14529\ttest-mlogloss:0.156592\n",
      "[259]\ttrain-mlogloss:0.144012\ttest-mlogloss:0.155319\n",
      "[260]\ttrain-mlogloss:0.142744\ttest-mlogloss:0.154056\n",
      "[261]\ttrain-mlogloss:0.141491\ttest-mlogloss:0.152811\n",
      "[262]\ttrain-mlogloss:0.140239\ttest-mlogloss:0.151564\n",
      "[263]\ttrain-mlogloss:0.139008\ttest-mlogloss:0.15034\n",
      "[264]\ttrain-mlogloss:0.137789\ttest-mlogloss:0.14912\n",
      "[265]\ttrain-mlogloss:0.136576\ttest-mlogloss:0.147915\n",
      "[266]\ttrain-mlogloss:0.135374\ttest-mlogloss:0.146716\n",
      "[267]\ttrain-mlogloss:0.134182\ttest-mlogloss:0.145534\n",
      "[268]\ttrain-mlogloss:0.133005\ttest-mlogloss:0.144368\n",
      "[269]\ttrain-mlogloss:0.131836\ttest-mlogloss:0.143205\n",
      "[270]\ttrain-mlogloss:0.130677\ttest-mlogloss:0.142048\n",
      "[271]\ttrain-mlogloss:0.129534\ttest-mlogloss:0.140904\n",
      "[272]\ttrain-mlogloss:0.128401\ttest-mlogloss:0.139781\n",
      "[273]\ttrain-mlogloss:0.127284\ttest-mlogloss:0.138667\n",
      "[274]\ttrain-mlogloss:0.12617\ttest-mlogloss:0.13756\n",
      "[275]\ttrain-mlogloss:0.125067\ttest-mlogloss:0.136469\n",
      "[276]\ttrain-mlogloss:0.123966\ttest-mlogloss:0.135369\n",
      "[277]\ttrain-mlogloss:0.122882\ttest-mlogloss:0.134305\n",
      "[278]\ttrain-mlogloss:0.121814\ttest-mlogloss:0.133239\n",
      "[279]\ttrain-mlogloss:0.12075\ttest-mlogloss:0.132175\n",
      "[280]\ttrain-mlogloss:0.119699\ttest-mlogloss:0.131127\n",
      "[281]\ttrain-mlogloss:0.118656\ttest-mlogloss:0.130087\n",
      "[282]\ttrain-mlogloss:0.117622\ttest-mlogloss:0.129056\n",
      "[283]\ttrain-mlogloss:0.11661\ttest-mlogloss:0.128062\n",
      "[284]\ttrain-mlogloss:0.115604\ttest-mlogloss:0.127073\n",
      "[285]\ttrain-mlogloss:0.114605\ttest-mlogloss:0.12608\n",
      "[286]\ttrain-mlogloss:0.113609\ttest-mlogloss:0.12509\n",
      "[287]\ttrain-mlogloss:0.112621\ttest-mlogloss:0.124103\n",
      "[288]\ttrain-mlogloss:0.111639\ttest-mlogloss:0.123124\n",
      "[289]\ttrain-mlogloss:0.110673\ttest-mlogloss:0.122156\n",
      "[290]\ttrain-mlogloss:0.109718\ttest-mlogloss:0.12121\n",
      "[291]\ttrain-mlogloss:0.108771\ttest-mlogloss:0.120262\n",
      "[292]\ttrain-mlogloss:0.10783\ttest-mlogloss:0.119317\n",
      "[293]\ttrain-mlogloss:0.106898\ttest-mlogloss:0.118387\n",
      "[294]\ttrain-mlogloss:0.105973\ttest-mlogloss:0.117466\n",
      "[295]\ttrain-mlogloss:0.10506\ttest-mlogloss:0.11655\n",
      "[296]\ttrain-mlogloss:0.104152\ttest-mlogloss:0.115641\n",
      "[297]\ttrain-mlogloss:0.103254\ttest-mlogloss:0.114742\n",
      "[298]\ttrain-mlogloss:0.10237\ttest-mlogloss:0.113863\n",
      "[299]\ttrain-mlogloss:0.101486\ttest-mlogloss:0.11298\n",
      "[300]\ttrain-mlogloss:0.100612\ttest-mlogloss:0.112106\n",
      "[301]\ttrain-mlogloss:0.09975\ttest-mlogloss:0.111243\n",
      "[302]\ttrain-mlogloss:0.0988922\ttest-mlogloss:0.110386\n",
      "[303]\ttrain-mlogloss:0.0980508\ttest-mlogloss:0.109544\n",
      "[304]\ttrain-mlogloss:0.0972036\ttest-mlogloss:0.108699\n",
      "[305]\ttrain-mlogloss:0.0963664\ttest-mlogloss:0.107867\n",
      "[306]\ttrain-mlogloss:0.0955444\ttest-mlogloss:0.107048\n",
      "[307]\ttrain-mlogloss:0.0947292\ttest-mlogloss:0.106237\n",
      "[308]\ttrain-mlogloss:0.0939206\ttest-mlogloss:0.105432\n",
      "[309]\ttrain-mlogloss:0.0931204\ttest-mlogloss:0.104633\n",
      "[310]\ttrain-mlogloss:0.092322\ttest-mlogloss:0.10383\n",
      "[311]\ttrain-mlogloss:0.0915388\ttest-mlogloss:0.10304\n",
      "[312]\ttrain-mlogloss:0.090758\ttest-mlogloss:0.102265\n",
      "[313]\ttrain-mlogloss:0.0899888\ttest-mlogloss:0.10149\n",
      "[314]\ttrain-mlogloss:0.089222\ttest-mlogloss:0.100728\n",
      "[315]\ttrain-mlogloss:0.0884636\ttest-mlogloss:0.0999684\n",
      "[316]\ttrain-mlogloss:0.0877136\ttest-mlogloss:0.0992154\n",
      "[317]\ttrain-mlogloss:0.086967\ttest-mlogloss:0.0984712\n",
      "[318]\ttrain-mlogloss:0.086233\ttest-mlogloss:0.09774\n",
      "[319]\ttrain-mlogloss:0.0855032\ttest-mlogloss:0.0970108\n",
      "[320]\ttrain-mlogloss:0.084778\ttest-mlogloss:0.0962852\n",
      "[321]\ttrain-mlogloss:0.0840626\ttest-mlogloss:0.0955734\n",
      "[322]\ttrain-mlogloss:0.0833528\ttest-mlogloss:0.0948616\n",
      "[323]\ttrain-mlogloss:0.0826438\ttest-mlogloss:0.0941592\n",
      "[324]\ttrain-mlogloss:0.0819458\ttest-mlogloss:0.0934588\n",
      "[325]\ttrain-mlogloss:0.0812552\ttest-mlogloss:0.0927712\n",
      "[326]\ttrain-mlogloss:0.0805734\ttest-mlogloss:0.092083\n",
      "[327]\ttrain-mlogloss:0.0798968\ttest-mlogloss:0.0914184\n",
      "[328]\ttrain-mlogloss:0.0792252\ttest-mlogloss:0.0907404\n",
      "[329]\ttrain-mlogloss:0.0785648\ttest-mlogloss:0.0900882\n",
      "[330]\ttrain-mlogloss:0.0779062\ttest-mlogloss:0.0894348\n",
      "[331]\ttrain-mlogloss:0.0772498\ttest-mlogloss:0.0887796\n",
      "[332]\ttrain-mlogloss:0.0766044\ttest-mlogloss:0.0881324\n",
      "[333]\ttrain-mlogloss:0.0759608\ttest-mlogloss:0.0874972\n",
      "[334]\ttrain-mlogloss:0.0753198\ttest-mlogloss:0.0868582\n",
      "[335]\ttrain-mlogloss:0.0746922\ttest-mlogloss:0.0862304\n",
      "[336]\ttrain-mlogloss:0.0740688\ttest-mlogloss:0.085608\n",
      "[337]\ttrain-mlogloss:0.0734512\ttest-mlogloss:0.084989\n",
      "[338]\ttrain-mlogloss:0.0728366\ttest-mlogloss:0.0843732\n",
      "[339]\ttrain-mlogloss:0.0722294\ttest-mlogloss:0.0837662\n",
      "[340]\ttrain-mlogloss:0.071626\ttest-mlogloss:0.0831632\n",
      "[341]\ttrain-mlogloss:0.0710336\ttest-mlogloss:0.0825718\n",
      "[342]\ttrain-mlogloss:0.070444\ttest-mlogloss:0.0819822\n",
      "[343]\ttrain-mlogloss:0.0698604\ttest-mlogloss:0.0813958\n",
      "[344]\ttrain-mlogloss:0.0692818\ttest-mlogloss:0.0808182\n",
      "[345]\ttrain-mlogloss:0.0687048\ttest-mlogloss:0.0802408\n",
      "[346]\ttrain-mlogloss:0.0681396\ttest-mlogloss:0.0796774\n",
      "[347]\ttrain-mlogloss:0.0675746\ttest-mlogloss:0.0791188\n",
      "[348]\ttrain-mlogloss:0.0670154\ttest-mlogloss:0.0785544\n",
      "[349]\ttrain-mlogloss:0.0664636\ttest-mlogloss:0.078004\n",
      "[350]\ttrain-mlogloss:0.0659166\ttest-mlogloss:0.0774518\n",
      "[351]\ttrain-mlogloss:0.0653694\ttest-mlogloss:0.0769056\n",
      "[352]\ttrain-mlogloss:0.0648316\ttest-mlogloss:0.0763694\n",
      "[353]\ttrain-mlogloss:0.0642968\ttest-mlogloss:0.0758332\n",
      "[354]\ttrain-mlogloss:0.0637676\ttest-mlogloss:0.0753102\n",
      "[355]\ttrain-mlogloss:0.0632452\ttest-mlogloss:0.0747808\n",
      "[356]\ttrain-mlogloss:0.06272\ttest-mlogloss:0.0742542\n",
      "[357]\ttrain-mlogloss:0.0622072\ttest-mlogloss:0.073739\n",
      "[358]\ttrain-mlogloss:0.0616964\ttest-mlogloss:0.0732222\n",
      "[359]\ttrain-mlogloss:0.0611944\ttest-mlogloss:0.072722\n",
      "[360]\ttrain-mlogloss:0.060692\ttest-mlogloss:0.0722272\n",
      "[361]\ttrain-mlogloss:0.0601904\ttest-mlogloss:0.0717262\n",
      "[362]\ttrain-mlogloss:0.0597028\ttest-mlogloss:0.0712408\n",
      "[363]\ttrain-mlogloss:0.0592172\ttest-mlogloss:0.0707452\n",
      "[364]\ttrain-mlogloss:0.058737\ttest-mlogloss:0.0702588\n",
      "[365]\ttrain-mlogloss:0.058261\ttest-mlogloss:0.0697744\n",
      "[366]\ttrain-mlogloss:0.05779\ttest-mlogloss:0.0693032\n",
      "[367]\ttrain-mlogloss:0.0573192\ttest-mlogloss:0.0688336\n",
      "[368]\ttrain-mlogloss:0.056859\ttest-mlogloss:0.0683702\n",
      "[369]\ttrain-mlogloss:0.0563972\ttest-mlogloss:0.067912\n",
      "[370]\ttrain-mlogloss:0.0559414\ttest-mlogloss:0.0674566\n",
      "[371]\ttrain-mlogloss:0.0554856\ttest-mlogloss:0.067\n",
      "[372]\ttrain-mlogloss:0.0550402\ttest-mlogloss:0.066552\n",
      "[373]\ttrain-mlogloss:0.0545978\ttest-mlogloss:0.0661142\n",
      "[374]\ttrain-mlogloss:0.054161\ttest-mlogloss:0.0656766\n",
      "[375]\ttrain-mlogloss:0.053727\ttest-mlogloss:0.0652436\n",
      "[376]\ttrain-mlogloss:0.0532936\ttest-mlogloss:0.0648012\n",
      "[377]\ttrain-mlogloss:0.0528652\ttest-mlogloss:0.0643728\n",
      "[378]\ttrain-mlogloss:0.052442\ttest-mlogloss:0.063949\n",
      "[379]\ttrain-mlogloss:0.0520184\ttest-mlogloss:0.0635192\n",
      "[380]\ttrain-mlogloss:0.051598\ttest-mlogloss:0.0631022\n",
      "[381]\ttrain-mlogloss:0.0511848\ttest-mlogloss:0.0626806\n",
      "[382]\ttrain-mlogloss:0.050774\ttest-mlogloss:0.0622688\n",
      "[383]\ttrain-mlogloss:0.05037\ttest-mlogloss:0.0618724\n",
      "[384]\ttrain-mlogloss:0.0499652\ttest-mlogloss:0.0614726\n",
      "[385]\ttrain-mlogloss:0.0495662\ttest-mlogloss:0.0610736\n",
      "[386]\ttrain-mlogloss:0.0491728\ttest-mlogloss:0.0606814\n",
      "[387]\ttrain-mlogloss:0.0487788\ttest-mlogloss:0.0602896\n",
      "[388]\ttrain-mlogloss:0.0483896\ttest-mlogloss:0.0599012\n",
      "[389]\ttrain-mlogloss:0.0480046\ttest-mlogloss:0.0595206\n",
      "[390]\ttrain-mlogloss:0.047626\ttest-mlogloss:0.0591404\n",
      "[391]\ttrain-mlogloss:0.0472458\ttest-mlogloss:0.058752\n",
      "[392]\ttrain-mlogloss:0.0468762\ttest-mlogloss:0.058376\n",
      "[393]\ttrain-mlogloss:0.046505\ttest-mlogloss:0.0580054\n",
      "[394]\ttrain-mlogloss:0.0461392\ttest-mlogloss:0.0576362\n",
      "[395]\ttrain-mlogloss:0.0457748\ttest-mlogloss:0.0572746\n",
      "[396]\ttrain-mlogloss:0.045417\ttest-mlogloss:0.0569224\n",
      "[397]\ttrain-mlogloss:0.0450614\ttest-mlogloss:0.056566\n",
      "[398]\ttrain-mlogloss:0.0447068\ttest-mlogloss:0.0562102\n",
      "[399]\ttrain-mlogloss:0.0443532\ttest-mlogloss:0.0558546\n",
      "[400]\ttrain-mlogloss:0.0440074\ttest-mlogloss:0.0555114\n",
      "[401]\ttrain-mlogloss:0.043663\ttest-mlogloss:0.0551668\n",
      "[402]\ttrain-mlogloss:0.0433232\ttest-mlogloss:0.054836\n",
      "[403]\ttrain-mlogloss:0.0429832\ttest-mlogloss:0.0544924\n",
      "[404]\ttrain-mlogloss:0.0426494\ttest-mlogloss:0.0541556\n",
      "[405]\ttrain-mlogloss:0.0423152\ttest-mlogloss:0.0538246\n",
      "[406]\ttrain-mlogloss:0.0419848\ttest-mlogloss:0.0534894\n",
      "[407]\ttrain-mlogloss:0.041659\ttest-mlogloss:0.0531586\n",
      "[408]\ttrain-mlogloss:0.0413378\ttest-mlogloss:0.0528388\n",
      "[409]\ttrain-mlogloss:0.0410202\ttest-mlogloss:0.0525208\n",
      "[410]\ttrain-mlogloss:0.0406994\ttest-mlogloss:0.0521968\n",
      "[411]\ttrain-mlogloss:0.0403852\ttest-mlogloss:0.0518768\n",
      "[412]\ttrain-mlogloss:0.0400718\ttest-mlogloss:0.051562\n",
      "[413]\ttrain-mlogloss:0.0397622\ttest-mlogloss:0.051252\n",
      "[414]\ttrain-mlogloss:0.0394564\ttest-mlogloss:0.0509468\n",
      "[415]\ttrain-mlogloss:0.0391516\ttest-mlogloss:0.0506462\n",
      "[416]\ttrain-mlogloss:0.038852\ttest-mlogloss:0.0503574\n",
      "[417]\ttrain-mlogloss:0.038552\ttest-mlogloss:0.0500608\n",
      "[418]\ttrain-mlogloss:0.0382558\ttest-mlogloss:0.0497636\n",
      "[419]\ttrain-mlogloss:0.0379642\ttest-mlogloss:0.0494694\n",
      "[420]\ttrain-mlogloss:0.037671\ttest-mlogloss:0.0491762\n",
      "[421]\ttrain-mlogloss:0.0373848\ttest-mlogloss:0.0488916\n",
      "[422]\ttrain-mlogloss:0.037101\ttest-mlogloss:0.0486062\n",
      "[423]\ttrain-mlogloss:0.0368184\ttest-mlogloss:0.048331\n",
      "[424]\ttrain-mlogloss:0.036537\ttest-mlogloss:0.0480506\n",
      "[425]\ttrain-mlogloss:0.0362568\ttest-mlogloss:0.047768\n",
      "[426]\ttrain-mlogloss:0.035981\ttest-mlogloss:0.0474948\n",
      "[427]\ttrain-mlogloss:0.0357108\ttest-mlogloss:0.047227\n",
      "[428]\ttrain-mlogloss:0.0354372\ttest-mlogloss:0.0469516\n",
      "[429]\ttrain-mlogloss:0.03517\ttest-mlogloss:0.0466892\n",
      "[430]\ttrain-mlogloss:0.0349022\ttest-mlogloss:0.046428\n",
      "[431]\ttrain-mlogloss:0.0346364\ttest-mlogloss:0.0461538\n",
      "[432]\ttrain-mlogloss:0.0343724\ttest-mlogloss:0.0458874\n",
      "[433]\ttrain-mlogloss:0.0341152\ttest-mlogloss:0.045632\n",
      "[434]\ttrain-mlogloss:0.0338604\ttest-mlogloss:0.0453788\n",
      "[435]\ttrain-mlogloss:0.0336072\ttest-mlogloss:0.0451148\n",
      "[436]\ttrain-mlogloss:0.0333556\ttest-mlogloss:0.044864\n",
      "[437]\ttrain-mlogloss:0.0331026\ttest-mlogloss:0.0446048\n",
      "[438]\ttrain-mlogloss:0.0328504\ttest-mlogloss:0.04436\n",
      "[439]\ttrain-mlogloss:0.0326034\ttest-mlogloss:0.0441096\n",
      "[440]\ttrain-mlogloss:0.0323578\ttest-mlogloss:0.0438586\n",
      "[441]\ttrain-mlogloss:0.0321144\ttest-mlogloss:0.043617\n",
      "[442]\ttrain-mlogloss:0.0318738\ttest-mlogloss:0.0433702\n",
      "[443]\ttrain-mlogloss:0.0316342\ttest-mlogloss:0.0431336\n",
      "[444]\ttrain-mlogloss:0.031401\ttest-mlogloss:0.0429022\n",
      "[445]\ttrain-mlogloss:0.031169\ttest-mlogloss:0.0426656\n",
      "[446]\ttrain-mlogloss:0.0309364\ttest-mlogloss:0.042425\n",
      "[447]\ttrain-mlogloss:0.0307096\ttest-mlogloss:0.0421982\n",
      "[448]\ttrain-mlogloss:0.030482\ttest-mlogloss:0.041971\n",
      "[449]\ttrain-mlogloss:0.0302556\ttest-mlogloss:0.0417454\n",
      "[450]\ttrain-mlogloss:0.0300318\ttest-mlogloss:0.0415172\n",
      "[451]\ttrain-mlogloss:0.029814\ttest-mlogloss:0.0413006\n",
      "[452]\ttrain-mlogloss:0.029593\ttest-mlogloss:0.0410756\n",
      "[453]\ttrain-mlogloss:0.0293766\ttest-mlogloss:0.0408584\n",
      "[454]\ttrain-mlogloss:0.0291608\ttest-mlogloss:0.04065\n",
      "[455]\ttrain-mlogloss:0.028946\ttest-mlogloss:0.0404356\n",
      "[456]\ttrain-mlogloss:0.028734\ttest-mlogloss:0.0402246\n",
      "[457]\ttrain-mlogloss:0.028525\ttest-mlogloss:0.040012\n",
      "[458]\ttrain-mlogloss:0.0283126\ttest-mlogloss:0.0397964\n",
      "[459]\ttrain-mlogloss:0.0281038\ttest-mlogloss:0.0395836\n",
      "[460]\ttrain-mlogloss:0.0278982\ttest-mlogloss:0.039379\n",
      "[461]\ttrain-mlogloss:0.0276916\ttest-mlogloss:0.0391748\n",
      "[462]\ttrain-mlogloss:0.027491\ttest-mlogloss:0.038975\n",
      "[463]\ttrain-mlogloss:0.0272946\ttest-mlogloss:0.038776\n",
      "[464]\ttrain-mlogloss:0.027093\ttest-mlogloss:0.0385692\n",
      "[465]\ttrain-mlogloss:0.0268982\ttest-mlogloss:0.038374\n",
      "[466]\ttrain-mlogloss:0.0267008\ttest-mlogloss:0.0381716\n",
      "[467]\ttrain-mlogloss:0.026509\ttest-mlogloss:0.0379754\n",
      "[468]\ttrain-mlogloss:0.0263186\ttest-mlogloss:0.037783\n",
      "[469]\ttrain-mlogloss:0.0261292\ttest-mlogloss:0.037597\n",
      "[470]\ttrain-mlogloss:0.0259426\ttest-mlogloss:0.0374086\n",
      "[471]\ttrain-mlogloss:0.0257556\ttest-mlogloss:0.0372232\n",
      "[472]\ttrain-mlogloss:0.025568\ttest-mlogloss:0.0370348\n",
      "[473]\ttrain-mlogloss:0.0253838\ttest-mlogloss:0.0368514\n",
      "[474]\ttrain-mlogloss:0.0252034\ttest-mlogloss:0.0366676\n",
      "[475]\ttrain-mlogloss:0.0250244\ttest-mlogloss:0.0364876\n",
      "[476]\ttrain-mlogloss:0.0248434\ttest-mlogloss:0.0363004\n",
      "[477]\ttrain-mlogloss:0.0246654\ttest-mlogloss:0.0361192\n",
      "[478]\ttrain-mlogloss:0.0244876\ttest-mlogloss:0.0359344\n",
      "[479]\ttrain-mlogloss:0.0243132\ttest-mlogloss:0.035758\n",
      "[480]\ttrain-mlogloss:0.0241414\ttest-mlogloss:0.0355814\n",
      "[481]\ttrain-mlogloss:0.0239694\ttest-mlogloss:0.0354024\n",
      "[482]\ttrain-mlogloss:0.0237998\ttest-mlogloss:0.0352302\n",
      "[483]\ttrain-mlogloss:0.0236308\ttest-mlogloss:0.0350654\n",
      "[484]\ttrain-mlogloss:0.0234646\ttest-mlogloss:0.0349016\n",
      "[485]\ttrain-mlogloss:0.0232998\ttest-mlogloss:0.034748\n",
      "[486]\ttrain-mlogloss:0.0231326\ttest-mlogloss:0.0345816\n",
      "[487]\ttrain-mlogloss:0.0229694\ttest-mlogloss:0.0344208\n",
      "[488]\ttrain-mlogloss:0.022807\ttest-mlogloss:0.0342568\n",
      "[489]\ttrain-mlogloss:0.0226442\ttest-mlogloss:0.0340902\n",
      "[490]\ttrain-mlogloss:0.022488\ttest-mlogloss:0.0339276\n",
      "[491]\ttrain-mlogloss:0.0223292\ttest-mlogloss:0.033767\n",
      "[492]\ttrain-mlogloss:0.022174\ttest-mlogloss:0.0336148\n",
      "[493]\ttrain-mlogloss:0.0220206\ttest-mlogloss:0.033463\n",
      "[494]\ttrain-mlogloss:0.0218666\ttest-mlogloss:0.0333112\n",
      "[495]\ttrain-mlogloss:0.0217166\ttest-mlogloss:0.03316\n",
      "[496]\ttrain-mlogloss:0.0215634\ttest-mlogloss:0.0330036\n",
      "[497]\ttrain-mlogloss:0.021413\ttest-mlogloss:0.0328564\n",
      "[498]\ttrain-mlogloss:0.021265\ttest-mlogloss:0.0327084\n",
      "[499]\ttrain-mlogloss:0.02112\ttest-mlogloss:0.0325642\n",
      "[500]\ttrain-mlogloss:0.020975\ttest-mlogloss:0.0324232\n",
      "[501]\ttrain-mlogloss:0.0208284\ttest-mlogloss:0.0322694\n",
      "[502]\ttrain-mlogloss:0.0206832\ttest-mlogloss:0.0321244\n",
      "[503]\ttrain-mlogloss:0.0205426\ttest-mlogloss:0.0319838\n",
      "[504]\ttrain-mlogloss:0.0204016\ttest-mlogloss:0.0318438\n",
      "[505]\ttrain-mlogloss:0.0202614\ttest-mlogloss:0.031704\n",
      "[506]\ttrain-mlogloss:0.0201218\ttest-mlogloss:0.031562\n",
      "[507]\ttrain-mlogloss:0.019984\ttest-mlogloss:0.0314246\n",
      "[508]\ttrain-mlogloss:0.0198464\ttest-mlogloss:0.0312878\n",
      "[509]\ttrain-mlogloss:0.0197092\ttest-mlogloss:0.0311502\n",
      "[510]\ttrain-mlogloss:0.0195758\ttest-mlogloss:0.0310166\n",
      "[511]\ttrain-mlogloss:0.0194412\ttest-mlogloss:0.0308818\n",
      "[512]\ttrain-mlogloss:0.0193106\ttest-mlogloss:0.0307464\n",
      "[513]\ttrain-mlogloss:0.0191772\ttest-mlogloss:0.030614\n",
      "[514]\ttrain-mlogloss:0.0190456\ttest-mlogloss:0.0304792\n",
      "[515]\ttrain-mlogloss:0.0189178\ttest-mlogloss:0.0303568\n",
      "[516]\ttrain-mlogloss:0.01879\ttest-mlogloss:0.0302358\n",
      "[517]\ttrain-mlogloss:0.0186622\ttest-mlogloss:0.0301058\n",
      "[518]\ttrain-mlogloss:0.0185346\ttest-mlogloss:0.029982\n",
      "[519]\ttrain-mlogloss:0.0184102\ttest-mlogloss:0.0298576\n",
      "[520]\ttrain-mlogloss:0.018287\ttest-mlogloss:0.0297306\n",
      "[521]\ttrain-mlogloss:0.0181666\ttest-mlogloss:0.029611\n",
      "[522]\ttrain-mlogloss:0.018045\ttest-mlogloss:0.029486\n",
      "[523]\ttrain-mlogloss:0.0179254\ttest-mlogloss:0.029365\n",
      "[524]\ttrain-mlogloss:0.0178068\ttest-mlogloss:0.0292472\n",
      "[525]\ttrain-mlogloss:0.0176894\ttest-mlogloss:0.0291284\n",
      "[526]\ttrain-mlogloss:0.0175718\ttest-mlogloss:0.02901\n",
      "[527]\ttrain-mlogloss:0.0174558\ttest-mlogloss:0.0288928\n",
      "[528]\ttrain-mlogloss:0.01734\ttest-mlogloss:0.0287776\n",
      "[529]\ttrain-mlogloss:0.017227\ttest-mlogloss:0.0286618\n",
      "[530]\ttrain-mlogloss:0.0171126\ttest-mlogloss:0.028544\n",
      "[531]\ttrain-mlogloss:0.016997\ttest-mlogloss:0.028427\n",
      "[532]\ttrain-mlogloss:0.016884\ttest-mlogloss:0.0283078\n",
      "[533]\ttrain-mlogloss:0.016772\ttest-mlogloss:0.0281982\n",
      "[534]\ttrain-mlogloss:0.0166608\ttest-mlogloss:0.0280822\n",
      "[535]\ttrain-mlogloss:0.016551\ttest-mlogloss:0.0279784\n",
      "[536]\ttrain-mlogloss:0.0164408\ttest-mlogloss:0.0278648\n",
      "[537]\ttrain-mlogloss:0.016333\ttest-mlogloss:0.0277612\n",
      "[538]\ttrain-mlogloss:0.0162276\ttest-mlogloss:0.02765\n",
      "[539]\ttrain-mlogloss:0.0161218\ttest-mlogloss:0.0275356\n",
      "[540]\ttrain-mlogloss:0.0160172\ttest-mlogloss:0.0274294\n",
      "[541]\ttrain-mlogloss:0.0159142\ttest-mlogloss:0.0273268\n",
      "[542]\ttrain-mlogloss:0.0158122\ttest-mlogloss:0.0272294\n",
      "[543]\ttrain-mlogloss:0.0157102\ttest-mlogloss:0.0271218\n",
      "[544]\ttrain-mlogloss:0.015607\ttest-mlogloss:0.0270172\n",
      "[545]\ttrain-mlogloss:0.015509\ttest-mlogloss:0.0269184\n",
      "[546]\ttrain-mlogloss:0.0154102\ttest-mlogloss:0.0268218\n",
      "[547]\ttrain-mlogloss:0.0153108\ttest-mlogloss:0.0267244\n",
      "[548]\ttrain-mlogloss:0.0152128\ttest-mlogloss:0.0266334\n",
      "[549]\ttrain-mlogloss:0.015116\ttest-mlogloss:0.0265392\n",
      "[550]\ttrain-mlogloss:0.0150188\ttest-mlogloss:0.0264394\n",
      "[551]\ttrain-mlogloss:0.0149224\ttest-mlogloss:0.026343\n",
      "[552]\ttrain-mlogloss:0.0148294\ttest-mlogloss:0.026246\n",
      "[553]\ttrain-mlogloss:0.0147346\ttest-mlogloss:0.0261548\n",
      "[554]\ttrain-mlogloss:0.0146408\ttest-mlogloss:0.0260578\n",
      "[555]\ttrain-mlogloss:0.0145484\ttest-mlogloss:0.0259654\n",
      "[556]\ttrain-mlogloss:0.0144564\ttest-mlogloss:0.0258752\n",
      "[557]\ttrain-mlogloss:0.0143624\ttest-mlogloss:0.0257796\n",
      "[558]\ttrain-mlogloss:0.01427\ttest-mlogloss:0.0256872\n",
      "[559]\ttrain-mlogloss:0.0141798\ttest-mlogloss:0.0255964\n",
      "[560]\ttrain-mlogloss:0.0140888\ttest-mlogloss:0.0255056\n",
      "[561]\ttrain-mlogloss:0.014\ttest-mlogloss:0.0254218\n",
      "[562]\ttrain-mlogloss:0.0139116\ttest-mlogloss:0.0253296\n",
      "[563]\ttrain-mlogloss:0.013822\ttest-mlogloss:0.0252374\n",
      "[564]\ttrain-mlogloss:0.0137356\ttest-mlogloss:0.025153\n",
      "[565]\ttrain-mlogloss:0.0136476\ttest-mlogloss:0.0250632\n",
      "[566]\ttrain-mlogloss:0.013563\ttest-mlogloss:0.0249776\n",
      "[567]\ttrain-mlogloss:0.0134772\ttest-mlogloss:0.0248928\n",
      "[568]\ttrain-mlogloss:0.0133928\ttest-mlogloss:0.024806\n",
      "[569]\ttrain-mlogloss:0.0133092\ttest-mlogloss:0.024721\n",
      "[570]\ttrain-mlogloss:0.0132242\ttest-mlogloss:0.0246372\n",
      "[571]\ttrain-mlogloss:0.0131422\ttest-mlogloss:0.0245518\n",
      "[572]\ttrain-mlogloss:0.01306\ttest-mlogloss:0.0244706\n",
      "[573]\ttrain-mlogloss:0.0129812\ttest-mlogloss:0.024391\n",
      "[574]\ttrain-mlogloss:0.0129038\ttest-mlogloss:0.0243148\n",
      "[575]\ttrain-mlogloss:0.0128258\ttest-mlogloss:0.0242362\n",
      "[576]\ttrain-mlogloss:0.0127468\ttest-mlogloss:0.0241528\n",
      "[577]\ttrain-mlogloss:0.01267\ttest-mlogloss:0.02408\n",
      "[578]\ttrain-mlogloss:0.0125906\ttest-mlogloss:0.0240024\n",
      "[579]\ttrain-mlogloss:0.0125128\ttest-mlogloss:0.023924\n",
      "[580]\ttrain-mlogloss:0.0124372\ttest-mlogloss:0.0238476\n",
      "[581]\ttrain-mlogloss:0.01236\ttest-mlogloss:0.0237714\n",
      "[582]\ttrain-mlogloss:0.012285\ttest-mlogloss:0.0236982\n",
      "[583]\ttrain-mlogloss:0.0122128\ttest-mlogloss:0.0236254\n",
      "[584]\ttrain-mlogloss:0.0121396\ttest-mlogloss:0.0235552\n",
      "[585]\ttrain-mlogloss:0.0120666\ttest-mlogloss:0.0234854\n",
      "[586]\ttrain-mlogloss:0.0119942\ttest-mlogloss:0.0234154\n",
      "[587]\ttrain-mlogloss:0.0119216\ttest-mlogloss:0.0233348\n",
      "[588]\ttrain-mlogloss:0.011849\ttest-mlogloss:0.0232622\n",
      "[589]\ttrain-mlogloss:0.0117792\ttest-mlogloss:0.0231888\n",
      "[590]\ttrain-mlogloss:0.0117096\ttest-mlogloss:0.023116\n",
      "[591]\ttrain-mlogloss:0.0116394\ttest-mlogloss:0.0230488\n",
      "[592]\ttrain-mlogloss:0.011568\ttest-mlogloss:0.0229766\n",
      "[593]\ttrain-mlogloss:0.0114996\ttest-mlogloss:0.022907\n",
      "[594]\ttrain-mlogloss:0.011432\ttest-mlogloss:0.0228358\n",
      "[595]\ttrain-mlogloss:0.0113646\ttest-mlogloss:0.0227644\n",
      "[596]\ttrain-mlogloss:0.0112978\ttest-mlogloss:0.0226978\n",
      "[597]\ttrain-mlogloss:0.0112318\ttest-mlogloss:0.0226314\n",
      "[598]\ttrain-mlogloss:0.0111658\ttest-mlogloss:0.022559\n",
      "[599]\ttrain-mlogloss:0.011101\ttest-mlogloss:0.0224894\n",
      "[600]\ttrain-mlogloss:0.0110346\ttest-mlogloss:0.0224252\n",
      "[601]\ttrain-mlogloss:0.0109694\ttest-mlogloss:0.0223614\n",
      "[602]\ttrain-mlogloss:0.0109048\ttest-mlogloss:0.0222992\n",
      "[603]\ttrain-mlogloss:0.0108412\ttest-mlogloss:0.0222358\n",
      "[604]\ttrain-mlogloss:0.0107782\ttest-mlogloss:0.0221746\n",
      "[605]\ttrain-mlogloss:0.0107164\ttest-mlogloss:0.0221132\n",
      "[606]\ttrain-mlogloss:0.0106564\ttest-mlogloss:0.0220542\n",
      "[607]\ttrain-mlogloss:0.010595\ttest-mlogloss:0.0219904\n",
      "[608]\ttrain-mlogloss:0.0105336\ttest-mlogloss:0.02193\n",
      "[609]\ttrain-mlogloss:0.0104726\ttest-mlogloss:0.021862\n",
      "[610]\ttrain-mlogloss:0.0104136\ttest-mlogloss:0.0218044\n",
      "[611]\ttrain-mlogloss:0.0103544\ttest-mlogloss:0.0217464\n",
      "[612]\ttrain-mlogloss:0.0102948\ttest-mlogloss:0.0216842\n",
      "[613]\ttrain-mlogloss:0.010237\ttest-mlogloss:0.021623\n",
      "[614]\ttrain-mlogloss:0.0101788\ttest-mlogloss:0.0215644\n",
      "[615]\ttrain-mlogloss:0.0101224\ttest-mlogloss:0.0215076\n",
      "[616]\ttrain-mlogloss:0.010064\ttest-mlogloss:0.0214476\n",
      "[617]\ttrain-mlogloss:0.0100052\ttest-mlogloss:0.021387\n",
      "[618]\ttrain-mlogloss:0.0099484\ttest-mlogloss:0.0213292\n",
      "[619]\ttrain-mlogloss:0.0098922\ttest-mlogloss:0.021271\n",
      "[620]\ttrain-mlogloss:0.0098356\ttest-mlogloss:0.0212134\n",
      "[621]\ttrain-mlogloss:0.0097804\ttest-mlogloss:0.0211562\n",
      "[622]\ttrain-mlogloss:0.009725\ttest-mlogloss:0.0211006\n",
      "[623]\ttrain-mlogloss:0.0096712\ttest-mlogloss:0.0210448\n",
      "[624]\ttrain-mlogloss:0.0096166\ttest-mlogloss:0.0209872\n",
      "[625]\ttrain-mlogloss:0.0095622\ttest-mlogloss:0.020934\n",
      "[626]\ttrain-mlogloss:0.0095092\ttest-mlogloss:0.0208814\n",
      "[627]\ttrain-mlogloss:0.0094562\ttest-mlogloss:0.0208332\n",
      "[628]\ttrain-mlogloss:0.0094034\ttest-mlogloss:0.0207846\n",
      "[629]\ttrain-mlogloss:0.0093526\ttest-mlogloss:0.0207332\n",
      "[630]\ttrain-mlogloss:0.0093008\ttest-mlogloss:0.0206826\n",
      "[631]\ttrain-mlogloss:0.0092498\ttest-mlogloss:0.02063\n",
      "[632]\ttrain-mlogloss:0.0091982\ttest-mlogloss:0.0205834\n",
      "[633]\ttrain-mlogloss:0.0091486\ttest-mlogloss:0.0205278\n",
      "[634]\ttrain-mlogloss:0.0090972\ttest-mlogloss:0.0204758\n",
      "[635]\ttrain-mlogloss:0.009047\ttest-mlogloss:0.020426\n",
      "[636]\ttrain-mlogloss:0.0089984\ttest-mlogloss:0.0203746\n",
      "[637]\ttrain-mlogloss:0.0089494\ttest-mlogloss:0.0203258\n",
      "[638]\ttrain-mlogloss:0.0089008\ttest-mlogloss:0.0202782\n",
      "[639]\ttrain-mlogloss:0.0088544\ttest-mlogloss:0.0202288\n",
      "[640]\ttrain-mlogloss:0.0088054\ttest-mlogloss:0.0201784\n",
      "[641]\ttrain-mlogloss:0.0087576\ttest-mlogloss:0.02013\n",
      "[642]\ttrain-mlogloss:0.0087116\ttest-mlogloss:0.0200836\n",
      "[643]\ttrain-mlogloss:0.0086638\ttest-mlogloss:0.020043\n",
      "[644]\ttrain-mlogloss:0.0086174\ttest-mlogloss:0.0200016\n",
      "[645]\ttrain-mlogloss:0.0085708\ttest-mlogloss:0.019956\n",
      "[646]\ttrain-mlogloss:0.0085242\ttest-mlogloss:0.0199098\n",
      "[647]\ttrain-mlogloss:0.0084784\ttest-mlogloss:0.0198626\n",
      "[648]\ttrain-mlogloss:0.0084342\ttest-mlogloss:0.019822\n",
      "[649]\ttrain-mlogloss:0.0083896\ttest-mlogloss:0.0197758\n",
      "[650]\ttrain-mlogloss:0.0083432\ttest-mlogloss:0.0197244\n",
      "[651]\ttrain-mlogloss:0.0083\ttest-mlogloss:0.0196784\n",
      "[652]\ttrain-mlogloss:0.0082556\ttest-mlogloss:0.0196346\n",
      "[653]\ttrain-mlogloss:0.0082122\ttest-mlogloss:0.019591\n",
      "[654]\ttrain-mlogloss:0.008168\ttest-mlogloss:0.0195506\n",
      "[655]\ttrain-mlogloss:0.0081262\ttest-mlogloss:0.0195066\n",
      "[656]\ttrain-mlogloss:0.0080834\ttest-mlogloss:0.0194622\n",
      "[657]\ttrain-mlogloss:0.0080424\ttest-mlogloss:0.0194244\n",
      "[658]\ttrain-mlogloss:0.0080002\ttest-mlogloss:0.0193734\n",
      "[659]\ttrain-mlogloss:0.0079588\ttest-mlogloss:0.0193294\n",
      "[660]\ttrain-mlogloss:0.0079156\ttest-mlogloss:0.0192904\n",
      "[661]\ttrain-mlogloss:0.0078762\ttest-mlogloss:0.0192442\n",
      "[662]\ttrain-mlogloss:0.007837\ttest-mlogloss:0.0192066\n",
      "[663]\ttrain-mlogloss:0.0077974\ttest-mlogloss:0.0191672\n",
      "[664]\ttrain-mlogloss:0.0077574\ttest-mlogloss:0.0191296\n",
      "[665]\ttrain-mlogloss:0.007717\ttest-mlogloss:0.0190904\n",
      "[666]\ttrain-mlogloss:0.0076766\ttest-mlogloss:0.019049\n",
      "[667]\ttrain-mlogloss:0.007637\ttest-mlogloss:0.0190086\n",
      "[668]\ttrain-mlogloss:0.0075976\ttest-mlogloss:0.0189674\n",
      "[669]\ttrain-mlogloss:0.0075584\ttest-mlogloss:0.0189286\n",
      "[670]\ttrain-mlogloss:0.0075194\ttest-mlogloss:0.0188886\n",
      "[671]\ttrain-mlogloss:0.007481\ttest-mlogloss:0.0188492\n",
      "[672]\ttrain-mlogloss:0.0074436\ttest-mlogloss:0.0188106\n",
      "[673]\ttrain-mlogloss:0.0074054\ttest-mlogloss:0.0187738\n",
      "[674]\ttrain-mlogloss:0.0073676\ttest-mlogloss:0.018732\n",
      "[675]\ttrain-mlogloss:0.0073304\ttest-mlogloss:0.018699\n",
      "[676]\ttrain-mlogloss:0.0072942\ttest-mlogloss:0.018666\n",
      "[677]\ttrain-mlogloss:0.0072572\ttest-mlogloss:0.0186278\n",
      "[678]\ttrain-mlogloss:0.007221\ttest-mlogloss:0.0185946\n",
      "[679]\ttrain-mlogloss:0.0071842\ttest-mlogloss:0.0185576\n",
      "[680]\ttrain-mlogloss:0.0071484\ttest-mlogloss:0.0185284\n",
      "[681]\ttrain-mlogloss:0.0071124\ttest-mlogloss:0.0184926\n",
      "[682]\ttrain-mlogloss:0.0070774\ttest-mlogloss:0.0184576\n",
      "[683]\ttrain-mlogloss:0.007043\ttest-mlogloss:0.018422\n",
      "[684]\ttrain-mlogloss:0.0070082\ttest-mlogloss:0.018386\n",
      "[685]\ttrain-mlogloss:0.0069734\ttest-mlogloss:0.0183524\n",
      "[686]\ttrain-mlogloss:0.0069386\ttest-mlogloss:0.018318\n",
      "[687]\ttrain-mlogloss:0.0069048\ttest-mlogloss:0.018285\n",
      "[688]\ttrain-mlogloss:0.0068722\ttest-mlogloss:0.0182558\n",
      "[689]\ttrain-mlogloss:0.006839\ttest-mlogloss:0.018221\n",
      "[690]\ttrain-mlogloss:0.0068034\ttest-mlogloss:0.018182\n",
      "[691]\ttrain-mlogloss:0.0067706\ttest-mlogloss:0.0181512\n",
      "[692]\ttrain-mlogloss:0.0067386\ttest-mlogloss:0.0181202\n",
      "[693]\ttrain-mlogloss:0.0067054\ttest-mlogloss:0.0180872\n",
      "[694]\ttrain-mlogloss:0.0066736\ttest-mlogloss:0.0180548\n",
      "[695]\ttrain-mlogloss:0.0066418\ttest-mlogloss:0.0180222\n",
      "[696]\ttrain-mlogloss:0.0066108\ttest-mlogloss:0.017993\n",
      "[697]\ttrain-mlogloss:0.0065786\ttest-mlogloss:0.0179602\n",
      "[698]\ttrain-mlogloss:0.0065464\ttest-mlogloss:0.0179266\n",
      "[699]\ttrain-mlogloss:0.0065152\ttest-mlogloss:0.0178936\n",
      "[700]\ttrain-mlogloss:0.0064838\ttest-mlogloss:0.0178608\n",
      "[701]\ttrain-mlogloss:0.0064534\ttest-mlogloss:0.0178268\n",
      "[702]\ttrain-mlogloss:0.0064224\ttest-mlogloss:0.0177942\n",
      "[703]\ttrain-mlogloss:0.0063918\ttest-mlogloss:0.0177578\n",
      "[704]\ttrain-mlogloss:0.0063616\ttest-mlogloss:0.0177312\n",
      "[705]\ttrain-mlogloss:0.0063308\ttest-mlogloss:0.0177028\n",
      "[706]\ttrain-mlogloss:0.0063024\ttest-mlogloss:0.0176768\n",
      "[707]\ttrain-mlogloss:0.0062728\ttest-mlogloss:0.0176476\n",
      "[708]\ttrain-mlogloss:0.0062444\ttest-mlogloss:0.0176168\n",
      "[709]\ttrain-mlogloss:0.0062148\ttest-mlogloss:0.0175922\n",
      "[710]\ttrain-mlogloss:0.006187\ttest-mlogloss:0.017564\n",
      "[711]\ttrain-mlogloss:0.0061576\ttest-mlogloss:0.0175346\n",
      "[712]\ttrain-mlogloss:0.0061286\ttest-mlogloss:0.0175082\n",
      "[713]\ttrain-mlogloss:0.0061006\ttest-mlogloss:0.0174824\n",
      "[714]\ttrain-mlogloss:0.0060736\ttest-mlogloss:0.0174542\n",
      "[715]\ttrain-mlogloss:0.0060454\ttest-mlogloss:0.017423\n",
      "[716]\ttrain-mlogloss:0.0060164\ttest-mlogloss:0.0173984\n",
      "[717]\ttrain-mlogloss:0.0059882\ttest-mlogloss:0.017372\n",
      "[718]\ttrain-mlogloss:0.0059614\ttest-mlogloss:0.0173412\n",
      "[719]\ttrain-mlogloss:0.0059334\ttest-mlogloss:0.0173146\n",
      "[720]\ttrain-mlogloss:0.0059058\ttest-mlogloss:0.0172878\n",
      "[721]\ttrain-mlogloss:0.0058792\ttest-mlogloss:0.0172606\n",
      "[722]\ttrain-mlogloss:0.005852\ttest-mlogloss:0.0172352\n",
      "[723]\ttrain-mlogloss:0.0058252\ttest-mlogloss:0.0172062\n",
      "[724]\ttrain-mlogloss:0.0057992\ttest-mlogloss:0.017177\n",
      "[725]\ttrain-mlogloss:0.0057738\ttest-mlogloss:0.0171532\n",
      "[726]\ttrain-mlogloss:0.005749\ttest-mlogloss:0.0171296\n",
      "[727]\ttrain-mlogloss:0.0057238\ttest-mlogloss:0.0171048\n",
      "[728]\ttrain-mlogloss:0.0056978\ttest-mlogloss:0.0170768\n",
      "[729]\ttrain-mlogloss:0.0056726\ttest-mlogloss:0.0170516\n",
      "[730]\ttrain-mlogloss:0.0056462\ttest-mlogloss:0.0170256\n",
      "[731]\ttrain-mlogloss:0.0056216\ttest-mlogloss:0.0170012\n",
      "[732]\ttrain-mlogloss:0.0055958\ttest-mlogloss:0.0169714\n",
      "[733]\ttrain-mlogloss:0.0055714\ttest-mlogloss:0.0169426\n",
      "[734]\ttrain-mlogloss:0.0055478\ttest-mlogloss:0.0169176\n",
      "[735]\ttrain-mlogloss:0.005524\ttest-mlogloss:0.0168968\n",
      "[736]\ttrain-mlogloss:0.0055002\ttest-mlogloss:0.0168712\n",
      "[737]\ttrain-mlogloss:0.0054764\ttest-mlogloss:0.0168478\n",
      "[738]\ttrain-mlogloss:0.005452\ttest-mlogloss:0.0168254\n",
      "[739]\ttrain-mlogloss:0.0054282\ttest-mlogloss:0.0167994\n",
      "[740]\ttrain-mlogloss:0.0054034\ttest-mlogloss:0.0167766\n",
      "[741]\ttrain-mlogloss:0.0053798\ttest-mlogloss:0.0167534\n",
      "[742]\ttrain-mlogloss:0.0053562\ttest-mlogloss:0.0167268\n",
      "[743]\ttrain-mlogloss:0.0053334\ttest-mlogloss:0.0167014\n",
      "[744]\ttrain-mlogloss:0.0053104\ttest-mlogloss:0.016673\n",
      "[745]\ttrain-mlogloss:0.0052874\ttest-mlogloss:0.0166474\n",
      "[746]\ttrain-mlogloss:0.005265\ttest-mlogloss:0.016625\n",
      "[747]\ttrain-mlogloss:0.0052432\ttest-mlogloss:0.0166058\n",
      "[748]\ttrain-mlogloss:0.0052212\ttest-mlogloss:0.0165824\n",
      "[749]\ttrain-mlogloss:0.0051976\ttest-mlogloss:0.0165588\n",
      "[750]\ttrain-mlogloss:0.0051752\ttest-mlogloss:0.0165358\n",
      "[751]\ttrain-mlogloss:0.0051532\ttest-mlogloss:0.0165114\n",
      "[752]\ttrain-mlogloss:0.0051314\ttest-mlogloss:0.0164902\n",
      "[753]\ttrain-mlogloss:0.0051086\ttest-mlogloss:0.0164668\n",
      "[754]\ttrain-mlogloss:0.0050866\ttest-mlogloss:0.0164448\n",
      "[755]\ttrain-mlogloss:0.0050656\ttest-mlogloss:0.0164256\n",
      "[756]\ttrain-mlogloss:0.0050444\ttest-mlogloss:0.0164034\n",
      "[757]\ttrain-mlogloss:0.005022\ttest-mlogloss:0.0163828\n",
      "[758]\ttrain-mlogloss:0.0050004\ttest-mlogloss:0.0163524\n",
      "[759]\ttrain-mlogloss:0.0049798\ttest-mlogloss:0.0163342\n",
      "[760]\ttrain-mlogloss:0.0049592\ttest-mlogloss:0.0163112\n",
      "[761]\ttrain-mlogloss:0.0049382\ttest-mlogloss:0.0162924\n",
      "[762]\ttrain-mlogloss:0.0049172\ttest-mlogloss:0.0162706\n",
      "[763]\ttrain-mlogloss:0.0048974\ttest-mlogloss:0.0162496\n",
      "[764]\ttrain-mlogloss:0.004877\ttest-mlogloss:0.0162332\n",
      "[765]\ttrain-mlogloss:0.004856\ttest-mlogloss:0.0162164\n",
      "[766]\ttrain-mlogloss:0.0048358\ttest-mlogloss:0.016193\n",
      "[767]\ttrain-mlogloss:0.0048156\ttest-mlogloss:0.0161718\n",
      "[768]\ttrain-mlogloss:0.0047958\ttest-mlogloss:0.0161498\n",
      "[769]\ttrain-mlogloss:0.004777\ttest-mlogloss:0.0161296\n",
      "[770]\ttrain-mlogloss:0.004757\ttest-mlogloss:0.01611\n",
      "[771]\ttrain-mlogloss:0.0047382\ttest-mlogloss:0.0160896\n",
      "[772]\ttrain-mlogloss:0.0047188\ttest-mlogloss:0.0160698\n",
      "[773]\ttrain-mlogloss:0.0047006\ttest-mlogloss:0.0160528\n",
      "[774]\ttrain-mlogloss:0.0046814\ttest-mlogloss:0.016035\n",
      "[775]\ttrain-mlogloss:0.004663\ttest-mlogloss:0.01602\n",
      "[776]\ttrain-mlogloss:0.004644\ttest-mlogloss:0.015999\n",
      "[777]\ttrain-mlogloss:0.0046258\ttest-mlogloss:0.0159844\n",
      "[778]\ttrain-mlogloss:0.0046076\ttest-mlogloss:0.0159648\n",
      "[779]\ttrain-mlogloss:0.0045896\ttest-mlogloss:0.0159464\n",
      "[780]\ttrain-mlogloss:0.0045714\ttest-mlogloss:0.0159296\n",
      "[781]\ttrain-mlogloss:0.004553\ttest-mlogloss:0.0159106\n",
      "[782]\ttrain-mlogloss:0.004535\ttest-mlogloss:0.015894\n",
      "[783]\ttrain-mlogloss:0.0045172\ttest-mlogloss:0.0158716\n",
      "[784]\ttrain-mlogloss:0.004499\ttest-mlogloss:0.0158552\n",
      "[785]\ttrain-mlogloss:0.004481\ttest-mlogloss:0.015841\n",
      "[786]\ttrain-mlogloss:0.0044636\ttest-mlogloss:0.015827\n",
      "[787]\ttrain-mlogloss:0.0044452\ttest-mlogloss:0.0158058\n",
      "[788]\ttrain-mlogloss:0.0044268\ttest-mlogloss:0.0157848\n",
      "[789]\ttrain-mlogloss:0.0044104\ttest-mlogloss:0.015769\n",
      "[790]\ttrain-mlogloss:0.004393\ttest-mlogloss:0.015751\n",
      "[791]\ttrain-mlogloss:0.0043754\ttest-mlogloss:0.0157348\n",
      "[792]\ttrain-mlogloss:0.0043584\ttest-mlogloss:0.0157148\n",
      "[793]\ttrain-mlogloss:0.0043414\ttest-mlogloss:0.0156956\n",
      "[794]\ttrain-mlogloss:0.0043246\ttest-mlogloss:0.0156816\n",
      "[795]\ttrain-mlogloss:0.0043076\ttest-mlogloss:0.015665\n",
      "[796]\ttrain-mlogloss:0.0042916\ttest-mlogloss:0.0156504\n",
      "[797]\ttrain-mlogloss:0.0042752\ttest-mlogloss:0.0156362\n",
      "[798]\ttrain-mlogloss:0.0042582\ttest-mlogloss:0.0156204\n",
      "[799]\ttrain-mlogloss:0.0042422\ttest-mlogloss:0.0156038\n",
      "[800]\ttrain-mlogloss:0.0042256\ttest-mlogloss:0.0155884\n",
      "[801]\ttrain-mlogloss:0.0042094\ttest-mlogloss:0.0155688\n",
      "[802]\ttrain-mlogloss:0.0041934\ttest-mlogloss:0.0155514\n",
      "[803]\ttrain-mlogloss:0.004177\ttest-mlogloss:0.015537\n",
      "[804]\ttrain-mlogloss:0.0041618\ttest-mlogloss:0.0155218\n",
      "[805]\ttrain-mlogloss:0.0041464\ttest-mlogloss:0.015505\n",
      "[806]\ttrain-mlogloss:0.0041308\ttest-mlogloss:0.0154914\n",
      "[807]\ttrain-mlogloss:0.0041152\ttest-mlogloss:0.015479\n",
      "[808]\ttrain-mlogloss:0.0041006\ttest-mlogloss:0.0154648\n",
      "[809]\ttrain-mlogloss:0.0040848\ttest-mlogloss:0.0154506\n",
      "[810]\ttrain-mlogloss:0.0040696\ttest-mlogloss:0.0154386\n",
      "[811]\ttrain-mlogloss:0.0040544\ttest-mlogloss:0.01542\n",
      "[812]\ttrain-mlogloss:0.0040394\ttest-mlogloss:0.0154066\n",
      "[813]\ttrain-mlogloss:0.004025\ttest-mlogloss:0.0153916\n",
      "[814]\ttrain-mlogloss:0.0040102\ttest-mlogloss:0.015375\n",
      "[815]\ttrain-mlogloss:0.003996\ttest-mlogloss:0.015361\n",
      "[816]\ttrain-mlogloss:0.003981\ttest-mlogloss:0.0153474\n",
      "[817]\ttrain-mlogloss:0.0039666\ttest-mlogloss:0.015333\n",
      "[818]\ttrain-mlogloss:0.0039512\ttest-mlogloss:0.0153144\n",
      "[819]\ttrain-mlogloss:0.0039362\ttest-mlogloss:0.0152982\n",
      "[820]\ttrain-mlogloss:0.003922\ttest-mlogloss:0.0152842\n",
      "[821]\ttrain-mlogloss:0.003908\ttest-mlogloss:0.0152672\n",
      "[822]\ttrain-mlogloss:0.0038942\ttest-mlogloss:0.0152532\n",
      "[823]\ttrain-mlogloss:0.0038802\ttest-mlogloss:0.0152392\n",
      "[824]\ttrain-mlogloss:0.0038664\ttest-mlogloss:0.0152272\n",
      "[825]\ttrain-mlogloss:0.0038526\ttest-mlogloss:0.0152128\n",
      "[826]\ttrain-mlogloss:0.0038384\ttest-mlogloss:0.0151966\n",
      "[827]\ttrain-mlogloss:0.0038246\ttest-mlogloss:0.0151824\n",
      "[828]\ttrain-mlogloss:0.0038112\ttest-mlogloss:0.0151688\n",
      "[829]\ttrain-mlogloss:0.0037974\ttest-mlogloss:0.015153\n",
      "[830]\ttrain-mlogloss:0.0037842\ttest-mlogloss:0.0151404\n",
      "[831]\ttrain-mlogloss:0.0037702\ttest-mlogloss:0.015128\n",
      "[832]\ttrain-mlogloss:0.0037564\ttest-mlogloss:0.0151152\n",
      "[833]\ttrain-mlogloss:0.0037426\ttest-mlogloss:0.0151\n",
      "[834]\ttrain-mlogloss:0.0037288\ttest-mlogloss:0.0150864\n",
      "[835]\ttrain-mlogloss:0.0037156\ttest-mlogloss:0.0150726\n",
      "[836]\ttrain-mlogloss:0.0037028\ttest-mlogloss:0.0150618\n",
      "[837]\ttrain-mlogloss:0.0036904\ttest-mlogloss:0.015047\n",
      "[838]\ttrain-mlogloss:0.0036768\ttest-mlogloss:0.015033\n",
      "[839]\ttrain-mlogloss:0.003664\ttest-mlogloss:0.015022\n",
      "[840]\ttrain-mlogloss:0.0036514\ttest-mlogloss:0.0150064\n",
      "[841]\ttrain-mlogloss:0.0036384\ttest-mlogloss:0.0149944\n",
      "[842]\ttrain-mlogloss:0.0036264\ttest-mlogloss:0.014982\n",
      "[843]\ttrain-mlogloss:0.0036138\ttest-mlogloss:0.014966\n",
      "[844]\ttrain-mlogloss:0.0036012\ttest-mlogloss:0.014953\n",
      "[845]\ttrain-mlogloss:0.0035884\ttest-mlogloss:0.0149398\n",
      "[846]\ttrain-mlogloss:0.0035758\ttest-mlogloss:0.0149282\n",
      "[847]\ttrain-mlogloss:0.0035636\ttest-mlogloss:0.014912\n",
      "[848]\ttrain-mlogloss:0.0035514\ttest-mlogloss:0.0148984\n",
      "[849]\ttrain-mlogloss:0.0035394\ttest-mlogloss:0.0148888\n",
      "[850]\ttrain-mlogloss:0.0035272\ttest-mlogloss:0.014873\n",
      "[851]\ttrain-mlogloss:0.0035158\ttest-mlogloss:0.0148602\n",
      "[852]\ttrain-mlogloss:0.0035032\ttest-mlogloss:0.0148456\n",
      "[853]\ttrain-mlogloss:0.0034914\ttest-mlogloss:0.0148316\n",
      "[854]\ttrain-mlogloss:0.0034796\ttest-mlogloss:0.0148236\n",
      "[855]\ttrain-mlogloss:0.0034678\ttest-mlogloss:0.0148128\n",
      "[856]\ttrain-mlogloss:0.0034552\ttest-mlogloss:0.0147986\n",
      "[857]\ttrain-mlogloss:0.0034436\ttest-mlogloss:0.0147854\n",
      "[858]\ttrain-mlogloss:0.0034326\ttest-mlogloss:0.01477\n",
      "[859]\ttrain-mlogloss:0.0034212\ttest-mlogloss:0.0147558\n",
      "[860]\ttrain-mlogloss:0.0034098\ttest-mlogloss:0.014742\n",
      "[861]\ttrain-mlogloss:0.0033988\ttest-mlogloss:0.0147292\n",
      "[862]\ttrain-mlogloss:0.0033874\ttest-mlogloss:0.0147206\n",
      "[863]\ttrain-mlogloss:0.0033762\ttest-mlogloss:0.0147096\n",
      "[864]\ttrain-mlogloss:0.0033652\ttest-mlogloss:0.0147002\n",
      "[865]\ttrain-mlogloss:0.003354\ttest-mlogloss:0.014693\n",
      "[866]\ttrain-mlogloss:0.0033428\ttest-mlogloss:0.0146822\n",
      "[867]\ttrain-mlogloss:0.0033324\ttest-mlogloss:0.014672\n",
      "[868]\ttrain-mlogloss:0.0033208\ttest-mlogloss:0.0146576\n",
      "[869]\ttrain-mlogloss:0.0033102\ttest-mlogloss:0.0146454\n",
      "[870]\ttrain-mlogloss:0.0032994\ttest-mlogloss:0.0146342\n",
      "[871]\ttrain-mlogloss:0.0032886\ttest-mlogloss:0.014622\n",
      "[872]\ttrain-mlogloss:0.0032774\ttest-mlogloss:0.014611\n",
      "[873]\ttrain-mlogloss:0.0032666\ttest-mlogloss:0.0146004\n",
      "[874]\ttrain-mlogloss:0.0032558\ttest-mlogloss:0.014588\n",
      "[875]\ttrain-mlogloss:0.0032454\ttest-mlogloss:0.014578\n",
      "[876]\ttrain-mlogloss:0.0032346\ttest-mlogloss:0.0145638\n",
      "[877]\ttrain-mlogloss:0.0032242\ttest-mlogloss:0.0145542\n",
      "[878]\ttrain-mlogloss:0.003213\ttest-mlogloss:0.0145414\n",
      "[879]\ttrain-mlogloss:0.0032024\ttest-mlogloss:0.0145284\n",
      "[880]\ttrain-mlogloss:0.0031918\ttest-mlogloss:0.0145202\n",
      "[881]\ttrain-mlogloss:0.0031816\ttest-mlogloss:0.0145126\n",
      "[882]\ttrain-mlogloss:0.0031714\ttest-mlogloss:0.014503\n",
      "[883]\ttrain-mlogloss:0.0031606\ttest-mlogloss:0.0144972\n",
      "[884]\ttrain-mlogloss:0.0031504\ttest-mlogloss:0.014487\n",
      "[885]\ttrain-mlogloss:0.0031406\ttest-mlogloss:0.0144738\n",
      "[886]\ttrain-mlogloss:0.0031308\ttest-mlogloss:0.0144662\n",
      "[887]\ttrain-mlogloss:0.003121\ttest-mlogloss:0.0144552\n",
      "[888]\ttrain-mlogloss:0.0031112\ttest-mlogloss:0.014443\n",
      "[889]\ttrain-mlogloss:0.0031012\ttest-mlogloss:0.0144336\n",
      "[890]\ttrain-mlogloss:0.0030916\ttest-mlogloss:0.0144246\n",
      "[891]\ttrain-mlogloss:0.0030814\ttest-mlogloss:0.014418\n",
      "[892]\ttrain-mlogloss:0.0030718\ttest-mlogloss:0.0144086\n",
      "[893]\ttrain-mlogloss:0.0030624\ttest-mlogloss:0.0144\n",
      "[894]\ttrain-mlogloss:0.0030534\ttest-mlogloss:0.0143896\n",
      "[895]\ttrain-mlogloss:0.003044\ttest-mlogloss:0.0143788\n",
      "[896]\ttrain-mlogloss:0.0030342\ttest-mlogloss:0.0143678\n",
      "[897]\ttrain-mlogloss:0.003025\ttest-mlogloss:0.01436\n",
      "[898]\ttrain-mlogloss:0.0030152\ttest-mlogloss:0.0143476\n",
      "[899]\ttrain-mlogloss:0.0030058\ttest-mlogloss:0.0143396\n",
      "[900]\ttrain-mlogloss:0.0029966\ttest-mlogloss:0.0143276\n",
      "[901]\ttrain-mlogloss:0.0029874\ttest-mlogloss:0.014319\n",
      "[902]\ttrain-mlogloss:0.0029786\ttest-mlogloss:0.0143116\n",
      "[903]\ttrain-mlogloss:0.0029698\ttest-mlogloss:0.0143028\n",
      "[904]\ttrain-mlogloss:0.002961\ttest-mlogloss:0.0142956\n",
      "[905]\ttrain-mlogloss:0.0029518\ttest-mlogloss:0.014285\n",
      "[906]\ttrain-mlogloss:0.0029436\ttest-mlogloss:0.0142764\n",
      "[907]\ttrain-mlogloss:0.002935\ttest-mlogloss:0.0142684\n",
      "[908]\ttrain-mlogloss:0.0029264\ttest-mlogloss:0.0142572\n",
      "[909]\ttrain-mlogloss:0.0029174\ttest-mlogloss:0.0142496\n",
      "[910]\ttrain-mlogloss:0.0029086\ttest-mlogloss:0.0142414\n",
      "[911]\ttrain-mlogloss:0.0028996\ttest-mlogloss:0.0142316\n",
      "[912]\ttrain-mlogloss:0.002891\ttest-mlogloss:0.0142234\n",
      "[913]\ttrain-mlogloss:0.0028826\ttest-mlogloss:0.0142146\n",
      "[914]\ttrain-mlogloss:0.0028736\ttest-mlogloss:0.014206\n",
      "[915]\ttrain-mlogloss:0.0028652\ttest-mlogloss:0.014195\n",
      "[916]\ttrain-mlogloss:0.0028566\ttest-mlogloss:0.0141846\n",
      "[917]\ttrain-mlogloss:0.0028474\ttest-mlogloss:0.0141738\n",
      "[918]\ttrain-mlogloss:0.0028388\ttest-mlogloss:0.0141658\n",
      "[919]\ttrain-mlogloss:0.0028302\ttest-mlogloss:0.0141526\n",
      "[920]\ttrain-mlogloss:0.0028216\ttest-mlogloss:0.0141458\n",
      "[921]\ttrain-mlogloss:0.0028134\ttest-mlogloss:0.0141354\n",
      "[922]\ttrain-mlogloss:0.0028052\ttest-mlogloss:0.014127\n",
      "[923]\ttrain-mlogloss:0.002797\ttest-mlogloss:0.0141204\n",
      "[924]\ttrain-mlogloss:0.0027896\ttest-mlogloss:0.0141148\n",
      "[925]\ttrain-mlogloss:0.0027816\ttest-mlogloss:0.014105\n",
      "[926]\ttrain-mlogloss:0.0027734\ttest-mlogloss:0.014094\n",
      "[927]\ttrain-mlogloss:0.0027656\ttest-mlogloss:0.0140876\n",
      "[928]\ttrain-mlogloss:0.0027578\ttest-mlogloss:0.0140804\n",
      "[929]\ttrain-mlogloss:0.00275\ttest-mlogloss:0.01407\n",
      "[930]\ttrain-mlogloss:0.0027424\ttest-mlogloss:0.0140598\n",
      "[931]\ttrain-mlogloss:0.0027344\ttest-mlogloss:0.0140526\n",
      "[932]\ttrain-mlogloss:0.002727\ttest-mlogloss:0.0140464\n",
      "[933]\ttrain-mlogloss:0.0027194\ttest-mlogloss:0.014039\n",
      "[934]\ttrain-mlogloss:0.0027118\ttest-mlogloss:0.0140306\n",
      "[935]\ttrain-mlogloss:0.0027038\ttest-mlogloss:0.0140188\n",
      "[936]\ttrain-mlogloss:0.002696\ttest-mlogloss:0.0140094\n",
      "[937]\ttrain-mlogloss:0.002689\ttest-mlogloss:0.0140028\n",
      "[938]\ttrain-mlogloss:0.0026818\ttest-mlogloss:0.0139954\n",
      "[939]\ttrain-mlogloss:0.0026746\ttest-mlogloss:0.0139888\n",
      "[940]\ttrain-mlogloss:0.0026666\ttest-mlogloss:0.0139832\n",
      "[941]\ttrain-mlogloss:0.0026586\ttest-mlogloss:0.013974\n",
      "[942]\ttrain-mlogloss:0.0026512\ttest-mlogloss:0.0139674\n",
      "[943]\ttrain-mlogloss:0.0026438\ttest-mlogloss:0.0139562\n",
      "[944]\ttrain-mlogloss:0.002637\ttest-mlogloss:0.0139478\n",
      "[945]\ttrain-mlogloss:0.0026302\ttest-mlogloss:0.0139396\n",
      "[946]\ttrain-mlogloss:0.002623\ttest-mlogloss:0.0139356\n",
      "[947]\ttrain-mlogloss:0.0026156\ttest-mlogloss:0.0139256\n",
      "[948]\ttrain-mlogloss:0.002608\ttest-mlogloss:0.0139194\n",
      "[949]\ttrain-mlogloss:0.0026008\ttest-mlogloss:0.0139128\n",
      "[950]\ttrain-mlogloss:0.0025936\ttest-mlogloss:0.0139064\n",
      "[951]\ttrain-mlogloss:0.0025866\ttest-mlogloss:0.0138956\n",
      "[952]\ttrain-mlogloss:0.0025796\ttest-mlogloss:0.0138906\n",
      "[953]\ttrain-mlogloss:0.0025724\ttest-mlogloss:0.0138854\n",
      "[954]\ttrain-mlogloss:0.0025656\ttest-mlogloss:0.01388\n",
      "[955]\ttrain-mlogloss:0.0025588\ttest-mlogloss:0.0138748\n",
      "[956]\ttrain-mlogloss:0.002552\ttest-mlogloss:0.0138682\n",
      "[957]\ttrain-mlogloss:0.0025452\ttest-mlogloss:0.0138606\n",
      "[958]\ttrain-mlogloss:0.0025384\ttest-mlogloss:0.0138516\n",
      "[959]\ttrain-mlogloss:0.0025316\ttest-mlogloss:0.013845\n",
      "[960]\ttrain-mlogloss:0.0025254\ttest-mlogloss:0.0138354\n",
      "[961]\ttrain-mlogloss:0.0025188\ttest-mlogloss:0.013832\n",
      "[962]\ttrain-mlogloss:0.0025116\ttest-mlogloss:0.013822\n",
      "[963]\ttrain-mlogloss:0.002506\ttest-mlogloss:0.0138146\n",
      "[964]\ttrain-mlogloss:0.0024996\ttest-mlogloss:0.0138068\n",
      "[965]\ttrain-mlogloss:0.0024932\ttest-mlogloss:0.0138018\n",
      "[966]\ttrain-mlogloss:0.0024868\ttest-mlogloss:0.0137956\n",
      "[967]\ttrain-mlogloss:0.002481\ttest-mlogloss:0.01379\n",
      "[968]\ttrain-mlogloss:0.0024744\ttest-mlogloss:0.0137812\n",
      "[969]\ttrain-mlogloss:0.0024682\ttest-mlogloss:0.013774\n",
      "[970]\ttrain-mlogloss:0.0024624\ttest-mlogloss:0.0137698\n",
      "[971]\ttrain-mlogloss:0.0024562\ttest-mlogloss:0.0137622\n",
      "[972]\ttrain-mlogloss:0.0024502\ttest-mlogloss:0.013757\n",
      "[973]\ttrain-mlogloss:0.0024438\ttest-mlogloss:0.0137474\n",
      "[974]\ttrain-mlogloss:0.0024372\ttest-mlogloss:0.0137414\n",
      "[975]\ttrain-mlogloss:0.002431\ttest-mlogloss:0.0137362\n",
      "[976]\ttrain-mlogloss:0.0024244\ttest-mlogloss:0.0137276\n",
      "[977]\ttrain-mlogloss:0.0024184\ttest-mlogloss:0.0137216\n",
      "[978]\ttrain-mlogloss:0.0024126\ttest-mlogloss:0.0137162\n",
      "[979]\ttrain-mlogloss:0.0024064\ttest-mlogloss:0.0137068\n",
      "[980]\ttrain-mlogloss:0.0024012\ttest-mlogloss:0.0137038\n",
      "[981]\ttrain-mlogloss:0.002395\ttest-mlogloss:0.013698\n",
      "[982]\ttrain-mlogloss:0.0023896\ttest-mlogloss:0.013693\n",
      "[983]\ttrain-mlogloss:0.0023838\ttest-mlogloss:0.0136854\n",
      "[984]\ttrain-mlogloss:0.0023782\ttest-mlogloss:0.0136784\n",
      "[985]\ttrain-mlogloss:0.0023722\ttest-mlogloss:0.0136716\n",
      "[986]\ttrain-mlogloss:0.0023664\ttest-mlogloss:0.0136624\n",
      "[987]\ttrain-mlogloss:0.0023606\ttest-mlogloss:0.0136608\n",
      "[988]\ttrain-mlogloss:0.0023552\ttest-mlogloss:0.0136556\n",
      "[989]\ttrain-mlogloss:0.0023498\ttest-mlogloss:0.0136514\n",
      "[990]\ttrain-mlogloss:0.0023442\ttest-mlogloss:0.0136466\n",
      "[991]\ttrain-mlogloss:0.0023388\ttest-mlogloss:0.0136424\n",
      "[992]\ttrain-mlogloss:0.0023336\ttest-mlogloss:0.0136354\n",
      "[993]\ttrain-mlogloss:0.002328\ttest-mlogloss:0.0136274\n",
      "[994]\ttrain-mlogloss:0.0023222\ttest-mlogloss:0.0136178\n",
      "[995]\ttrain-mlogloss:0.0023166\ttest-mlogloss:0.0136102\n",
      "[996]\ttrain-mlogloss:0.0023114\ttest-mlogloss:0.0136032\n",
      "[997]\ttrain-mlogloss:0.0023058\ttest-mlogloss:0.013597\n",
      "[998]\ttrain-mlogloss:0.0023004\ttest-mlogloss:0.0135918\n",
      "[999]\ttrain-mlogloss:0.0022946\ttest-mlogloss:0.0135884\n",
      "[1000]\ttrain-mlogloss:0.002289\ttest-mlogloss:0.0135828\n",
      "[1001]\ttrain-mlogloss:0.0022834\ttest-mlogloss:0.0135772\n",
      "[1002]\ttrain-mlogloss:0.0022782\ttest-mlogloss:0.0135748\n",
      "[1003]\ttrain-mlogloss:0.0022724\ttest-mlogloss:0.0135706\n",
      "[1004]\ttrain-mlogloss:0.0022676\ttest-mlogloss:0.0135648\n",
      "[1005]\ttrain-mlogloss:0.0022618\ttest-mlogloss:0.0135544\n",
      "[1006]\ttrain-mlogloss:0.0022566\ttest-mlogloss:0.0135506\n",
      "[1007]\ttrain-mlogloss:0.0022518\ttest-mlogloss:0.0135464\n",
      "[1008]\ttrain-mlogloss:0.0022458\ttest-mlogloss:0.013542\n",
      "[1009]\ttrain-mlogloss:0.0022406\ttest-mlogloss:0.0135378\n",
      "[1010]\ttrain-mlogloss:0.0022356\ttest-mlogloss:0.0135332\n",
      "[1011]\ttrain-mlogloss:0.0022306\ttest-mlogloss:0.0135296\n",
      "[1012]\ttrain-mlogloss:0.0022254\ttest-mlogloss:0.0135246\n",
      "[1013]\ttrain-mlogloss:0.0022202\ttest-mlogloss:0.0135164\n",
      "[1014]\ttrain-mlogloss:0.0022154\ttest-mlogloss:0.0135112\n",
      "[1015]\ttrain-mlogloss:0.0022106\ttest-mlogloss:0.0135072\n",
      "[1016]\ttrain-mlogloss:0.0022054\ttest-mlogloss:0.0135024\n",
      "[1017]\ttrain-mlogloss:0.0022004\ttest-mlogloss:0.013497\n",
      "[1018]\ttrain-mlogloss:0.0021952\ttest-mlogloss:0.0134904\n",
      "[1019]\ttrain-mlogloss:0.0021904\ttest-mlogloss:0.0134858\n",
      "[1020]\ttrain-mlogloss:0.0021854\ttest-mlogloss:0.0134762\n",
      "[1021]\ttrain-mlogloss:0.0021808\ttest-mlogloss:0.0134742\n",
      "[1022]\ttrain-mlogloss:0.0021756\ttest-mlogloss:0.0134716\n",
      "[1023]\ttrain-mlogloss:0.0021708\ttest-mlogloss:0.0134642\n",
      "[1024]\ttrain-mlogloss:0.002166\ttest-mlogloss:0.0134566\n",
      "[1025]\ttrain-mlogloss:0.0021612\ttest-mlogloss:0.0134506\n",
      "[1026]\ttrain-mlogloss:0.0021562\ttest-mlogloss:0.0134462\n",
      "[1027]\ttrain-mlogloss:0.0021514\ttest-mlogloss:0.0134408\n",
      "[1028]\ttrain-mlogloss:0.0021466\ttest-mlogloss:0.0134326\n",
      "[1029]\ttrain-mlogloss:0.0021424\ttest-mlogloss:0.0134266\n",
      "[1030]\ttrain-mlogloss:0.0021372\ttest-mlogloss:0.01342\n",
      "[1031]\ttrain-mlogloss:0.0021324\ttest-mlogloss:0.0134138\n",
      "[1032]\ttrain-mlogloss:0.0021276\ttest-mlogloss:0.0134106\n",
      "[1033]\ttrain-mlogloss:0.002123\ttest-mlogloss:0.0134048\n",
      "[1034]\ttrain-mlogloss:0.0021186\ttest-mlogloss:0.0134\n",
      "[1035]\ttrain-mlogloss:0.0021136\ttest-mlogloss:0.0133952\n",
      "[1036]\ttrain-mlogloss:0.0021094\ttest-mlogloss:0.0133878\n",
      "[1037]\ttrain-mlogloss:0.0021048\ttest-mlogloss:0.0133796\n",
      "[1038]\ttrain-mlogloss:0.0021004\ttest-mlogloss:0.0133722\n",
      "[1039]\ttrain-mlogloss:0.0020954\ttest-mlogloss:0.013367\n",
      "[1040]\ttrain-mlogloss:0.0020912\ttest-mlogloss:0.0133634\n",
      "[1041]\ttrain-mlogloss:0.0020868\ttest-mlogloss:0.013357\n",
      "[1042]\ttrain-mlogloss:0.0020828\ttest-mlogloss:0.0133522\n",
      "[1043]\ttrain-mlogloss:0.0020784\ttest-mlogloss:0.0133466\n",
      "[1044]\ttrain-mlogloss:0.002074\ttest-mlogloss:0.0133424\n",
      "[1045]\ttrain-mlogloss:0.0020696\ttest-mlogloss:0.0133414\n",
      "[1046]\ttrain-mlogloss:0.0020648\ttest-mlogloss:0.0133378\n",
      "[1047]\ttrain-mlogloss:0.0020604\ttest-mlogloss:0.013333\n",
      "[1048]\ttrain-mlogloss:0.002056\ttest-mlogloss:0.013327\n",
      "[1049]\ttrain-mlogloss:0.0020514\ttest-mlogloss:0.0133206\n",
      "[1050]\ttrain-mlogloss:0.0020474\ttest-mlogloss:0.0133174\n",
      "[1051]\ttrain-mlogloss:0.0020432\ttest-mlogloss:0.0133128\n",
      "[1052]\ttrain-mlogloss:0.0020388\ttest-mlogloss:0.0133066\n",
      "[1053]\ttrain-mlogloss:0.0020348\ttest-mlogloss:0.013301\n",
      "[1054]\ttrain-mlogloss:0.0020304\ttest-mlogloss:0.0132942\n",
      "[1055]\ttrain-mlogloss:0.0020264\ttest-mlogloss:0.0132892\n",
      "[1056]\ttrain-mlogloss:0.0020222\ttest-mlogloss:0.0132824\n",
      "[1057]\ttrain-mlogloss:0.0020178\ttest-mlogloss:0.0132784\n",
      "[1058]\ttrain-mlogloss:0.002014\ttest-mlogloss:0.0132746\n",
      "[1059]\ttrain-mlogloss:0.0020098\ttest-mlogloss:0.013271\n",
      "[1060]\ttrain-mlogloss:0.0020056\ttest-mlogloss:0.0132654\n",
      "[1061]\ttrain-mlogloss:0.0020018\ttest-mlogloss:0.0132644\n",
      "[1062]\ttrain-mlogloss:0.0019974\ttest-mlogloss:0.013261\n",
      "[1063]\ttrain-mlogloss:0.0019938\ttest-mlogloss:0.0132556\n",
      "[1064]\ttrain-mlogloss:0.0019894\ttest-mlogloss:0.0132516\n",
      "[1065]\ttrain-mlogloss:0.0019852\ttest-mlogloss:0.0132476\n",
      "[1066]\ttrain-mlogloss:0.001981\ttest-mlogloss:0.0132434\n",
      "[1067]\ttrain-mlogloss:0.0019768\ttest-mlogloss:0.0132366\n",
      "[1068]\ttrain-mlogloss:0.001973\ttest-mlogloss:0.0132302\n",
      "[1069]\ttrain-mlogloss:0.001969\ttest-mlogloss:0.0132268\n",
      "[1070]\ttrain-mlogloss:0.0019652\ttest-mlogloss:0.0132216\n",
      "[1071]\ttrain-mlogloss:0.001961\ttest-mlogloss:0.013217\n",
      "[1072]\ttrain-mlogloss:0.001957\ttest-mlogloss:0.013212\n",
      "[1073]\ttrain-mlogloss:0.0019532\ttest-mlogloss:0.0132082\n",
      "[1074]\ttrain-mlogloss:0.0019492\ttest-mlogloss:0.013205\n",
      "[1075]\ttrain-mlogloss:0.0019452\ttest-mlogloss:0.0131992\n",
      "[1076]\ttrain-mlogloss:0.0019414\ttest-mlogloss:0.0131974\n",
      "[1077]\ttrain-mlogloss:0.0019378\ttest-mlogloss:0.0131936\n",
      "[1078]\ttrain-mlogloss:0.0019342\ttest-mlogloss:0.0131914\n",
      "[1079]\ttrain-mlogloss:0.0019302\ttest-mlogloss:0.013188\n",
      "[1080]\ttrain-mlogloss:0.0019262\ttest-mlogloss:0.0131864\n",
      "[1081]\ttrain-mlogloss:0.0019222\ttest-mlogloss:0.013183\n",
      "[1082]\ttrain-mlogloss:0.001919\ttest-mlogloss:0.0131798\n",
      "[1083]\ttrain-mlogloss:0.001915\ttest-mlogloss:0.0131762\n",
      "[1084]\ttrain-mlogloss:0.001911\ttest-mlogloss:0.0131726\n",
      "[1085]\ttrain-mlogloss:0.0019072\ttest-mlogloss:0.0131676\n",
      "[1086]\ttrain-mlogloss:0.0019034\ttest-mlogloss:0.0131636\n",
      "[1087]\ttrain-mlogloss:0.0018996\ttest-mlogloss:0.0131566\n",
      "[1088]\ttrain-mlogloss:0.001896\ttest-mlogloss:0.013152\n",
      "[1089]\ttrain-mlogloss:0.0018922\ttest-mlogloss:0.0131488\n",
      "[1090]\ttrain-mlogloss:0.0018886\ttest-mlogloss:0.0131446\n",
      "[1091]\ttrain-mlogloss:0.0018852\ttest-mlogloss:0.0131396\n",
      "[1092]\ttrain-mlogloss:0.0018816\ttest-mlogloss:0.0131352\n",
      "[1093]\ttrain-mlogloss:0.0018782\ttest-mlogloss:0.0131336\n",
      "[1094]\ttrain-mlogloss:0.0018748\ttest-mlogloss:0.0131294\n",
      "[1095]\ttrain-mlogloss:0.0018712\ttest-mlogloss:0.013126\n",
      "[1096]\ttrain-mlogloss:0.001868\ttest-mlogloss:0.0131204\n",
      "[1097]\ttrain-mlogloss:0.0018646\ttest-mlogloss:0.013116\n",
      "[1098]\ttrain-mlogloss:0.0018612\ttest-mlogloss:0.0131128\n",
      "[1099]\ttrain-mlogloss:0.0018578\ttest-mlogloss:0.0131064\n",
      "[1100]\ttrain-mlogloss:0.0018544\ttest-mlogloss:0.0131014\n",
      "[1101]\ttrain-mlogloss:0.0018512\ttest-mlogloss:0.0130982\n",
      "[1102]\ttrain-mlogloss:0.001848\ttest-mlogloss:0.0130958\n",
      "[1103]\ttrain-mlogloss:0.0018442\ttest-mlogloss:0.0130928\n",
      "[1104]\ttrain-mlogloss:0.0018406\ttest-mlogloss:0.0130898\n",
      "[1105]\ttrain-mlogloss:0.0018372\ttest-mlogloss:0.013085\n",
      "[1106]\ttrain-mlogloss:0.0018336\ttest-mlogloss:0.013084\n",
      "[1107]\ttrain-mlogloss:0.0018302\ttest-mlogloss:0.0130792\n",
      "[1108]\ttrain-mlogloss:0.001827\ttest-mlogloss:0.013076\n",
      "[1109]\ttrain-mlogloss:0.001824\ttest-mlogloss:0.013074\n",
      "[1110]\ttrain-mlogloss:0.0018208\ttest-mlogloss:0.0130728\n",
      "[1111]\ttrain-mlogloss:0.0018172\ttest-mlogloss:0.0130674\n",
      "[1112]\ttrain-mlogloss:0.001814\ttest-mlogloss:0.0130636\n",
      "[1113]\ttrain-mlogloss:0.0018104\ttest-mlogloss:0.0130606\n",
      "[1114]\ttrain-mlogloss:0.0018076\ttest-mlogloss:0.013059\n",
      "[1115]\ttrain-mlogloss:0.001804\ttest-mlogloss:0.0130542\n",
      "[1116]\ttrain-mlogloss:0.001801\ttest-mlogloss:0.0130532\n",
      "[1117]\ttrain-mlogloss:0.001798\ttest-mlogloss:0.013048\n",
      "[1118]\ttrain-mlogloss:0.001795\ttest-mlogloss:0.0130474\n",
      "[1119]\ttrain-mlogloss:0.0017916\ttest-mlogloss:0.0130478\n",
      "[1120]\ttrain-mlogloss:0.0017886\ttest-mlogloss:0.0130466\n",
      "[1121]\ttrain-mlogloss:0.0017854\ttest-mlogloss:0.0130416\n",
      "[1122]\ttrain-mlogloss:0.001782\ttest-mlogloss:0.0130376\n",
      "[1123]\ttrain-mlogloss:0.001779\ttest-mlogloss:0.013033\n",
      "[1124]\ttrain-mlogloss:0.001776\ttest-mlogloss:0.01303\n",
      "[1125]\ttrain-mlogloss:0.001773\ttest-mlogloss:0.0130274\n",
      "[1126]\ttrain-mlogloss:0.00177\ttest-mlogloss:0.0130242\n",
      "[1127]\ttrain-mlogloss:0.0017668\ttest-mlogloss:0.0130192\n",
      "[1128]\ttrain-mlogloss:0.0017636\ttest-mlogloss:0.013018\n",
      "[1129]\ttrain-mlogloss:0.0017606\ttest-mlogloss:0.0130172\n",
      "[1130]\ttrain-mlogloss:0.0017576\ttest-mlogloss:0.0130154\n",
      "[1131]\ttrain-mlogloss:0.0017546\ttest-mlogloss:0.0130104\n",
      "[1132]\ttrain-mlogloss:0.0017516\ttest-mlogloss:0.0130066\n",
      "[1133]\ttrain-mlogloss:0.001749\ttest-mlogloss:0.0130038\n",
      "[1134]\ttrain-mlogloss:0.0017458\ttest-mlogloss:0.013002\n",
      "[1135]\ttrain-mlogloss:0.001743\ttest-mlogloss:0.012999\n",
      "[1136]\ttrain-mlogloss:0.0017402\ttest-mlogloss:0.0129962\n",
      "[1137]\ttrain-mlogloss:0.0017374\ttest-mlogloss:0.0129938\n",
      "[1138]\ttrain-mlogloss:0.0017342\ttest-mlogloss:0.012992\n",
      "[1139]\ttrain-mlogloss:0.0017316\ttest-mlogloss:0.0129898\n",
      "[1140]\ttrain-mlogloss:0.0017288\ttest-mlogloss:0.0129864\n",
      "[1141]\ttrain-mlogloss:0.0017258\ttest-mlogloss:0.0129838\n",
      "[1142]\ttrain-mlogloss:0.001723\ttest-mlogloss:0.0129822\n",
      "[1143]\ttrain-mlogloss:0.0017204\ttest-mlogloss:0.0129784\n",
      "[1144]\ttrain-mlogloss:0.0017174\ttest-mlogloss:0.012975\n",
      "[1145]\ttrain-mlogloss:0.0017144\ttest-mlogloss:0.012972\n",
      "[1146]\ttrain-mlogloss:0.0017118\ttest-mlogloss:0.0129704\n",
      "[1147]\ttrain-mlogloss:0.001709\ttest-mlogloss:0.0129666\n",
      "[1148]\ttrain-mlogloss:0.0017064\ttest-mlogloss:0.012964\n",
      "[1149]\ttrain-mlogloss:0.0017036\ttest-mlogloss:0.012961\n",
      "[1150]\ttrain-mlogloss:0.0017008\ttest-mlogloss:0.0129574\n",
      "[1151]\ttrain-mlogloss:0.0016978\ttest-mlogloss:0.0129558\n",
      "[1152]\ttrain-mlogloss:0.0016946\ttest-mlogloss:0.012952\n",
      "[1153]\ttrain-mlogloss:0.0016922\ttest-mlogloss:0.0129482\n",
      "[1154]\ttrain-mlogloss:0.0016896\ttest-mlogloss:0.012945\n",
      "[1155]\ttrain-mlogloss:0.001687\ttest-mlogloss:0.012942\n",
      "[1156]\ttrain-mlogloss:0.001684\ttest-mlogloss:0.0129406\n",
      "[1157]\ttrain-mlogloss:0.0016818\ttest-mlogloss:0.0129384\n",
      "[1158]\ttrain-mlogloss:0.0016786\ttest-mlogloss:0.012935\n",
      "[1159]\ttrain-mlogloss:0.0016762\ttest-mlogloss:0.0129304\n",
      "[1160]\ttrain-mlogloss:0.0016732\ttest-mlogloss:0.012928\n",
      "[1161]\ttrain-mlogloss:0.001671\ttest-mlogloss:0.0129268\n",
      "[1162]\ttrain-mlogloss:0.001668\ttest-mlogloss:0.0129234\n",
      "[1163]\ttrain-mlogloss:0.0016656\ttest-mlogloss:0.0129208\n",
      "[1164]\ttrain-mlogloss:0.001663\ttest-mlogloss:0.012919\n",
      "[1165]\ttrain-mlogloss:0.0016604\ttest-mlogloss:0.0129166\n",
      "[1166]\ttrain-mlogloss:0.0016582\ttest-mlogloss:0.012916\n",
      "[1167]\ttrain-mlogloss:0.0016556\ttest-mlogloss:0.0129128\n",
      "[1168]\ttrain-mlogloss:0.001653\ttest-mlogloss:0.0129094\n",
      "[1169]\ttrain-mlogloss:0.0016508\ttest-mlogloss:0.0129056\n",
      "[1170]\ttrain-mlogloss:0.0016482\ttest-mlogloss:0.012903\n",
      "[1171]\ttrain-mlogloss:0.0016456\ttest-mlogloss:0.0128958\n",
      "[1172]\ttrain-mlogloss:0.0016434\ttest-mlogloss:0.0128924\n",
      "[1173]\ttrain-mlogloss:0.0016406\ttest-mlogloss:0.0128902\n",
      "[1174]\ttrain-mlogloss:0.0016384\ttest-mlogloss:0.012885\n",
      "[1175]\ttrain-mlogloss:0.0016358\ttest-mlogloss:0.012882\n",
      "[1176]\ttrain-mlogloss:0.0016332\ttest-mlogloss:0.0128788\n",
      "[1177]\ttrain-mlogloss:0.0016308\ttest-mlogloss:0.0128754\n",
      "[1178]\ttrain-mlogloss:0.0016284\ttest-mlogloss:0.012874\n",
      "[1179]\ttrain-mlogloss:0.0016258\ttest-mlogloss:0.0128716\n",
      "[1180]\ttrain-mlogloss:0.0016236\ttest-mlogloss:0.0128676\n",
      "[1181]\ttrain-mlogloss:0.0016212\ttest-mlogloss:0.0128646\n",
      "[1182]\ttrain-mlogloss:0.001619\ttest-mlogloss:0.0128608\n",
      "[1183]\ttrain-mlogloss:0.0016168\ttest-mlogloss:0.0128588\n",
      "[1184]\ttrain-mlogloss:0.001614\ttest-mlogloss:0.012857\n",
      "[1185]\ttrain-mlogloss:0.0016114\ttest-mlogloss:0.0128548\n",
      "[1186]\ttrain-mlogloss:0.0016094\ttest-mlogloss:0.0128518\n",
      "[1187]\ttrain-mlogloss:0.0016068\ttest-mlogloss:0.0128476\n",
      "[1188]\ttrain-mlogloss:0.0016048\ttest-mlogloss:0.0128476\n",
      "[1189]\ttrain-mlogloss:0.0016024\ttest-mlogloss:0.012846\n",
      "[1190]\ttrain-mlogloss:0.0016002\ttest-mlogloss:0.0128436\n",
      "[1191]\ttrain-mlogloss:0.0015982\ttest-mlogloss:0.0128408\n",
      "[1192]\ttrain-mlogloss:0.001596\ttest-mlogloss:0.0128398\n",
      "[1193]\ttrain-mlogloss:0.001594\ttest-mlogloss:0.01284\n",
      "[1194]\ttrain-mlogloss:0.0015916\ttest-mlogloss:0.012839\n",
      "[1195]\ttrain-mlogloss:0.0015892\ttest-mlogloss:0.0128368\n",
      "[1196]\ttrain-mlogloss:0.0015872\ttest-mlogloss:0.0128328\n",
      "[1197]\ttrain-mlogloss:0.001585\ttest-mlogloss:0.0128298\n",
      "[1198]\ttrain-mlogloss:0.001583\ttest-mlogloss:0.0128268\n",
      "[1199]\ttrain-mlogloss:0.0015804\ttest-mlogloss:0.0128236\n",
      "[1200]\ttrain-mlogloss:0.0015784\ttest-mlogloss:0.01282\n",
      "[1201]\ttrain-mlogloss:0.001576\ttest-mlogloss:0.0128162\n",
      "[1202]\ttrain-mlogloss:0.0015738\ttest-mlogloss:0.0128134\n",
      "[1203]\ttrain-mlogloss:0.0015716\ttest-mlogloss:0.012812\n",
      "[1204]\ttrain-mlogloss:0.0015692\ttest-mlogloss:0.0128108\n",
      "[1205]\ttrain-mlogloss:0.0015668\ttest-mlogloss:0.0128088\n",
      "[1206]\ttrain-mlogloss:0.0015648\ttest-mlogloss:0.0128058\n",
      "[1207]\ttrain-mlogloss:0.0015626\ttest-mlogloss:0.0128032\n",
      "[1208]\ttrain-mlogloss:0.0015604\ttest-mlogloss:0.0128006\n",
      "[1209]\ttrain-mlogloss:0.0015586\ttest-mlogloss:0.0127996\n",
      "[1210]\ttrain-mlogloss:0.0015566\ttest-mlogloss:0.0127992\n",
      "[1211]\ttrain-mlogloss:0.0015542\ttest-mlogloss:0.0127964\n",
      "[1212]\ttrain-mlogloss:0.001552\ttest-mlogloss:0.0127962\n",
      "[1213]\ttrain-mlogloss:0.0015498\ttest-mlogloss:0.0127924\n",
      "[1214]\ttrain-mlogloss:0.0015482\ttest-mlogloss:0.0127902\n",
      "[1215]\ttrain-mlogloss:0.001546\ttest-mlogloss:0.0127878\n",
      "[1216]\ttrain-mlogloss:0.0015436\ttest-mlogloss:0.0127846\n",
      "[1217]\ttrain-mlogloss:0.0015418\ttest-mlogloss:0.0127808\n",
      "[1218]\ttrain-mlogloss:0.0015394\ttest-mlogloss:0.0127764\n",
      "[1219]\ttrain-mlogloss:0.0015374\ttest-mlogloss:0.0127738\n",
      "[1220]\ttrain-mlogloss:0.0015352\ttest-mlogloss:0.0127714\n",
      "[1221]\ttrain-mlogloss:0.0015332\ttest-mlogloss:0.0127702\n",
      "[1222]\ttrain-mlogloss:0.0015314\ttest-mlogloss:0.0127682\n",
      "[1223]\ttrain-mlogloss:0.0015294\ttest-mlogloss:0.0127668\n",
      "[1224]\ttrain-mlogloss:0.0015274\ttest-mlogloss:0.0127636\n",
      "[1225]\ttrain-mlogloss:0.0015254\ttest-mlogloss:0.0127602\n",
      "[1226]\ttrain-mlogloss:0.0015232\ttest-mlogloss:0.0127556\n",
      "[1227]\ttrain-mlogloss:0.0015212\ttest-mlogloss:0.0127534\n",
      "[1228]\ttrain-mlogloss:0.0015192\ttest-mlogloss:0.0127506\n",
      "[1229]\ttrain-mlogloss:0.0015172\ttest-mlogloss:0.0127476\n",
      "[1230]\ttrain-mlogloss:0.0015152\ttest-mlogloss:0.012745\n",
      "[1231]\ttrain-mlogloss:0.001513\ttest-mlogloss:0.012741\n",
      "[1232]\ttrain-mlogloss:0.0015114\ttest-mlogloss:0.012738\n",
      "[1233]\ttrain-mlogloss:0.0015092\ttest-mlogloss:0.0127354\n",
      "[1234]\ttrain-mlogloss:0.0015076\ttest-mlogloss:0.0127342\n",
      "[1235]\ttrain-mlogloss:0.0015054\ttest-mlogloss:0.012731\n",
      "[1236]\ttrain-mlogloss:0.0015032\ttest-mlogloss:0.0127304\n",
      "[1237]\ttrain-mlogloss:0.0015014\ttest-mlogloss:0.012728\n",
      "[1238]\ttrain-mlogloss:0.0014994\ttest-mlogloss:0.0127242\n",
      "[1239]\ttrain-mlogloss:0.0014976\ttest-mlogloss:0.0127246\n",
      "[1240]\ttrain-mlogloss:0.0014958\ttest-mlogloss:0.0127212\n",
      "[1241]\ttrain-mlogloss:0.0014938\ttest-mlogloss:0.0127168\n",
      "[1242]\ttrain-mlogloss:0.001492\ttest-mlogloss:0.0127162\n",
      "[1243]\ttrain-mlogloss:0.0014902\ttest-mlogloss:0.012715\n",
      "[1244]\ttrain-mlogloss:0.0014882\ttest-mlogloss:0.0127124\n",
      "[1245]\ttrain-mlogloss:0.0014864\ttest-mlogloss:0.0127086\n",
      "[1246]\ttrain-mlogloss:0.0014848\ttest-mlogloss:0.012705\n",
      "[1247]\ttrain-mlogloss:0.0014826\ttest-mlogloss:0.0127028\n",
      "[1248]\ttrain-mlogloss:0.0014812\ttest-mlogloss:0.0127004\n",
      "[1249]\ttrain-mlogloss:0.0014794\ttest-mlogloss:0.0126988\n",
      "[1250]\ttrain-mlogloss:0.0014776\ttest-mlogloss:0.0126974\n",
      "[1251]\ttrain-mlogloss:0.0014758\ttest-mlogloss:0.0126948\n",
      "[1252]\ttrain-mlogloss:0.001474\ttest-mlogloss:0.0126924\n",
      "[1253]\ttrain-mlogloss:0.0014722\ttest-mlogloss:0.0126898\n",
      "[1254]\ttrain-mlogloss:0.0014702\ttest-mlogloss:0.0126872\n",
      "[1255]\ttrain-mlogloss:0.0014684\ttest-mlogloss:0.0126854\n",
      "[1256]\ttrain-mlogloss:0.001467\ttest-mlogloss:0.012682\n",
      "[1257]\ttrain-mlogloss:0.001465\ttest-mlogloss:0.0126792\n",
      "[1258]\ttrain-mlogloss:0.0014632\ttest-mlogloss:0.0126764\n",
      "[1259]\ttrain-mlogloss:0.0014614\ttest-mlogloss:0.0126734\n",
      "[1260]\ttrain-mlogloss:0.0014594\ttest-mlogloss:0.0126702\n",
      "[1261]\ttrain-mlogloss:0.0014576\ttest-mlogloss:0.0126686\n",
      "[1262]\ttrain-mlogloss:0.0014558\ttest-mlogloss:0.0126664\n",
      "[1263]\ttrain-mlogloss:0.0014546\ttest-mlogloss:0.0126642\n",
      "[1264]\ttrain-mlogloss:0.0014526\ttest-mlogloss:0.0126618\n",
      "[1265]\ttrain-mlogloss:0.001451\ttest-mlogloss:0.012661\n",
      "[1266]\ttrain-mlogloss:0.0014494\ttest-mlogloss:0.0126568\n",
      "[1267]\ttrain-mlogloss:0.0014476\ttest-mlogloss:0.0126528\n",
      "[1268]\ttrain-mlogloss:0.0014458\ttest-mlogloss:0.0126516\n",
      "[1269]\ttrain-mlogloss:0.0014444\ttest-mlogloss:0.0126518\n",
      "[1270]\ttrain-mlogloss:0.0014422\ttest-mlogloss:0.012647\n",
      "[1271]\ttrain-mlogloss:0.0014406\ttest-mlogloss:0.0126454\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-4998d7a51f07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m  \u001b[0mscale_pos_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m  seed=27)\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mmodelfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgb4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-f18fd734036a>\u001b[0m in \u001b[0;36mmodelfit\u001b[0;34m(alg, dtrain, predictors, useTrainCV, cv_folds, early_stopping_rounds)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mxgtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredictors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,stratified=True,\n\u001b[0;32m----> 7\u001b[0;31m             metrics={'mlogloss'}, early_stopping_rounds=early_stopping_rounds, callbacks=[xgb.callback.print_evaluation(show_stdv=False),                                                               xgb.callback.early_stop(3)])\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcvresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/peter/xgboost/python-package/xgboost/training.py\u001b[0m in \u001b[0;36mcv\u001b[0;34m(params, dtrain, num_boost_round, nfold, stratified, folds, metrics, obj, feval, maximize, early_stopping_rounds, fpreproc, as_pandas, verbose_eval, show_stdv, seed, callbacks, shuffle)\u001b[0m\n\u001b[1;32m    405\u001b[0m                            evaluation_result_list=None))\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcvfolds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0mfold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maggcv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcvfolds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/peter/xgboost/python-package/xgboost/training.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, iteration, fobj)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;34m\"\"\"\"Update the boosters for one iteration\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/peter/xgboost/python-package/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m             \u001b[0m_check_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBoosterUpdateOneIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Reducing Learning Rate\n",
    "xgb4 = XGBClassifier(\n",
    " learning_rate =0.01,\n",
    " n_estimators=5000,\n",
    " max_depth=3,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.65,\n",
    " reg_alpha=0,\n",
    " colsample_bytree=0.9,\n",
    " objective= 'multi:softprob',\n",
    " num_class = 10,\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "modelfit(xgb4, train, predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
