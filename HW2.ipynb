{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Import libraries:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import cross_validation, metrics   #Additional     scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV   #Perforing grid search\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('LargeTrain.csv')\n",
    "target = 'Class'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def modelfit(alg, dtrain, predictors,useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,stratified=True,\n",
    "            metrics={'mlogloss'}, early_stopping_rounds=early_stopping_rounds, callbacks=[xgb.callback.print_evaluation(show_stdv=False),                                                               xgb.callback.early_stop(3)])\n",
    "\n",
    "        print (cvresult)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain['Class'],eval_metric='mlogloss')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])\n",
    "\n",
    "    #Print model report:\n",
    "    print (\"\\nModel Report\")\n",
    "    print (\"Accuracy : %.4g\" % metrics.accuracy_score(dtrain['Class'].values, dtrain_predictions))\n",
    "    print (\"Log Loss Score (Train): %f\" % metrics.log_loss(dtrain['Class'], dtrain_predprob))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/xgboost/python-package/xgboost/sklearn.py:161: DeprecationWarning: The seed parameter is deprecated as of version .6.Please use random_state instead.seed is deprecated.\n",
      "  'seed is deprecated.', DeprecationWarning)\n",
      "/home/peter/xgboost/python-package/xgboost/sklearn.py:171: DeprecationWarning: The nthread parameter is deprecated as of version .6.Please use n_jobs instead.nthread is deprecated.\n",
      "  'nthread is deprecated.', DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.99623, std: 0.00068, params: {'min_child_weight': 1},\n",
       "  mean: 0.99614, std: 0.00111, params: {'min_child_weight': 3}],\n",
       " {'min_child_weight': 1},\n",
       " 0.9962272969064084)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first try with all random para\n",
    "\n",
    "#Choose all predictors except target \n",
    "predictors = [x for x in train.columns if x not in target]\n",
    "\n",
    "param_test1 = {\n",
    " 'min_child_weight':[1, 3]\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=50, max_depth=5,\n",
    " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'multi:softprob', num_class = 10, nthread=4, scale_pos_weight=1, seed=27), \n",
    " param_grid = param_test1, n_jobs=1,iid=False, cv=5)\n",
    "gsearch1.fit(train[predictors],train[target])\n",
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.83874\ttest-mlogloss:1.84137\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 3 rounds.\n",
      "[1]\ttrain-mlogloss:1.54737\ttest-mlogloss:1.55199\n",
      "[2]\ttrain-mlogloss:1.3338\ttest-mlogloss:1.34004\n",
      "[3]\ttrain-mlogloss:1.16554\ttest-mlogloss:1.17334\n",
      "[4]\ttrain-mlogloss:1.02753\ttest-mlogloss:1.03639\n",
      "[5]\ttrain-mlogloss:0.911387\ttest-mlogloss:0.921146\n",
      "[6]\ttrain-mlogloss:0.812091\ttest-mlogloss:0.822702\n",
      "[7]\ttrain-mlogloss:0.726076\ttest-mlogloss:0.73731\n",
      "[8]\ttrain-mlogloss:0.651017\ttest-mlogloss:0.663016\n",
      "[9]\ttrain-mlogloss:0.584863\ttest-mlogloss:0.597296\n",
      "[10]\ttrain-mlogloss:0.526404\ttest-mlogloss:0.539375\n",
      "[11]\ttrain-mlogloss:0.47434\ttest-mlogloss:0.48783\n",
      "[12]\ttrain-mlogloss:0.427926\ttest-mlogloss:0.44187\n",
      "[13]\ttrain-mlogloss:0.38654\ttest-mlogloss:0.40084\n",
      "[14]\ttrain-mlogloss:0.349427\ttest-mlogloss:0.363964\n",
      "[15]\ttrain-mlogloss:0.31608\ttest-mlogloss:0.330723\n",
      "[16]\ttrain-mlogloss:0.286152\ttest-mlogloss:0.30093\n",
      "[17]\ttrain-mlogloss:0.259172\ttest-mlogloss:0.274146\n",
      "[18]\ttrain-mlogloss:0.234889\ttest-mlogloss:0.250031\n",
      "[19]\ttrain-mlogloss:0.212927\ttest-mlogloss:0.228064\n",
      "[20]\ttrain-mlogloss:0.193152\ttest-mlogloss:0.208383\n",
      "[21]\ttrain-mlogloss:0.175278\ttest-mlogloss:0.190525\n",
      "[22]\ttrain-mlogloss:0.159047\ttest-mlogloss:0.174305\n",
      "[23]\ttrain-mlogloss:0.144401\ttest-mlogloss:0.159617\n",
      "[24]\ttrain-mlogloss:0.131144\ttest-mlogloss:0.146359\n",
      "[25]\ttrain-mlogloss:0.119132\ttest-mlogloss:0.134185\n",
      "[26]\ttrain-mlogloss:0.108293\ttest-mlogloss:0.123379\n",
      "[27]\ttrain-mlogloss:0.0984564\ttest-mlogloss:0.113506\n",
      "[28]\ttrain-mlogloss:0.0895458\ttest-mlogloss:0.104556\n",
      "[29]\ttrain-mlogloss:0.0814936\ttest-mlogloss:0.0964566\n",
      "[30]\ttrain-mlogloss:0.0741758\ttest-mlogloss:0.089047\n",
      "[31]\ttrain-mlogloss:0.0675464\ttest-mlogloss:0.0823176\n",
      "[32]\ttrain-mlogloss:0.0615386\ttest-mlogloss:0.0761546\n",
      "[33]\ttrain-mlogloss:0.0560864\ttest-mlogloss:0.0705618\n",
      "[34]\ttrain-mlogloss:0.0511286\ttest-mlogloss:0.0654766\n",
      "[35]\ttrain-mlogloss:0.046658\ttest-mlogloss:0.0609106\n",
      "[36]\ttrain-mlogloss:0.0425992\ttest-mlogloss:0.056734\n",
      "[37]\ttrain-mlogloss:0.038917\ttest-mlogloss:0.0529918\n",
      "[38]\ttrain-mlogloss:0.0355796\ttest-mlogloss:0.049495\n",
      "[39]\ttrain-mlogloss:0.032551\ttest-mlogloss:0.0464088\n",
      "[40]\ttrain-mlogloss:0.0298012\ttest-mlogloss:0.0436074\n",
      "[41]\ttrain-mlogloss:0.0272888\ttest-mlogloss:0.0409516\n",
      "[42]\ttrain-mlogloss:0.0250102\ttest-mlogloss:0.038606\n",
      "[43]\ttrain-mlogloss:0.0229354\ttest-mlogloss:0.0364476\n",
      "[44]\ttrain-mlogloss:0.0210492\ttest-mlogloss:0.0344858\n",
      "[45]\ttrain-mlogloss:0.0193274\ttest-mlogloss:0.0326448\n",
      "[46]\ttrain-mlogloss:0.017766\ttest-mlogloss:0.0310228\n",
      "[47]\ttrain-mlogloss:0.0163516\ttest-mlogloss:0.0295502\n",
      "[48]\ttrain-mlogloss:0.0150626\ttest-mlogloss:0.0281362\n",
      "[49]\ttrain-mlogloss:0.013894\ttest-mlogloss:0.0268488\n",
      "[50]\ttrain-mlogloss:0.0128358\ttest-mlogloss:0.0257342\n",
      "[51]\ttrain-mlogloss:0.0118676\ttest-mlogloss:0.0246868\n",
      "[52]\ttrain-mlogloss:0.0109816\ttest-mlogloss:0.0237634\n",
      "[53]\ttrain-mlogloss:0.0101752\ttest-mlogloss:0.022905\n",
      "[54]\ttrain-mlogloss:0.0094376\ttest-mlogloss:0.0220936\n",
      "[55]\ttrain-mlogloss:0.0087694\ttest-mlogloss:0.0213444\n",
      "[56]\ttrain-mlogloss:0.0081542\ttest-mlogloss:0.0206856\n",
      "[57]\ttrain-mlogloss:0.0075968\ttest-mlogloss:0.0200714\n",
      "[58]\ttrain-mlogloss:0.0070878\ttest-mlogloss:0.0194602\n",
      "[59]\ttrain-mlogloss:0.0066232\ttest-mlogloss:0.0189488\n",
      "[60]\ttrain-mlogloss:0.0061858\ttest-mlogloss:0.0184448\n",
      "[61]\ttrain-mlogloss:0.0057946\ttest-mlogloss:0.0179956\n",
      "[62]\ttrain-mlogloss:0.0054408\ttest-mlogloss:0.0176192\n",
      "[63]\ttrain-mlogloss:0.0051052\ttest-mlogloss:0.0172244\n",
      "[64]\ttrain-mlogloss:0.0048\ttest-mlogloss:0.0168492\n",
      "[65]\ttrain-mlogloss:0.0045192\ttest-mlogloss:0.0165282\n",
      "[66]\ttrain-mlogloss:0.0042658\ttest-mlogloss:0.0162244\n",
      "[67]\ttrain-mlogloss:0.0040282\ttest-mlogloss:0.0159298\n",
      "[68]\ttrain-mlogloss:0.0038102\ttest-mlogloss:0.0156408\n",
      "[69]\ttrain-mlogloss:0.003612\ttest-mlogloss:0.0153782\n",
      "[70]\ttrain-mlogloss:0.0034302\ttest-mlogloss:0.0151734\n",
      "[71]\ttrain-mlogloss:0.0032614\ttest-mlogloss:0.0149838\n",
      "[72]\ttrain-mlogloss:0.0031036\ttest-mlogloss:0.014789\n",
      "[73]\ttrain-mlogloss:0.0029614\ttest-mlogloss:0.014607\n",
      "[74]\ttrain-mlogloss:0.0028292\ttest-mlogloss:0.0144374\n",
      "[75]\ttrain-mlogloss:0.0027066\ttest-mlogloss:0.0143094\n",
      "[76]\ttrain-mlogloss:0.0025922\ttest-mlogloss:0.0141942\n",
      "[77]\ttrain-mlogloss:0.0024852\ttest-mlogloss:0.0140558\n",
      "[78]\ttrain-mlogloss:0.0023882\ttest-mlogloss:0.0139486\n",
      "[79]\ttrain-mlogloss:0.0022994\ttest-mlogloss:0.0138654\n",
      "[80]\ttrain-mlogloss:0.0022148\ttest-mlogloss:0.0137502\n",
      "[81]\ttrain-mlogloss:0.0021338\ttest-mlogloss:0.0136284\n",
      "[82]\ttrain-mlogloss:0.0020586\ttest-mlogloss:0.0135012\n",
      "[83]\ttrain-mlogloss:0.001988\ttest-mlogloss:0.0134116\n",
      "[84]\ttrain-mlogloss:0.0019208\ttest-mlogloss:0.013306\n",
      "[85]\ttrain-mlogloss:0.0018584\ttest-mlogloss:0.0131956\n",
      "[86]\ttrain-mlogloss:0.001801\ttest-mlogloss:0.0131194\n",
      "[87]\ttrain-mlogloss:0.001746\ttest-mlogloss:0.013052\n",
      "[88]\ttrain-mlogloss:0.0016938\ttest-mlogloss:0.0129952\n",
      "[89]\ttrain-mlogloss:0.0016454\ttest-mlogloss:0.0129374\n",
      "[90]\ttrain-mlogloss:0.0015998\ttest-mlogloss:0.0128722\n",
      "[91]\ttrain-mlogloss:0.001558\ttest-mlogloss:0.0127954\n",
      "[92]\ttrain-mlogloss:0.001518\ttest-mlogloss:0.0127408\n",
      "[93]\ttrain-mlogloss:0.0014812\ttest-mlogloss:0.0127004\n",
      "[94]\ttrain-mlogloss:0.001447\ttest-mlogloss:0.0126538\n",
      "[95]\ttrain-mlogloss:0.001413\ttest-mlogloss:0.0126208\n",
      "[96]\ttrain-mlogloss:0.0013822\ttest-mlogloss:0.0125718\n",
      "[97]\ttrain-mlogloss:0.0013532\ttest-mlogloss:0.0125186\n",
      "[98]\ttrain-mlogloss:0.0013258\ttest-mlogloss:0.012494\n",
      "[99]\ttrain-mlogloss:0.001299\ttest-mlogloss:0.012429\n",
      "[100]\ttrain-mlogloss:0.0012732\ttest-mlogloss:0.0123874\n",
      "[101]\ttrain-mlogloss:0.0012494\ttest-mlogloss:0.0123554\n",
      "[102]\ttrain-mlogloss:0.0012268\ttest-mlogloss:0.0123458\n",
      "[103]\ttrain-mlogloss:0.001204\ttest-mlogloss:0.0123196\n",
      "[104]\ttrain-mlogloss:0.0011828\ttest-mlogloss:0.0123042\n",
      "[105]\ttrain-mlogloss:0.0011628\ttest-mlogloss:0.0122962\n",
      "[106]\ttrain-mlogloss:0.0011432\ttest-mlogloss:0.0122616\n",
      "[107]\ttrain-mlogloss:0.0011262\ttest-mlogloss:0.0122532\n",
      "[108]\ttrain-mlogloss:0.0011094\ttest-mlogloss:0.0122442\n",
      "[109]\ttrain-mlogloss:0.0010924\ttest-mlogloss:0.0122366\n",
      "[110]\ttrain-mlogloss:0.0010778\ttest-mlogloss:0.012225\n",
      "[111]\ttrain-mlogloss:0.0010636\ttest-mlogloss:0.0121864\n",
      "[112]\ttrain-mlogloss:0.0010514\ttest-mlogloss:0.012155\n",
      "[113]\ttrain-mlogloss:0.0010394\ttest-mlogloss:0.0121302\n",
      "[114]\ttrain-mlogloss:0.0010266\ttest-mlogloss:0.0121204\n",
      "[115]\ttrain-mlogloss:0.0010146\ttest-mlogloss:0.012107\n",
      "[116]\ttrain-mlogloss:0.0010044\ttest-mlogloss:0.0120992\n",
      "[117]\ttrain-mlogloss:0.000994\ttest-mlogloss:0.012097\n",
      "[118]\ttrain-mlogloss:0.0009844\ttest-mlogloss:0.0120848\n",
      "[119]\ttrain-mlogloss:0.000975\ttest-mlogloss:0.012079\n",
      "[120]\ttrain-mlogloss:0.0009654\ttest-mlogloss:0.0120638\n",
      "[121]\ttrain-mlogloss:0.0009568\ttest-mlogloss:0.012059\n",
      "[122]\ttrain-mlogloss:0.0009484\ttest-mlogloss:0.0120474\n",
      "[123]\ttrain-mlogloss:0.00094\ttest-mlogloss:0.012036\n",
      "[124]\ttrain-mlogloss:0.000933\ttest-mlogloss:0.0120176\n",
      "[125]\ttrain-mlogloss:0.0009248\ttest-mlogloss:0.012008\n",
      "[126]\ttrain-mlogloss:0.0009176\ttest-mlogloss:0.0120022\n",
      "[127]\ttrain-mlogloss:0.0009112\ttest-mlogloss:0.012003\n",
      "[128]\ttrain-mlogloss:0.0009054\ttest-mlogloss:0.0119806\n",
      "[129]\ttrain-mlogloss:0.000899\ttest-mlogloss:0.0119596\n",
      "[130]\ttrain-mlogloss:0.000893\ttest-mlogloss:0.0119468\n",
      "[131]\ttrain-mlogloss:0.0008868\ttest-mlogloss:0.0119496\n",
      "[132]\ttrain-mlogloss:0.0008814\ttest-mlogloss:0.0119404\n",
      "[133]\ttrain-mlogloss:0.0008762\ttest-mlogloss:0.0119172\n",
      "[134]\ttrain-mlogloss:0.0008706\ttest-mlogloss:0.0119134\n",
      "[135]\ttrain-mlogloss:0.0008654\ttest-mlogloss:0.0118998\n",
      "[136]\ttrain-mlogloss:0.0008602\ttest-mlogloss:0.0118986\n",
      "[137]\ttrain-mlogloss:0.0008558\ttest-mlogloss:0.0118874\n",
      "[138]\ttrain-mlogloss:0.0008512\ttest-mlogloss:0.0118796\n",
      "[139]\ttrain-mlogloss:0.0008472\ttest-mlogloss:0.011873\n",
      "[140]\ttrain-mlogloss:0.0008432\ttest-mlogloss:0.0118866\n",
      "[141]\ttrain-mlogloss:0.0008392\ttest-mlogloss:0.0118758\n",
      "[142]\ttrain-mlogloss:0.0008348\ttest-mlogloss:0.011872\n",
      "[143]\ttrain-mlogloss:0.000831\ttest-mlogloss:0.011867\n",
      "[144]\ttrain-mlogloss:0.0008274\ttest-mlogloss:0.011866\n",
      "[145]\ttrain-mlogloss:0.0008236\ttest-mlogloss:0.0118632\n",
      "[146]\ttrain-mlogloss:0.00082\ttest-mlogloss:0.0118714\n",
      "[147]\ttrain-mlogloss:0.0008166\ttest-mlogloss:0.011869\n",
      "[148]\ttrain-mlogloss:0.000813\ttest-mlogloss:0.0118652\n",
      "Stopping. Best iteration:\n",
      "[145]\ttrain-mlogloss:0.0008236+1.08554e-05\ttest-mlogloss:0.0118632+0.00403778\n",
      "\n",
      "     test-mlogloss-mean  test-mlogloss-std  train-mlogloss-mean  \\\n",
      "0              1.841367           0.002341             1.838736   \n",
      "1              1.551989           0.002449             1.547366   \n",
      "2              1.340038           0.003013             1.333800   \n",
      "3              1.173339           0.003088             1.165542   \n",
      "4              1.036389           0.003105             1.027535   \n",
      "5              0.921146           0.003202             0.911387   \n",
      "6              0.822702           0.003449             0.812091   \n",
      "7              0.737310           0.003437             0.726076   \n",
      "8              0.663016           0.003288             0.651017   \n",
      "9              0.597296           0.003382             0.584863   \n",
      "10             0.539375           0.003454             0.526404   \n",
      "11             0.487830           0.003489             0.474340   \n",
      "12             0.441870           0.003521             0.427926   \n",
      "13             0.400840           0.003626             0.386540   \n",
      "14             0.363964           0.003475             0.349427   \n",
      "15             0.330723           0.003627             0.316080   \n",
      "16             0.300930           0.003631             0.286152   \n",
      "17             0.274146           0.003681             0.259172   \n",
      "18             0.250031           0.003773             0.234889   \n",
      "19             0.228064           0.003683             0.212927   \n",
      "20             0.208383           0.003710             0.193152   \n",
      "21             0.190525           0.003646             0.175278   \n",
      "22             0.174305           0.003726             0.159047   \n",
      "23             0.159617           0.003802             0.144401   \n",
      "24             0.146359           0.003882             0.131144   \n",
      "25             0.134185           0.003860             0.119132   \n",
      "26             0.123379           0.003862             0.108293   \n",
      "27             0.113506           0.003884             0.098456   \n",
      "28             0.104556           0.003881             0.089546   \n",
      "29             0.096457           0.003884             0.081494   \n",
      "..                  ...                ...                  ...   \n",
      "116            0.012099           0.003982             0.001004   \n",
      "117            0.012097           0.003992             0.000994   \n",
      "118            0.012085           0.003992             0.000984   \n",
      "119            0.012079           0.003992             0.000975   \n",
      "120            0.012064           0.003991             0.000965   \n",
      "121            0.012059           0.003995             0.000957   \n",
      "122            0.012047           0.003995             0.000948   \n",
      "123            0.012036           0.003999             0.000940   \n",
      "124            0.012018           0.004005             0.000933   \n",
      "125            0.012008           0.004001             0.000925   \n",
      "126            0.012002           0.003999             0.000918   \n",
      "127            0.012003           0.004006             0.000911   \n",
      "128            0.011981           0.003995             0.000905   \n",
      "129            0.011960           0.004002             0.000899   \n",
      "130            0.011947           0.004000             0.000893   \n",
      "131            0.011950           0.004010             0.000887   \n",
      "132            0.011940           0.004008             0.000881   \n",
      "133            0.011917           0.004009             0.000876   \n",
      "134            0.011913           0.004014             0.000871   \n",
      "135            0.011900           0.004011             0.000865   \n",
      "136            0.011899           0.004014             0.000860   \n",
      "137            0.011887           0.004013             0.000856   \n",
      "138            0.011880           0.004019             0.000851   \n",
      "139            0.011873           0.004022             0.000847   \n",
      "140            0.011887           0.004030             0.000843   \n",
      "141            0.011876           0.004026             0.000839   \n",
      "142            0.011872           0.004034             0.000835   \n",
      "143            0.011867           0.004032             0.000831   \n",
      "144            0.011866           0.004036             0.000827   \n",
      "145            0.011863           0.004038             0.000824   \n",
      "\n",
      "     train-mlogloss-std  \n",
      "0              0.002714  \n",
      "1              0.001971  \n",
      "2              0.001480  \n",
      "3              0.001306  \n",
      "4              0.001263  \n",
      "5              0.001340  \n",
      "6              0.001289  \n",
      "7              0.001162  \n",
      "8              0.001069  \n",
      "9              0.001236  \n",
      "10             0.001162  \n",
      "11             0.001090  \n",
      "12             0.001072  \n",
      "13             0.001158  \n",
      "14             0.001112  \n",
      "15             0.001067  \n",
      "16             0.001040  \n",
      "17             0.001024  \n",
      "18             0.000959  \n",
      "19             0.000923  \n",
      "20             0.000839  \n",
      "21             0.000780  \n",
      "22             0.000706  \n",
      "23             0.000627  \n",
      "24             0.000603  \n",
      "25             0.000543  \n",
      "26             0.000515  \n",
      "27             0.000492  \n",
      "28             0.000434  \n",
      "29             0.000378  \n",
      "..                  ...  \n",
      "116            0.000013  \n",
      "117            0.000013  \n",
      "118            0.000014  \n",
      "119            0.000013  \n",
      "120            0.000012  \n",
      "121            0.000012  \n",
      "122            0.000012  \n",
      "123            0.000012  \n",
      "124            0.000012  \n",
      "125            0.000012  \n",
      "126            0.000012  \n",
      "127            0.000012  \n",
      "128            0.000012  \n",
      "129            0.000012  \n",
      "130            0.000011  \n",
      "131            0.000012  \n",
      "132            0.000012  \n",
      "133            0.000011  \n",
      "134            0.000012  \n",
      "135            0.000012  \n",
      "136            0.000012  \n",
      "137            0.000012  \n",
      "138            0.000012  \n",
      "139            0.000011  \n",
      "140            0.000012  \n",
      "141            0.000011  \n",
      "142            0.000011  \n",
      "143            0.000011  \n",
      "144            0.000011  \n",
      "145            0.000011  \n",
      "\n",
      "[146 rows x 4 columns]\n",
      "\n",
      "Model Report\n",
      "Accuracy : 1\n",
      "Log Loss Score (Train): 0.000665\n"
     ]
    }
   ],
   "source": [
    "# choose the best n_estimators\n",
    "xgb1 = XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'multi:softprob',\n",
    " num_class = 10,\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "modelfit(xgb1, train, predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-dfb7eddb3a35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m  objective= 'multi:softprob', num_class = 10, nthread=4, scale_pos_weight=1, seed=27), \n\u001b[1;32m     10\u001b[0m  param_grid = param_test1, n_jobs=2,iid=False, cv=5)\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mgsearch1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredictors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mgsearch1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_scores_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgsearch1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgsearch1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m         \"\"\"\n\u001b[0;32m--> 829\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[1;32m    571\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m                                     error_score=self.error_score)\n\u001b[0;32m--> 573\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m                 for train, test in cv)\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    766\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/peter/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0;31m# check if timeout supported in backend future implementation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m'timeout'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetfullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/peter/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/peter/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/peter/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/peter/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# tune max_depth and min_child_weight use the formal result of best n_estimators=145\n",
    "\n",
    "param_test1 = {\n",
    " 'max_depth' : [3, 5, 7 , 9] ,\n",
    " 'min_child_weight':[1, 3, 5]\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=145, max_depth=5,\n",
    " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'multi:softprob', num_class = 10, nthread=4, scale_pos_weight=1, seed=27), \n",
    " param_grid = param_test1, n_jobs=1,iid=False, cv=5)\n",
    "gsearch1.fit(train[predictors],train[target])\n",
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
